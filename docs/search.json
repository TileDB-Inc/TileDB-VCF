[
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "",
    "section": "",
    "text": "Thanks for your interest in TileDB-VCF. The notes below give some pointers for filing issues and bug reports, or contributing to the code.\n\n\n\nReporting a bug? Please include the following information\n\noperating system and version (windows, linux, macos, etc.)\nTileDB and TileDB-VCF version (for example, the output of conda list or git status).\nif possible, a minimal working example demonstrating the bug or issue (along with any data to re-create, when feasible)\n\nPlease paste code blocks with triple backquotes (```) so that github will format it nicely. See GitHub’s guide on Markdown for more formatting tricks.\n\n\n\n\nBy contributing code to TileDB-VCF, you are agreeing to release it under the MIT License.\n\n\n\nPlease follow these instructions to build from source\nMake changes locally, then rebuild as appropriate for the level of changes (e.g.: make for libtilebvcf or python setup.py develop for apis/python).\nMake sure to run make check, or pytest to verify changes against tests (add new tests where applicable).\nPlease submit pull requests against the default master branch of TileDB-VCF."
  },
  {
    "objectID": "documentation/ingestion/overview.html",
    "href": "documentation/ingestion/overview.html",
    "title": "Ingestion",
    "section": "",
    "text": "Ingestion of VCF files is easy and straight forward."
  },
  {
    "objectID": "documentation/ingestion/cli.html",
    "href": "documentation/ingestion/cli.html",
    "title": "Ingestion",
    "section": "",
    "text": "{% hint style=“success” %} The files used in the following examples can be obtained here. {% endhint %}\nThe first step is to create an empty dataset. Let’s save the dataset in a new array called small_dataset:\ntiledbvcf create --uri small_dataset"
  },
  {
    "objectID": "documentation/ingestion/cli.html#store-samples",
    "href": "documentation/ingestion/cli.html#store-samples",
    "title": "Ingestion",
    "section": "Store samples",
    "text": "Store samples\n\nIngest from local storage\nWe’ll start with a small example using 3 synthetic VCF files, assuming they are locally available in a folder data/vcfs:\ntree data\n## data\n## ├── gene-promoters-hg38.bed\n## ├── s3-bcf-samples.txt\n## └── vcfs\n##     ├── G1.vcf.gz\n##     ├── G1.vcf.gz.csi\n##     ├── G2.vcf.gz\n##     ├── G2.vcf.gz.csi\n##     ├── G3.vcf.gz\n##     └── G3.vcf.gz.csi\n##\n## 1 directory, 8 files\nIndex files are required for ingestion. If your VCF/BCF files have not been indexed you can use bcftools to do so:\nfor f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\nWe can ingest these files into small_dataset as follows:\ntiledbvcf store --uri small_dataset data/vcfs/G*.vcf.gz\nThat’s it! Let’s verify everything went okay using the stat command to provide high-level statistics about our dataset including the number of samples it contains and the variant attributes it includes.\ntiledbvcf stat --uri small_dataset\n## Statistics for dataset 'small_dataset':\n## - Version: 4\n## - Tile capacity: 10,000\n## - Anchor gap: 1,000\n## - Number of registered samples: 3\n## - Extracted attributes: none\nAt this point you have successfully created and populated a TileDB VCF dataset using data stored locally on your machine. Next we’ll look at the more common scenario of working with files stored on a cloud object store.\n\n\nIngest from S3\nTileDB Embedded’s native cloud features make it possible to ingest samples directly from remote locations. Here, we’ll ingest the following samples located on AWS S3:\ncat data/s3-bcf-samples.txt\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf\n{% hint style=“info” %} Samples in this second batch are stored as BCF files which are also supported by TileDB-VCF. {% endhint %}\nThis process is identical to the steps perfo_r_med above, the only changes needed to our code involve setting --scratch-mb to allocate some temporary space for downloading the files and providing the URLs for the remote files. In this case, we’ll simply pass the s3-bcf-samples.txt file, which includes a list of the BCF files we want to ingest.\n{% hint style=“info” %} When ingesting samples from S3, you must configure enough scratch space to hold at least 20 samples. In general, you need 2 × the sample dimension’s sample_bactch_size (which by default is 10). You can read more about the data model here. {% endhint %}\nYou can add the --verbose flag to print out more information during the store phase.\ntiledbvcf store \\\n  --uri small_dataset \\\n  --samples-file data/s3-bcf-samples.txt \\\n  --verbose\n  \n## Initialization completed in 3.17565 sec.\n## ...\n## Done. Ingested 1,391 records (+ 69,548 anchors) from 7 samples in 10.6751\n## seconds.\nConsolidating and vacuuming fragment metadata and commits are recommended after creating a new dataset or adding several new samples to an existing dataset.\ntiledbvcf utils consolidate fragment_meta --uri small_dataset\ntiledbvcf utils consolidate commits --uri small_dataset\ntiledbvcf utils vacuum fragment_meta --uri small_dataset\ntiledbvcf utils vacuum commits --uri small_dataset"
  },
  {
    "objectID": "documentation/ingestion/cli.html#incremental-updates",
    "href": "documentation/ingestion/cli.html#incremental-updates",
    "title": "Ingestion",
    "section": "Incremental Updates",
    "text": "Incremental Updates\nA key advantage to using TileDB as a data store for genomic variant data is the ability to efficiently add new samples as they become available. The dataset creation command should be called once. Then you can invoke the store command multiple commands.\n{% hint style=“success” %} The store command is thread- and process-safe, even on the cloud. That means it can be invoked in parallel, arbitrarily scaling out the ingestion of massive datasets. {% endhint %}\nSuppose we run the store for the first 3 local samples, followed by the store command for the 7 samples stored on the S3. If we run the stat command, we can verify that our dataset now includes 10 samples.\ntiledbvcf stat --uri small_dataset\n## Statistics for dataset 'small_dataset':\n## - Version: 4\n## - Tile capacity: 10,000\n## - Anchor gap: 1,000\n## - Number of registered samples: 10\n## - Extracted attributes: none\nBecause TileDB is designed to be updatable, the store process happens efficiently and without ever touching any previously or concurrently ingested data, avoiding computationally expensive operations like regenerating combined VCF files."
  },
  {
    "objectID": "documentation/ingestion/python.html",
    "href": "documentation/ingestion/python.html",
    "title": "Ingestion",
    "section": "",
    "text": "Similar to TileDB-VCF’s command-line interface (CLI), tiledbvcf supports ingesting VCF (or BCF) files into TileDB, either when creating a new dataset or updating an existing dataset with additional samples. See the CLI Usage for a more detailed description of the ingestion process. Here, we’ll only focus on the mechanics of ingestion from Python.\nThe text file data/s3-bcf-samples.txt contains a list of S3 URIs pointing to 7 BCF files from the same cohort.\nwith open(\"data/s3-bcf-samples.txt\") as f:\n    sample_uris = [l.rstrip(\"\\n\") for l in f.readlines()]\nsample_uris\n## ['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf']\nYou can add them to your existing dataset by re-opening it in write mode and providing the file URIs. It’s also necessary to allocate scratch space so the files can be downloaded to a temporary location prior to ingestion.\nsmall_ds = tiledbvcf.Dataset('small_dataset', mode = \"w\")\nsmall_ds.ingest_samples(sample_uris)\nThe TileDB-VCF dataset located at small_dataset now includes records for 660 variants across 10 samples. The next section provides examples demonstrating how to query this dataset."
  },
  {
    "objectID": "documentation/index.html#features",
    "href": "documentation/index.html#features",
    "title": "TileDB-VCF",
    "section": "Features",
    "text": "Features\n\nEasily ingest large amounts of variant-call data at scale\nSupports ingesting single sample VCF and BCF files\nNew samples are added incrementally, avoiding computationally expensive merging operations\nAllows for highly compressed storage using TileDB sparse arrays\nEfficient, parallelized queries of variant data stored locally or remotely on S3\nExport lossless VCF/BCF files or extract specific slices of a dataset"
  },
  {
    "objectID": "documentation/index.html#whats-included",
    "href": "documentation/index.html#whats-included",
    "title": "TileDB-VCF",
    "section": "What’s Included?",
    "text": "What’s Included?\n\nCommand line interface (CLI)\nAPIs for C, C++, Python, and Java\nIntegrates with Spark and Dask"
  },
  {
    "objectID": "documentation/index.html#quick-start",
    "href": "documentation/index.html#quick-start",
    "title": "TileDB-VCF",
    "section": "Quick Start",
    "text": "Quick Start\nThe documentation website provides comprehensive usage examples but here are a few quick exercises to get you started.\nWe’ll use a dataset that includes 20 synthetic samples, each one containing over 20 million variants. We host a publicly accessible version of this dataset on S3, so if you have TileDB-VCF installed and you’d like to follow along just swap out the uri’s below for s3://tiledb-inc-demo-data/tiledbvcf-arrays/v4/vcf-samples-20. And if you don’t have TileDB-VCF installed yet, you can use our Docker images to test things out.\n\nCLI\nExport complete chr1 BCF files for a subset of samples:\ntiledbvcf export \\\n  --uri vcf-samples-20 \\\n  --regions chr1:1-248956422 \\\n  --sample-names v2-usVwJUmo,v2-WpXCYApL\nCreate a TSV file containing all variants within one or more regions of interest:\ntiledbvcf export \\\n  --uri vcf-samples-20 \\\n  --sample-names v2-tJjMfKyL,v2-eBAdKwID \\\n  -Ot --tsv-fields \"CHR,POS,REF,S:GT\" \\\n  --regions \"chr7:144000320-144008793,chr11:56490349-56491395\"\n\n\nPython\nRunning the same query in python\nimport tiledbvcf\n\nds = tiledbvcf.Dataset(uri = \"vcf-samples-20\", mode=\"r\")\n\nds.read(\n    attrs = [\"sample_name\", \"pos_start\", \"fmt_GT\"],\n    regions = [\"chr7:144000320-144008793\", \"chr11:56490349-56491395\"],\n    samples = [\"v2-tJjMfKyL\", \"v2-eBAdKwID\"]\n)\nreturns results as a pandas DataFrame\n     sample_name  pos_start    fmt_GT\n0    v2-nGEAqwFT  143999569  [-1, -1]\n1    v2-tJjMfKyL  144000262  [-1, -1]\n2    v2-tJjMfKyL  144000518  [-1, -1]\n3    v2-nGEAqwFT  144000339  [-1, -1]\n4    v2-nzLyDgYW  144000102  [-1, -1]\n..           ...        ...       ...\n566  v2-nGEAqwFT   56491395    [0, 0]\n567  v2-ijrKdkKh   56491373    [0, 0]\n568  v2-eBAdKwID   56491391    [0, 0]\n569  v2-tJjMfKyL   56491392  [-1, -1]\n570  v2-nzLyDgYW   56491365  [-1, -1]"
  },
  {
    "objectID": "documentation/index.html#want-to-learn-more",
    "href": "documentation/index.html#want-to-learn-more",
    "title": "TileDB-VCF",
    "section": "Want to Learn More?",
    "text": "Want to Learn More?\n\nBlog “Population Genomics is a Data Management Problem”\nCheck out the full documentation\n\nWhy use TileDB-VCF?\nData Model\nInstallation\nHow To\nReference"
  }
]