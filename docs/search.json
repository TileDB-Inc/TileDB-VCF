[
  {
    "objectID": "documentation/index.html#features",
    "href": "documentation/index.html#features",
    "title": "",
    "section": "Features",
    "text": "Features\n\nEasily ingest large amounts of variant-call data at scale\nSupports ingesting single sample VCF and BCF files\nNew samples are added incrementally, avoiding computationally expensive merging operations\nAllows for highly compressed storage using TileDB sparse arrays\nEfficient, parallelized queries of variant data stored locally or remotely on S3\nExport lossless VCF/BCF files or extract specific slices of a dataset"
  },
  {
    "objectID": "documentation/index.html#whats-included",
    "href": "documentation/index.html#whats-included",
    "title": "",
    "section": "What’s Included?",
    "text": "What’s Included?\n\nCommand line interface (CLI)\nAPIs for C, C++, Python, and Java\nIntegrates with Spark and Dask"
  },
  {
    "objectID": "documentation/index.html#quick-start",
    "href": "documentation/index.html#quick-start",
    "title": "",
    "section": "Quick Start",
    "text": "Quick Start\nThe documentation website provides comprehensive usage examples but here are a few quick exercises to get you started.\nWe’ll use a dataset that includes 20 synthetic samples, each one containing over 20 million variants. We host a publicly accessible version of this dataset on S3, so if you have TileDB-VCF installed and you’d like to follow along just swap out the uri’s below for s3://tiledb-inc-demo-data/tiledbvcf-arrays/v4/vcf-samples-20. And if you don’t have TileDB-VCF installed yet, you can use our Docker images to test things out.\n\nCLI\nExport complete chr1 BCF files for a subset of samples:\ntiledbvcf export \\\n  --uri vcf-samples-20 \\\n  --regions chr1:1-248956422 \\\n  --sample-names v2-usVwJUmo,v2-WpXCYApL\nCreate a TSV file containing all variants within one or more regions of interest:\ntiledbvcf export \\\n  --uri vcf-samples-20 \\\n  --sample-names v2-tJjMfKyL,v2-eBAdKwID \\\n  -Ot --tsv-fields \"CHR,POS,REF,S:GT\" \\\n  --regions \"chr7:144000320-144008793,chr11:56490349-56491395\"\n\n\nPython\nRunning the same query in python\nimport tiledbvcf\n\nds = tiledbvcf.Dataset(uri = \"vcf-samples-20\", mode=\"r\")\n\nds.read(\n    attrs = [\"sample_name\", \"pos_start\", \"fmt_GT\"],\n    regions = [\"chr7:144000320-144008793\", \"chr11:56490349-56491395\"],\n    samples = [\"v2-tJjMfKyL\", \"v2-eBAdKwID\"]\n)\nreturns results as a pandas DataFrame\n     sample_name  pos_start    fmt_GT\n0    v2-nGEAqwFT  143999569  [-1, -1]\n1    v2-tJjMfKyL  144000262  [-1, -1]\n2    v2-tJjMfKyL  144000518  [-1, -1]\n3    v2-nGEAqwFT  144000339  [-1, -1]\n4    v2-nzLyDgYW  144000102  [-1, -1]\n..           ...        ...       ...\n566  v2-nGEAqwFT   56491395    [0, 0]\n567  v2-ijrKdkKh   56491373    [0, 0]\n568  v2-eBAdKwID   56491391    [0, 0]\n569  v2-tJjMfKyL   56491392  [-1, -1]\n570  v2-nzLyDgYW   56491365  [-1, -1]"
  },
  {
    "objectID": "documentation/index.html#want-to-learn-more",
    "href": "documentation/index.html#want-to-learn-more",
    "title": "",
    "section": "Want to Learn More?",
    "text": "Want to Learn More?\n\nBlog “Population Genomics is a Data Management Problem”\nCheck out the full documentation\n\nWhy use TileDB-VCF?\nData Model\nInstallation\nHow To\nReference"
  },
  {
    "objectID": "documentation/api-reference/cli.html",
    "href": "documentation/api-reference/cli.html",
    "title": "",
    "section": "",
    "text": "createa new dataset\nstorespecified VCF files in a dataset\nexportdata from a dataset\nlistall sample names present in a dataset\nstatprints high-level statistics about a dataset\nutils utility functions for dataset\n\n\n\n\nCreate an empty TileDB-VCF dataset.\n\n\ntiledbvcf create -u <uri> [-a <fields>] [-c <N>] [-g <N>] [--tiledb-config <params>] [--checksum <checksum>] [-n]\n\n\n\n\n\n\n\n\n\n\nFlag\nDescription\n\n\n\n\n-u,--uri\nTileDB dataset URI.\n\n\n-a,--attributes\nInfo or format field names (comma-delimited) to store as separate attributes. Names should be fmt_X or info_X for a field name X (case sensitive).\n\n\n-c,--tile-capacity\nTile capacity to use for the array schema [default 10000].\n\n\n-g,--anchor-gap\nAnchor gap size to use [default 1000].\n\n\n--tiledb-config\nCSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB configuration parameter settings.\n\n\n--checksum\nChecksum to use for dataset validation on read and writes [default \"sha256\"].\n\n\n-n,--no-duplicates\nDo not allow records with duplicate end positions to be written to the array.\n\n\n\n\n\n\n\nIngests registered samples into a TileDB-VCF dataset.\n\n\ntiledbvcf store -u <uri> [-t <N>] [-p <MB>] [-d <path>] [-s <MB>] [-n <N>] [-k <N>] [-b <MB>] [-v] [--remove-sample-file] [--tiledb-config <params>] ([-f <path>] | <paths>...) [-e <N>] [--stats] [--stats-vcf-header-array]\n\n\n\n\n\n\n\n\n\n\nFlag\nDescription\n\n\n\n\n-u,--uri\nTileDB dataset URI.\n\n\n-t,--threads\nNumber of threads [default 16].\n\n\n-p,--s3-part-size\n[S3 only] Part size to use for writes (MB) [default 50].\n\n\n-d,--scratch-dir\nDirectory used for local storage of downloaded remote samples.\n\n\n-s,--scratch-mb\nAmount of local storage (in MB) allocated for downloading remote VCF files prior to ingestion [default 0]. The you must configure enough scratch space to hold at least 20 samples. In general, you need 2 × the sample dimension’s sample_bactch_size (which by default is 10). You can read more about the data model here.\n\n\n-n, --max-record-buff\nMax number of BCF records to buffer per file [default 50000].\n\n\n-k, --thread-task-size\nMax length (# columns) of an ingestion task. Affects load balancing of ingestion work across threads, and total memory consumption [default 5000000].\n\n\n-b, --mem-budget-mb\nThe total memory budget (MB) used when submitting TileDB queries [default 1024].\n\n\n-v, --verbose\nEnable verbose output.\n\n\n--tiledb-config\nCSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB configuration parameter settings.\n\n\n-f, --samples-file\nFile with 1 VCF path to be ingested per line. The format can also include an explicit index path on each line, in the format <vcf-uri><TAB><index-uri>.\n\n\n--remove-sample-file\nIf specified, the samples file (-f argument) is deleted after successful ingestion\n\n\n-e, --sample-batch-size\nNumber of samples per batch for ingestion [default 10].\n\n\n--stats\nEnable TileDB stats\n\n\n--stats-vcf-header-array\nEnable TileDB stats for vcf header array usage.\n\n\n--resume\nResume incomplete ingestion of sample batch.\n\n\n\n\n\n\n\nExports data from a TileDB-VCF dataset.\n\n\ntiledbvcf export -u <uri> [-O <format>] [-o <path>] [-t <fields>] ([-r <regions>] | [-R <path>]) [--sorted] [-n <N>] [-d <path>] [--sample-partition <I:N>] [--region-partition <I:N>] [--upload-dir <path>] [--tiledb-config <params>] [-v] [-c] [-b <MB>] ([-f <path>] | [-s <samples>]) [--stats] [--stats-vcf-header-array]\n\n\n\n\n\n\n\n\n\n\nFlag\nDescription\n\n\n\n\n-u,--uri\nTileDB dataset URI.\n\n\n-O,--output-format\nExport format. Options are: b: bcf (compressed); u: bcf; z: vcf.gz; v: vcf; t: TSV. [default b] .\n\n\n-o,--output-path\n[TSV export only] The name of the output TSV file.\n\n\n-t,--tsv-fields\n[TSV export only] An ordered CSV list of fields to export in the TSV. A field name can be one of SAMPLE, ID, REF, ALT, QUAL, POS, CHR, FILTER. Additionally, INFO fields can be specified by I and FMT fields with S. To export the intersecting query region for each row in the output, use the field names Q:POS, Q:END, or Q:LINE.\n\n\n-r,--regions\nCSV list of regions to export in the format chr:min-max.\n\n\n-R,--regions-file\nFile containing regions (BED format).\n\n\n--sorted\nDo not sort regions or regions file if they are pre-sorted.\n\n\n-n,--limit\nOnly export the first N intersecting records.\n\n\n-d,--output-dir\nDirectory used for local output of exported samples.\n\n\n--sample-partition\nPartitions the list of samples to be exported and causes this export to export only a specific partition of them. Specify in the format I:N where I is the partition index and N is the total number of partitions. Useful for batch exports.\n\n\n--region-partition\nPartitions the list of regions to be exported and causes this export to export only a specific partition of them. Specify in the format I:N where I is the partition index and N is the total number of partitions. Useful for batch exports.\n\n\n--upload-dir\nIf set, all output file(s) from the export process will be copied to the given directory (or S3 prefix) upon completion.\n\n\n--tiledb-config\nCSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB configuration parameter settings.\n\n\n-v,--verbose\nEnable verbose output.\n\n\n-c,--count-only\nDon’t write output files, only print the count of the resulting number of intersecting records.\n\n\n-b,--mem-budget-mb\nThe memory budget (MB) used when submitting TileDB queries [default 2048].\n\n\n--mem-budget-buffer-percentage\nThe percentage of the memory budget to use for TileDB query buffers [default 25].\n\n\n--mem-budget-tile-cache-percentage\nThe percentage of the memory budget to use for TileDB tile cache [default 10].\n\n\n-f,--samples-file\nFile with 1 VCF path to be registered per line. The format can also include an explicit index path on each line, in the format in the format <vcf-uri><TAB><index-uri>.\n\n\n--stats\nEnable TileDB stats\n\n\n--stats-vcf-header-array\nEnable TileDB stats for vcf header array usage.\n\n\n--disable-check-samples\nDisable validating that samples passed exist in dataset before executing query and error if any sample requested is not in the dataset.\n\n\n--disable-progress-estimation\nDisable progress estimation in verbose mode. Progress estimation can sometimes cause a performance impact.\n\n\n\n\n\n\n\nLists all sample names present in a TileDB-VCF dataset.\n\n\ntiledbvcf list -u <uri> [--tiledb-config ]\n\n\n\n\n\n\n\n\n\n\nFlag\nDescription\n\n\n\n\n-u,--uri\nTileDB dataset URI.\n\n\n--tiledb-config\nCSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB configuration parameter settings.\n\n\n\n\n\n\n\nPrints high-level statistics about a TileDB-VCF dataset.\n\n\ntiledbvcf stat -u <uri> [--tiledb-config ]\n\n\n\n\n\n\n\n\n\n\nFlag\nDescription\n\n\n\n\n-u,--uri\nTileDB dataset URI.\n\n\n--tiledb-config\nCSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB configuration parameter settings.\n\n\n\n\n\n\n\nUtils for working with a TileDB-VCF dataset, such for consolidating and vacuuming fragments or fragment metadata.\n\n\ntiledbvcf utils (consolidate|vacuum) (fragment_meta|fragments) -u <uri> [--tiledb-config ]\n\n\n\n\n\n\n\n\n\n\nFlag\nDescription\n\n\n\n\n-u,--uri\nTileDB dataset URI.\n\n\n--tiledb-config\nCSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB configuration parameter settings."
  },
  {
    "objectID": "documentation/api-reference/overview.html",
    "href": "documentation/api-reference/overview.html",
    "title": "API Reference",
    "section": "",
    "text": "This is the API reference for TileDB-VCF:\n{% content-ref url=“cli.md” %} cli.md {% endcontent-ref %}\n{% content-ref url=“python.md” %} python.md {% endcontent-ref %}"
  },
  {
    "objectID": "documentation/api-reference/python.html",
    "href": "documentation/api-reference/python.html",
    "title": "",
    "section": "",
    "text": "This is the main Python module.\n\n\nRepresentation of the grouped TileDB arrays that constitute a TileDB-VCF dataset, which includes a sparse 3D array containing the actual variant data and a sparse 1D array containing various sample metadata and the VCF header lines. Read more about the data model here.\nDataset(self, uri, mode='r', cfg=None, stats=False, verbose=False)\n\n\n\nuri: URI of TileDB-VCF dataset\nmode: (default 'r') Open the array object in read 'r' or write 'w' mode\ncfg: TileDB-VCF configuration (optional)\nstats: (bool) Enable of disable TileDB stats\nverbose: (bool) Enable of disable TileDB-VCF verbose output\n\n\n\n\n\n\nCreate a new TileDB-VCF dataset.\ncreate_dataset(extra_attrs=None, tile_capacity=None, anchor_gap=None, checksum_type=None, allow_duplicates=True)\n\n\n\nextra_attrs: (list of str attrs) list of extra attributes to materialize from the FMT or INFO field. Names should be fmt_X or info_X for a field name X (case sensitive).\ntile_capacity: (int) Tile capacity to use for the array schema (default 10000)\nanchor_gap: (int) Length of gaps between inserted anchor records in bases (default = 1000)\nchecksum_type: (str checksum) Optional override checksum type for creating new dataset (valid values are 'sha256', 'md5' or None)\nallow_duplicates: (bool) Controls whether records with duplicate end positions can be ingested written to the dataset\n\n\n\n\nIngest samples into an existing TileDB-VCF dataset.\ningest_samples(sample_uris=None, threads=None, thread_task_size=None, memory_budget_mb=None, scratch_space_path=None, scratch_space_size=None, record_limit=None, sample_batch_size=None)\n\n\n\nsample_uris: (list of str samples) CSV list of VCF/BCF sample URIs to ingest\nthreads: (int) Set the number of threads used for ingestion\nthread_task_size: (int) Set the max length (# columns) of an ingestion task (affects load balancing of ingestion work across threads and total memory consumption)\nmemory_budget_mb: (int) Set the max size (MB) of TileDB buffers before flushing (default 1024)\nrecord_limit\nstr scratch_space_path: (str) Directory used for local storage of downloaded remote samples\nscratch_space_size: (int) Amount of local storage that can be used for downloading remote samples (MB)\nsample_batch_size: (int) Number of samples per batch for ingestion (default 10)\nrecord_limit: Limit the number of VCF records read into memory per file (default 50000)\nresume: (bool) Whether to check and attempt to resume a partial completed ingestion\n\n\n\n\n\nReads data from a TileDB-VCF dataset into a Pandas Dataframe (with read()) or a PyArrow Array (with read_arrow()).\nread(attrs, samples=None, regions=None, samples_file=None, bed_file=None, skip_check_samples=False)\n\n\n\nattrs: (list of str attrs) List of attributes to extract. Can include attributes from the VCF INFO and FORMAT fields (prefixed with info_ and fmt_, respectively) as well as any of the builtin attributes:\n\nsample_name\nid\ncontig\nalleles\nfilters\npos_start\npos_end\nqual\nquery_bed_end\nquery_bed_start\nquery_bed_line\n\nsamples: (list of str samples) CSV list of sample names to be read\nregions: (list of str regions) CSV list of genomic regions to be read\nsamples_file: (str filesystem location) URI of file containing sample names to be read, one per line\nbed_file: (str filesystem location) URI of a BED file of genomic regions to be read\nskip_check_samples: (bool) Should checking the samples requested exist in the array\ndisable_progress_estimation: (bool) Should we skip estimating the progress in verbose mode? Estimating progress can have performance or memory impacts in some cases.\n\n\n\n\nFor large datasets, a call to read() may not be able to fit all results in memory. In that case, the returned dataframe will contain as many results as possible, and in order to retrieve the rest of the results, use the continue_read() function.\nYou can also use the Python generator version, read_iter().\nReturns: Pandas DataFrame containing results.\n\n\n\n\nread_completed()\n\n\nA read is considered complete if the resulting dataframe contained all results.\nReturns: (bool) True if the previous read operation was complete\n\n\n\n\nCounts data in a TileDB-VCF dataset.\ncount(samples=None, regions=None)\n\n\n\nsamples: (list of str samples) CSV list of sample names to include in the count\nregions: (list of str regions) CSV list of genomic regions to include in the count\n\n\n\n\nReturns: Number of intersecting records in the dataset\n\n\n\n\nList queryable attributes available in the VCF dataset\nattributes(attr_type = \"all\")\n\n\n\nattr_type: (list of str attributes) The subset of attributes to retrieve; \"info\" or \"fmt\" will only retrieve attributes ingested from the VCF INFO and FORMAT fields, respectively, \"builtin\" retrieves the static attributes defined in TileDB-VCF’s schema, \"all\" (the default) returns all queryable attributes\n\n\n\n\nReturns: a list of strings representing the attribute names\n\n\n\n\n\nSet various configuration parameters.\nReadConfig(limit, region_partition, sample_partition, sort_regions, memory_budget_mb, tiledb_config)\nParameters\n\nlimit: max number of records (rows) to read\nregion_partition: Region partition tuple (idx, num_partitions)\nsample_partition: Samples partition tuple (idx, num_partitions)\nsort_regions: Whether or not to sort the regions to be read (default True)\nmemory_budget_mb: Memory budget (MB) for buffer and internal allocations (default 2048)\nbuffer_percentage: Percentage of memory to dedicate to TileDB Query Buffers (default: 25)\ntiledb_tile_cache_percentage: Percentage of memory to dedicate to TileDB Tile Cache (default: 10)\ntiledb_config: List of strings in the format \"option=value\" (see here for full list TileDB configuration parameters)\n\n\n\n\nThis module is for the TileDB-VCF integration with Dask.\n\n\nReads data from a TileDB-VCF dataset into a Dask DataFrame.\nread_dask(attrs, region_partitions=1, sample_partitions=1, samples=None, regions=None, samples_file=None, bed_file=None)\n\n\n\nattrs: (list of str attrs) List of attribute names to be read\nregion_partitions (int partitions) Number of partitions over regions\nsample_partitions (int partitions) Number of partitions over samples\nsamples: (list of str samples) CSV list of sample names to be read\nregions: (list of str regions) CSV list of genomic regions to be read\nsamples_file: (str filesystem location) URI of file containing sample names to be read, one per line\nbed_file: (str filesystem location) URI of a BED file of genomic regions to be read\n\n\n\n\nPartitioning proceeds by a straightforward block distribution, parameterized by the total number of partitions and the particular partition index that a particular read operation is responsible for.\nBoth region and sample partitioning can be used together.\nReturns: Dask DataFrame with results\n\n\n\n\nMaps a function on a Dask DataFrame obtained by reading from the dataset.\nmap_dask(fnc, attrs, region_partitions=1, sample_partitions=1, samples=None, regions=None, samples_file=None, bed_file=None)\n\n\n\nfnc: (function) Function applied to each partition\nattrs: (list of str attrs) List of attribute names to be read\nregion_partitions (int partitions) Number of partitions over regions\nsample_partitions (int partitions) Number of partitions over samples\nsamples: (list of str samples) CSV list of sample names to be read\nregions: (list of str regions) CSV list of genomic regions to be read\nsamples_file: (str filesystem location) URI of file containing sample names to be read, one per line\nbed_file: (str filesystem location) URI of a BED file of genomic regions to be read\n\n\n\n\nMay be more efficient in some cases than read_dask() followed by a regular Dask map operation.\nReturns: Dask DataFrame with results"
  },
  {
    "objectID": "documentation/data-model.html",
    "href": "documentation/data-model.html",
    "title": "",
    "section": "",
    "text": "The Solution section provides a high-level overview of how and why TileDB-VCF uses 3D sparse arrays to store genomic variant data. What follows are the technical implementation details about the underlying arrays, including their schemas, dimensions, tiling order, attributes, metadata.\n\n\nA TileDB-VCF dataset is composed of a group of two or more separate TileDB arrays:\n\na 3D sparse array for the actual genomic variants and associated fields/attributes\na 1D sparse array for the metadata stored in each single-sample VCF header\n\n\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nArray type\nSparse\n\n\nRank\n3D\n\n\nCell order\nRow-major\n\n\nTile order\nRow-major\n\n\n\n\n\n\nThe dimensions in the schema are:\n\n\n\nDimension Name\nTileDB Datatype\nCorresponding VCF Field\n\n\n\n\ncontig\nTILEDB_STRING_ASCII\nCHR\n\n\nstart_pos\nuint32_t\nVCFPOSplus TileDB anchors\n\n\nsample\nTILEDB_STRING_ASCII\nSample name\n\n\n\nAs mentioned before, the coordinates of the 3D array are contig along the first dimension, chromosomal location of the variants start position along the second dimension, and sample names along the third dimension.\n\n\n\nFor each field in a single-sample VCF record there is a corresponding attribute in the schema.\n\n\n\n\n\n\n\n\nAttribute Name\nTileDB Datatype\nDescription\n\n\n\n\nend_pos\nuint32_t\nVCF END position of VCF records\n\n\nqual\nfloat\nVCF QUAL field\n\n\nalleles\nvar<char>\nCSV list of REF and ALT VCF fields\n\n\nid\nvar<char>\nVCF ID field\n\n\nfilter_ids\nvar<int32_t>\nVector of integer IDs of entries in the FILTER VCF field\n\n\nreal_start_pos\nuint32_t\nVCF POS(no anchors)\n\n\ninfo\nvar<uint8_t>\nByte blob containing any INFO fields that are not stored as explicit attributes\n\n\nfmt\nvar<uint8_t>\nByte blob containing any FMT fields that are not stored as explicit attributes\n\n\ninfo_*\nvar<uint8_t>\nOne or more attributes storing specific VCF INFO fields, e.g. info_DP, info_MQ, etc.\n\n\nfmt_*\nvar<uint8_t>\nOne or more attributes storing specific VCF FORMAT fields, e.g. fmt_GT, fmt_MIN_DP, etc.\n\n\n\nThe info_* and fmt_* attributes allow individual INFO or FMT VCF fields to be extracted into explicit array attributes. This can be beneficial if your queries frequently access only a subset of the INFO or FMT fields, as no unrelated data then needs to be fetched from storage.\n\n\n\n\n\n\nNote\n\n\n\nThe choice of which fields to extract as explicit array attributes is user-configurable during array creation.\n\n\nAny extra info or format fields not extracted as explicit array attributes are stored in the byte blob attributes, info and fmt.\n\n\n\n\nanchor_gap Anchor gap value\nextra_attributes List of INFO or FMT field names that are stored as explicit array attributes\nversion Array schema version\n\nThese metadata values are updated during array creation, and are used during the export phase. The metadata is stored as “array metadata” in the sparse data array.\n::: {.callout-warning} When ingesting samples, the sample header must be identical for all samples with respect to the contig mappings. That means all samples must have the exact same set of contigs listed in the VCF header. This requirement will be relaxed in future versions. {% endhint %}\n\n\n\n\nThe vcf_headers array stores the original text of every ingested VCF header in order to:\n\nensure the original VCF file can be fully recovered for any given sample\nreconstruct an htslib header instance when reading from the dataset, which is used for operations such as mapping a filter ID back to the filter string, etc.\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nArray type\nSparse\n\n\nRank\n1D\n\n\nCell order\nRow-major\n\n\nTile order\nRow-major\n\n\n\n\n\n\n\n\n\nDimension Name\nTileDB Datatype\nDescription\n\n\n\n\nsample\nTILEDB_STRING_ASCII\nSample name\n\n\n\n\n\n\n\n\n\nAttribute Name\nTileDB Datatype\nDescription\n\n\n\n\nheader\nvar<char>\nOriginal text of the VCF header\n\n\n\n\n\n\n\nTo summarize, we’ve described three main entities:\n\nThe variant data array (3D sparse)\nThe general metadata, stored in the variant data array as metadata\nThe VCF header array (1D sparse)\n\nAll together we term this a “TileDB-VCF dataset.” Expressed as a directory hierarchy, a TileDB-VCF dataset has the following structure:\n<dataset_uri>/\n  |_ __tiledb_group.tdb\n  |_ data/\n      |_ __array_schema.tdb\n      |_ __meta/\n            |_ <general-metadata-here>\n      ... <other array directories/fragments and files>\n  |_ vcf_headers/\n      |_ __array_schema.tdb\n      ... <other array directories/fragments and files>\nThe root of the dataset, <dataset_uri> is a TileDB group. The data member is the TileDB 3D sparse array storing the variant data. This array stores the general TileDB-VCF metadata as its array metadata in folder data/__meta. The vcf_headers member is the TileDB 1D sparse array containing the VCF header data.\n\n\n\nDuring array creation, there are several array-related parameters that the user can control. These are:\n\nArray data tile capacity (default 10,000)\nThe “anchor gap” size (default 1,000)\nThe list of INFO and FMT fields to store as explicit array attributes (default is none).\n\nOnce chosen, these parameters cannot be changed.\nDuring sample ingestion, the user can specify the:\n\nSample batch size (default 10)\n\nThe above parameters may impact read and write performance, as well as the size of the persisted array. Therefore, some care should be taken to determine good values for these parameters before ingesting a large amount of data into an array."
  },
  {
    "objectID": "documentation/how-to/work-with-cloud-object-stores.html",
    "href": "documentation/how-to/work-with-cloud-object-stores.html",
    "title": "",
    "section": "",
    "text": "TileDB Embedded provides native support for reading from and writing to cloud object stores like AWS S3, Google Cloud Storage, and Microsoft Azure Blob Store. This guide will cover some considerations for using TileDB-VCF with these services. The examples will focus exclusively on S3, which is the most widely used, but note any of the aforementioned services can be substituted, as well as on-premise services like MinIO that provide S3-compatible APIs.\n\n\nThe process of creating a TileDB-VCF dataset on S3 is nearly identical to creating a local dataset. The only difference being an s3:// address is passed to the --uri argument rather than a local file path.\ntiledbvcf create --uri s3://my-bucket/my_dataset\nThis also works when querying a TileDB-VCF dataset located on S3.\ntiledbvcf export \\\n  --uri s3://tiledb-inc-demo-data/tiledbvcf-arrays/v4/vcf-samples-20 \\\n  --sample-names v2-tJjMfKyL,v2-eBAdKwID \\\n  -Ot --tsv-fields \"CHR,POS,REF,S:GT\" \\\n  --regions \"chr7:144000320-144008793,chr11:56490349-56491395\"\n\n\n\nVCF files located on S3 can be ingested directly into a TileDBVCF dataset using 1 of 2 different possible approaches. \n\n\nThe first approach is the easiest, you simply pass the tiledbvcf store command a list of S3 URIs and TileDB-VCF takes care of the rest:\ntiledbvcf store \\\n    --uri my_dataset \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\nIn this approach, remote VCF index files (which are relatively tiny) are downloaded locally, allowing TileDB-VCF to retrieve chunks of variant data from the remote VCF files without having to download them in full. By default, index files are downloaded to your current working directory, however, you can choose to store them in different location (e.g., a temporary directory) using the --scratch-dir argument.\n\n\n\nThe second approach is to download batches of VCF files in their entirety before ingestion, which may slightly improve ingestion performance. This approach requires allocating TileDB-VCF with scratch disk space using the --scratch-mb and --scratch-dir arguments.\ntiledbvcf store \\\n    --uri my_dataset \\\n    --scratch-dir \"$TMPDIR\" \\\n    --scratch-mb 4096 \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\nThe number of VCF files that are downloaded at a time is determined by the --sample-batch-size parameter, which defaults to 10. Downloading and ingestion happens asynchronously, so, for example, batch 3 will be downloaded as batch 2 is being ingestion. As a result, you must configure enough scratch space to store at least 20 samples, assuming a batch size of 10.\n\n\n\n\nFor TileDB to access a remote storage bucket you must be properly authenticated on the machine running TileDB. For S3, this means having access to the appropriate AWS access key ID and secret access key. This typically happens in one of three ways:\n\n\nIf the AWS Command Line Interface (CLI) is installed on your machine, running aws configure will store your credentials in a local profile that TileDB can access. You can verify the CLI has been previously configured by running:\naws s3 ls\nIf properly configured, this will output a list of the S3 buckets you (and thus TileDB) can access.\n\n\n\nYou can pass your AWS access key ID and secret access key to TileDB-VCF directly via the --tiledb-config argument, which expects a comma-separated string:\ntiledbvcf store \\\n    --uri my_dataset \\\n    --tiledb-config vfs.s3.aws_access_key_id=<id>,vfs.s3.aws_secret_access_key=<secret> \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n\n\n\nYour AWS credentials can also be passed to TileDB by defining the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables."
  },
  {
    "objectID": "documentation/how-to/read-from-the-dataset.html",
    "href": "documentation/how-to/read-from-the-dataset.html",
    "title": "",
    "section": "",
    "text": "Before slicing the data, you may wish to get some information about your dataset, such as the sample names, the attributes you can query, etc.\n You can get the sample names as follows:\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.samples()\ntiledbvcf list -u my_vcf_dataset\nYou can get the attributes as follows:\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.attributes()                      # will print all queryable attributes\nds.attributes(attr_type = \"builtin\") # will print all materialized attributes\ntiledbvcf stat -u my_vcf_datset\n\n\n\nYou can rapidly read from a TileDB-VCF dataset by providing three main parameters (all optional):\n\nA subset of the samples\nA subset of the attributes\nOne or more genomic ranges\n\nEither as strings in format chr:pos_range\nOr via a BED file\n\n\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.read(\n    attrs = [\"alleles\", \"pos_start\", \"pos_end\"],\n    regions = [\"1:113409605-113475691\", \"1:113500000-113600000\"],\n    # or pass regions as follows:\n    # bed_file = <bed_filename>\n    samples = ['HG0099', 'HG00100']\n)\ntiledbvcf export \\\n    --uri my_vcf_dataset \\\n    --output-format t \\\n    --tsv-fields ALT,Q:POS,Q:END\n    --sample-names HG0099,HG00100\n    --regions 1:113409605-113475691,1:113500000-113600000\n    # or pass the regions in a BED file as follows:\n    # --regions-file <bed_filename>"
  },
  {
    "objectID": "documentation/how-to/overview.html",
    "href": "documentation/how-to/overview.html",
    "title": "How To",
    "section": "",
    "text": "This section includes useful guides about the usage of TileDB-VCF:\n{% content-ref url=“handle-large-queries.md” %} handle-large-queries.md {% endcontent-ref %}\n{% content-ref url=“work-with-cloud-object-stores.md” %} work-with-cloud-object-stores.md {% endcontent-ref %}\n{% content-ref url=“perform-distributed-queries-with-spark.md” %} perform-distributed-queries-with-spark.md {% endcontent-ref %}\n{% content-ref url=“perform-distributed-queries-with-dask.md” %} perform-distributed-queries-with-dask.md {% endcontent-ref %}"
  },
  {
    "objectID": "documentation/how-to/perform-distributed-queries-with-dask.html",
    "href": "documentation/how-to/perform-distributed-queries-with-dask.html",
    "title": "",
    "section": "",
    "text": "The tiledbvcf Python package includes integration with Dask to enable distributing large queries across node clusters. \n\n\nYou can use the tiledbvcf package’s Dask integration to partition read operations across regions and samples. The partitioning semantics are identical to those used by the CLI and Spark. \nimport tiledbvcf\nimport dask\n\nds = tiledbvcf.Dataset('my-large-dataset', mode='r')\ndask_df = ds.read_dask(attrs=['sample_name', 'pos_start', 'pos_end'],\n                       bed_file='very-large-bedfile.bed',\n                       region_partitions=10,\n                       sample_partitions=2)\nThe result is a Dask dataframe (rather than a Pandas dataframe). We’re using a local machine for simplicity but the API works on any Dask cluster. \n\n\n\nIf you plan to perform filter the results in a Dask dataframe, it may be more efficient to use map_dask() rather than read_dask(). The map_dask() function takes an additional parameter, fnc, allowing you to provide a filtering function that is applied immediately after performing the read but before inserting the result of the partition into the Dask dataframe.\nIn the following example, any variants overlapping regions in very-large-bedfile.bed are filtered out if their start position overlaps the first 25kb of the chromosome.\nimport tiledbvcf\n\nds = tiledbvcf.TileDBVCFDataset('my-large-dataset', mode='r')\ndask_df = ds.map_dask(lambda df: df[df.pos_start < 25000],\n                      attrs=['sample_name', 'pos_start', 'pos_end'],\n                      bed_file='very-large-bedfile.bed',\n                      region_partitions=10,\n                      sample_partitions=2)\nThis approach can be more efficient than using read_dask() with a separate filtering step because it avoids the possibility that partitions require multiple read operations due to memory constraints. \nThe pseudocode describing the read_partition() algorithm (i.e., the code responsible for reading the partition on a Dask worker) is:\nds = tiledbvcf.Dataset(uri, mode='r')\noverall_df = ds.read(attrs, samples, regions, ...)\nwhile not ds.read_completed():\n    df = ds.continue_read()\n    overall_df = overall_df.append(df)\nWhen using map_dask() instead, the pseudocode becomes:\nds = tiledbvcf.Dataset(uri, mode='r')\noverall_df = filter_fnc(ds.read(attrs, samples, regions, ...))\nwhile not ds.read_completed():\n    df = filter_fnc(ds.continue_read())\n    overall_df = overall_df.append(df)\nYou can see that if the provided filter_fnc() reduces the size of the data substantially, using map_dask() can reduce the likelihood that the Dask workers will run out of memory and avoid needing to perform multiple reads."
  },
  {
    "objectID": "documentation/how-to/export-to-vcf.html",
    "href": "documentation/how-to/export-to-vcf.html",
    "title": "",
    "section": "",
    "text": "You can use the TileDB-VCF CLI to export the TileDB-VCF ingested dataset back into VCF formats for downstream analyses, in a lossless way.\n\n\n\n\n\n\nWarning\n\n\n\nWhile these exports are lossless in terms of the actual data stored, they may not be identical to the original files. For example, fields within the INFO and FORMAT columns may appear in a slightly different order in the exported files.\n\n\n\n\nTo recreate all of original (single-sample) VCF files simply run the export command and set the --output-formatto v, for VCF. \ntiledbvcf export \\\n  --uri my_vcf_dataset \\\n  --output-format v \\\n  --output-dir exported-vcfs\nIf bcftools is available on your system you can use it to easily examine any of the exported files:\nbcftools view --no-header exported-vcfs/G1.vcf\n\n## 1    13350    .    A    <NON_REF>    .    .    END=36258    GT:DP:GQ:MIN_DP:PL    1/0:50:3:43:44,29,99\n## 1    42091    .    A    <NON_REF>    .    .    END=101445    GT:DP:GQ:MIN_DP:PL    0/0:8:91:60:35,62,92\n## 2    11625    .    T    <NON_REF>    .    .    END=106375    GT:DP:GQ:MIN_DP:PL    0/0:27:72:76:70,30,83\n## 3    14580    .    T    <NON_REF>    .    .    END=86190    GT:DP:GQ:MIN_DP:PL    0/1:50:78:41:67,11,43\n\n\n\nThe same mechanics covered in the reading for filtering records by sample and genomic region also apply to exporting VCF files. \ntiledbvcf export \\\n  --uri my_vcf_dataset \\\n  --output-format v \\\n  --sample-names G1,G2,G3 \\\n  --regions 4:53227-196092,9:214865-465259 \\\n  --output-dir exported-filtered-vcfs"
  },
  {
    "objectID": "documentation/how-to/perform-distributed-queries-with-tiledb-cloud.html",
    "href": "documentation/how-to/perform-distributed-queries-with-tiledb-cloud.html",
    "title": "",
    "section": "",
    "text": "The tiledbvcf Python package includes integration with TileDB-Cloud to enable distributing large queries in a serverless maner. \n\n\nYou can use the tiledbvcf package’s TileDB Cloud integration to partition read operations across regions and samples. The partitioning semantics are identical to those used by the CLI and Spark. \nimport tiledbvcf\nimport tiledb.cloud.vcf\n\ntiledb.cloud.vcf.query.read('my-large-dataset',\n                       attrs=['sample_name', 'pos_start', 'pos_end'],\n                       bed_file='very-large-bedfile.bed',\n                       region_partitions=10,\n                       sample_partitions=2)\nThe result is a pyarrow table."
  },
  {
    "objectID": "documentation/how-to/perform-distributed-queries-with-spark.html",
    "href": "documentation/how-to/perform-distributed-queries-with-spark.html",
    "title": "",
    "section": "",
    "text": "TileDB-VCF’s Spark API offers a DataSourceV2 data source to read TileDB-VCF datasets into a Spark dataframe. To begin, launch a Spark shell with:\nspark-shell \\\n    --jars build/libs/TileDB-VCF-Spark-0.1.0-SNAPSHOT.jar\nDepending on the size of the dataset and query results, you may need to increase the configured memory from the defaults.:\nspark-shell \\\n    --jars build/libs/TileDB-VCF-Spark-0.1.0-SNAPSHOT.jar \\\n    --driver-memory 16g \\\n    --executor-memory 16g\nWhile the Spark API offers much of the same functionality provided by the CLI, the main considerations when using the Spark API are typically dataframe partitioning and memory overhead.\n\n\n\nThere are two ways to partition a TileDB-VCF dataframe, which can be used separately or together:\n\nPartitioning over samples (.option(\"sample_partitions\", N)).\nPartitioning over genomic regions (.option(\"range_partitions\", M)).\n\nConceptually, these correspond to partitioning over rows and columns, respectively, in the underlying TileDB array. For example, if you are reading a subset of 200 samples in a dataset, and specify 10 sample partitions, Spark will create 10 jobs, each of which will be responsible for handling the export from 20 of the 200 selected samples.\nSimilarly, if the provided BED file contains 1,000,000 regions, and you specify 10 region partitions, Spark will create 10 jobs, each of which will be responsible for handling the export of 100,000 of the 1,000,000 regions (across all samples).\nThese two partitioning parameters can be composed to form rectangular regions of work to distribute across the available Spark executors.\n\n\n\n\n\n\nInfo\n\n\n\nThe CLI interface offers the same partitioning feature, using the --sample-partition and --region-partition flags. \n\n\n\n\n\nBecause TileDB-VCF is implemented as a native library, the Spark API makes use of both JVM on-heap and off-heap memory. This can make it challenging to correctly tune the memory consumption of each Spark job.\nThe .option(\"memory\", mb) option is used as a best-effort memory budget that any particular job is allowed to consume, across on-heap and off-heap allocations. Increasing the available memory with this option can result in large performance improvements, provided the executors are configured with sufficient memory to avoid OOM failures.\n\n\n\nTo export a few genomic regions from a dataset (which must be accessible by the Spark jobs; here we assume it is located on S3):\nval df = spark.read.format(\"io.tiledb.vcf\")\\\n        .option(\"uri\", \"s3://my-bucket/my-dataset\")\\\n        .option(\"ranges\", \"chr1:1000-2000,chr2:500-501\")\\\n        .option(\"samples\", \"sampleA,sampleB,sampleD\")\\\n        .option(\"range_partitions\", 2).load()\n\ndf.createOrReplaceTempView(\"vcf\")\n\nspark.sql(\"select contig, posStart, alleles, genotype from vcf\").show\nBecause there are only two regions specified, and two region partitions, each of the two Spark jobs started will read one of the given regions.\nWe can also place a list of sample names and a list of genomic regions to read in explicit text files, and use those during reading:\nval df = spark.read.format(\"io.tiledb.vcf\")\\\n        .option(\"uri\", \"s3://my-bucket/my-dataset\")\\\n        .option(\"bedfile\", \"s3://my-bucket/query.bed\")\\\n        .option(\"samplefile\", \"s3://my-bucket/sample-names.txt\")\\\n        .option(\"memory\", 8*1024)\\\n        .option(\"sample_partitions\", 10)\\\n        .option(\"range_partitions\", 2).load()\n\ndf.createOrReplaceTempView(\"vcf\")\n\nspark.sql(\"select contig, posStart, alleles, genotype from vcf\").show\nHere, we’ve also increased the memory budget to 8GB, and added partitioning over samples."
  },
  {
    "objectID": "documentation/how-to/create-a-dataset.html",
    "href": "documentation/how-to/create-a-dataset.html",
    "title": "",
    "section": "",
    "text": "The first step before ingesting any VCF samples is to create a dataset. This effectively creates a TileDB group and the appropriate empty arrays in it. \nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"w\") # sets dataset to \"Write\" mode\nds.create_dataset()                     # creates the dataset and\n                                        # keeps it in \"Write\" mode\ntiledbvcf create --uri my_vcf_dataset\n\nIf you wish to turn some of the `INFO` and `FMT` fields into separate _materialized_ attributes, you can do so as follows (names should be `fmt_X` or `info_X` for a field name `X` - case sensitive).\n\n```python\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"w\") \nds.create_dataset(extra_attrs=[\"info_AA\"])\ntiledbvcf create --uri my_vcf_dataset --attributes info_AA"
  },
  {
    "objectID": "documentation/how-to/ingest-samples.html",
    "href": "documentation/how-to/ingest-samples.html",
    "title": "",
    "section": "",
    "text": "Info\n\n\n\nIndexed files are required for ingestion. If your VCF/BCF files have not been indexed, you can use bcftoolsto do so:\n\n\nfor f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\nYou can ingest samples into an already created dataset as follows:\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"w\")\nds.ingest_samples(sample_uris = [\"sample_1\", \"samples_2\"])\nJust add a regular expression for the VCF file locations at the end of the store command:\ntiledbvcf store --uri my_vcf_dataset *.bcf \nAlternatively, provide a text file with the absolute locations of the VCF files, separated by a new line:\ntiledbvcf store --uri my_vcf_dataset --samples-file samples.txt\n\n\n\n\n\n\nInfo\n\n\n\nIncremental updates work in the same manner as the ingestion above, nothing special is needed. In addition, the ingestion is thread- and process-safe and, therefore, can be performed in parallel."
  },
  {
    "objectID": "documentation/how-to/handle-large-queries.html",
    "href": "documentation/how-to/handle-large-queries.html",
    "title": "",
    "section": "",
    "text": "Unlike TileDB-VCF’s CLI, which exports directly to disk, results for queries performed using Python are read into memory. Therefore, when querying even moderately sized genomic datasets, the amount of available memory must be taken into consideration.\nThis guide demonstrates several of the TileDB-VCF features for overcoming memory limitations when querying large datasets. \n\n\nOne strategy for accommodating large queries is to simply increase the amount of memory available to tiledbvcf. By default tiledbvcf allocates 2GB of memory for queries. However, this value can be adjusted using the memory_budget_mb parameter. For the purposes of this tutorial the budget will be decreased to demonstrate how tiledbvcf is able to perform genome-scale queries even in a memory constrained environment.\nimport tiledbvcf\ncfg = tiledbvcf.ReadConfig(memory_budget_mb=256)\nds = tiledbvcf.Dataset(uri, mode = \"r\", cfg = cfg)\n\n\n\nFor queries that encompass many genomic regions you can simply provide an external bed file. In this example, you will query for any variants located in the promoter region of a known gene located on chromosomes 1-4.\nAfter performing a query, you can use read_completed() to verify whether or not all results were successfully returned.\nattrs = [\"sample_name\", \"contig\", \"pos_start\", \"fmt_GT\"]\ndf = ds.read(attrs, bed_file = \"data/gene-promoters-hg38.bed\")\nds.read_completed()\n\n## False\nIn this case, it returned False, indicating the requested data was too large to fit into the allocated memory so tiledbvcf retrieved as many records as possible in this first batch. The remaining records can be retrieved using continue_read(). Here, we’ve setup our code to accommodate the possibility that the full set of results are split across multiple batches.\nprint (\"The dataframe contains\")\n\nwhile not ds.read_completed():\n    print (f\"\\t...{df.shape[0]} rows\")\n    df = df.append(ds.continue_read())\n\nprint (f\"\\t...{df.shape[0]} rows\")\n\n## The dataframe contains\n##   ...1525201 rows\n##   ...3050402 rows\n##   ...3808687 rows\nHere is the final dataframe, which includes 3,808,687 records:\ndf\n\n##         sample_name contig  pos_start    fmt_GT\n## 0       v2-Qhhvcspe   chr1          1  [-1, -1]\n## 1       v2-YMaDHIoW   chr1          1  [-1, -1]\n## 2       v2-Mcwmkqnx   chr1          1  [-1, -1]\n## 3       v2-RzweTRSv   chr1          1  [-1, -1]\n## 4       v2-ijrKdkKh   chr1          1  [-1, -1]\n## ...             ...    ...        ...       ...\n## 758280  v2-PDeVyHSO   chr4  190063262    [0, 0]\n## 758281  v2-PDeVyHSO   chr4  190063264  [-1, -1]\n## 758282  v2-PDeVyHSO   chr4  190063265  [-1, -1]\n## 758283  v2-PDeVyHSO   chr4  190063392    [0, 0]\n## 758284  v2-PDeVyHSO   chr4  190063418  [-1, -1]\n## \n## [3808687 rows x 4 columns]\n\n\n\nA Python generator version of the read method is also provided. This pattern provides a powerful interface for batch processing variant data.\nds = tiledbvcf.Dataset(uri, mode = \"r\", cfg = cfg)\n\ndf = pd.DataFrame()\nfor batch in ds.read_iter(attrs, bed_file = \"data/gene-promoters-hg38.bed\"):\n    df = df.append(batch, ignore_index = True)\n\ndf\n\n##          sample_name contig  pos_start    fmt_GT\n## 0        v2-Qhhvcspe   chr1          1  [-1, -1]\n## 1        v2-YMaDHIoW   chr1          1  [-1, -1]\n## 2        v2-Mcwmkqnx   chr1          1  [-1, -1]\n## 3        v2-RzweTRSv   chr1          1  [-1, -1]\n## 4        v2-ijrKdkKh   chr1          1  [-1, -1]\n## ...              ...    ...        ...       ...\n## 3808682  v2-PDeVyHSO   chr4  190063262    [0, 0]\n## 3808683  v2-PDeVyHSO   chr4  190063264  [-1, -1]\n## 3808684  v2-PDeVyHSO   chr4  190063265  [-1, -1]\n## 3808685  v2-PDeVyHSO   chr4  190063392    [0, 0]\n## 3808686  v2-PDeVyHSO   chr4  190063418  [-1, -1]\n## \n## [3808687 rows x 4 columns]"
  },
  {
    "objectID": "documentation/ingestion/cli.html",
    "href": "documentation/ingestion/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Note\n\n\n\nThe files used in the following examples can be obtained here.\n\n\nThe first step is to create an empty dataset. Let’s save the dataset in a new array called small_dataset:\ntiledbvcf create --uri small_dataset"
  },
  {
    "objectID": "documentation/ingestion/cli.html#store-samples",
    "href": "documentation/ingestion/cli.html#store-samples",
    "title": "CLI",
    "section": "Store samples",
    "text": "Store samples\n\nIngest from local storage\nWe’ll start with a small example using 3 synthetic VCF files, assuming they are locally available in a folder data/vcfs:\ntree data\n## data\n## ├── gene-promoters-hg38.bed\n## ├── s3-bcf-samples.txt\n## └── vcfs\n##     ├── G1.vcf.gz\n##     ├── G1.vcf.gz.csi\n##     ├── G2.vcf.gz\n##     ├── G2.vcf.gz.csi\n##     ├── G3.vcf.gz\n##     └── G3.vcf.gz.csi\n##\n## 1 directory, 8 files\nIndex files are required for ingestion. If your VCF/BCF files have not been indexed you can use bcftools to do so:\nfor f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\nWe can ingest these files into small_dataset as follows:\ntiledbvcf store --uri small_dataset data/vcfs/G*.vcf.gz\nThat’s it! Let’s verify everything went okay using the stat command to provide high-level statistics about our dataset including the number of samples it contains and the variant attributes it includes.\ntiledbvcf stat --uri small_dataset\n## Statistics for dataset 'small_dataset':\n## - Version: 4\n## - Tile capacity: 10,000\n## - Anchor gap: 1,000\n## - Number of registered samples: 3\n## - Extracted attributes: none\nAt this point you have successfully created and populated a TileDB VCF dataset using data stored locally on your machine. Next we’ll look at the more common scenario of working with files stored on a cloud object store.\n\n\nIngest from S3\nTileDB Embedded’s native cloud features make it possible to ingest samples directly from remote locations. Here, we’ll ingest the following samples located on AWS S3:\ncat data/s3-bcf-samples.txt\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf\n\n\n\n\n\n\nInfo\n\n\n\nSamples in this second batch are stored as BCF files which are also supported by TileDB-VCF.\n\n\nThis process is identical to the steps perfo_r_med above, the only changes needed to our code involve setting --scratch-mb to allocate some temporary space for downloading the files and providing the URLs for the remote files. In this case, we’ll simply pass the s3-bcf-samples.txt file, which includes a list of the BCF files we want to ingest.\n\n\n\n\n\n\nInfo\n\n\n\nWhen ingesting samples from S3, you must configure enough scratch space to hold at least 20 samples. In general, you need 2 × the sample dimension’s sample_bactch_size (which by default is 10). You can read more about the data model here.\n\n\nYou can add the --verbose flag to print out more information during the store phase.\ntiledbvcf store \\\n  --uri small_dataset \\\n  --samples-file data/s3-bcf-samples.txt \\\n  --verbose\n  \n## Initialization completed in 3.17565 sec.\n## ...\n## Done. Ingested 1,391 records (+ 69,548 anchors) from 7 samples in 10.6751\n## seconds.\nConsolidating and vacuuming fragment metadata and commits are recommended after creating a new dataset or adding several new samples to an existing dataset.\ntiledbvcf utils consolidate fragment_meta --uri small_dataset\ntiledbvcf utils consolidate commits --uri small_dataset\ntiledbvcf utils vacuum fragment_meta --uri small_dataset\ntiledbvcf utils vacuum commits --uri small_dataset"
  },
  {
    "objectID": "documentation/ingestion/cli.html#incremental-updates",
    "href": "documentation/ingestion/cli.html#incremental-updates",
    "title": "CLI",
    "section": "Incremental Updates",
    "text": "Incremental Updates\nA key advantage to using TileDB as a data store for genomic variant data is the ability to efficiently add new samples as they become available. The dataset creation command should be called once. Then you can invoke the store command multiple commands.\n\n\n\n\n\n\nNote\n\n\n\nThe store command is thread- and process-safe, even on the cloud. That means it can be invoked in parallel, arbitrarily scaling out the ingestion of massive datasets.\n\n\nSuppose we run the store for the first 3 local samples, followed by the store command for the 7 samples stored on the S3. If we run the stat command, we can verify that our dataset now includes 10 samples.\ntiledbvcf stat --uri small_dataset\n## Statistics for dataset 'small_dataset':\n## - Version: 4\n## - Tile capacity: 10,000\n## - Anchor gap: 1,000\n## - Number of registered samples: 10\n## - Extracted attributes: none\nBecause TileDB is designed to be updatable, the store process happens efficiently and without ever touching any previously or concurrently ingested data, avoiding computationally expensive operations like regenerating combined VCF files."
  },
  {
    "objectID": "documentation/ingestion/overview.html",
    "href": "documentation/ingestion/overview.html",
    "title": "Ingestion",
    "section": "",
    "text": "TileDB offers quick and easy cli, a python api and capabilities in TileDB Cloud for simple one-line distributed ingestion supporting millions of VCF files."
  },
  {
    "objectID": "documentation/ingestion/distributed-ingestion.html",
    "href": "documentation/ingestion/distributed-ingestion.html",
    "title": "Distributed Ingestion",
    "section": "",
    "text": "TileDB Cloud has built in support for simple distributed ingestion.\ningest is a simply python command that will dispatch and run a task graph to load VCF samples in parallel across a number of machines."
  },
  {
    "objectID": "documentation/ingestion/distributed-ingestion.html#example",
    "href": "documentation/ingestion/distributed-ingestion.html#example",
    "title": "Distributed Ingestion",
    "section": "Example",
    "text": "Example\nIn order to run this example please make sure to have install TileDB-Cloud-Py with pip install --user tiledb-cloud.\nimport tiledb.cloud\nfrom tiledb.cloud.vcf import ingest\n\ns3_storage_uri = \"s3://my_bucket/my_array\"\nvcf_location = \"s3://1000genomes-dragen-v3.7.6/data/individuals/hg38-graph-based\"\npattern = \"*.hard-filtered.vcf.gz\"\nmax_files = 75\nname = f\"dragen-v3.7.6-example-{max_files}\"\n\nnamespace = \"my-organization\"\ntiledb_uri = f\"tiledb://{namespace}/{name}\"\n\n# Define which contigs we want to ingest\ncontigs = Contigs.CHROMOSOMES\n\n\ningest(\n    s3_storage_uri,\n    config=config,\n    search_uri=vcf_location,\n    pattern=pattern,\n    max_files=max_files,\n    contigs=contigs,\n)\n\nContigs\nTileDB support specifying the contigs you wish to ingest. The default behavior is to ingestion all contigs present in a VCF file. However you can specify if you’d like to restrict to a specific list or a predefined list\nOptions for contigs:\n\nALL\nCHROMOSOMES\nOTHER"
  },
  {
    "objectID": "documentation/ingestion/python.html",
    "href": "documentation/ingestion/python.html",
    "title": "Python",
    "section": "",
    "text": "Similar to TileDB-VCF’s command-line interface (CLI), tiledbvcf supports ingesting VCF (or BCF) files into TileDB, either when creating a new dataset or updating an existing dataset with additional samples. See the CLI Usage for a more detailed description of the ingestion process. Here, we’ll only focus on the mechanics of ingestion from Python.\nThe text file data/s3-bcf-samples.txt contains a list of S3 URIs pointing to 7 BCF files from the same cohort.\nwith open(\"data/s3-bcf-samples.txt\") as f:\n    sample_uris = [l.rstrip(\"\\n\") for l in f.readlines()]\nsample_uris\n## ['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf']\nYou can add them to your existing dataset by re-opening it in write mode and providing the file URIs. It’s also necessary to allocate scratch space so the files can be downloaded to a temporary location prior to ingestion.\nsmall_ds = tiledbvcf.Dataset('small_dataset', mode = \"w\")\nsmall_ds.ingest_samples(sample_uris)\nThe TileDB-VCF dataset located at small_dataset now includes records for 660 variants across 10 samples. The next section provides examples demonstrating how to query this dataset."
  },
  {
    "objectID": "documentation/the-solution.html",
    "href": "documentation/the-solution.html",
    "title": "",
    "section": "",
    "text": "Population variant data can be efficiently represented using a 3D sparse array. For each sample, imagine a 2D plane where the vertical axis is the contig and the horizontal axis is the genomic position. Every variant can be represented as a range within this plane; it can be unary (i.e., a SNP) or it can be a longer range (e.g., INDEL or CNV). Each sample is then indexed by a third dimension, which is unbounded to accommodate populations of any size. The figure below shows an example for one sample, with several variants distributed across contigs chr1 , chr2 and chr3.\n\n\n\n3D array representation of population variants\n\n\nIn TileDB-VCF, we represent the start position of each range as a non-empty cell in a sparse array (black squares in the above figure). In each of those array cells, we store the end position of each cell (to create a range) along with all other fields in the corresponding single-sample VCF files for each variant (e.g., REF, ALT, etc.). Therefore, for every sample, we map variants to 2D non-empty sparse array cells.\nTo facilitate rapid retrieval of interval intersections (explained in the next section), we also inject anchors (green square in the above figure) to breakup long ranges. Specifically, we create a new non-empty cell every anchor_gap bases from the start of the range (where anchor_gap is a user-defined parameter), which is identical to the range cell, except that (1) it has a new start coordinate and (2) it stores the real start position in an attribute.\nNote that regardless of the number of samples, we do not inject any additional information other than that of the anchors, which is user configurable and turns out to be negligible for real datasets. In other words, this solution leads to linear storage in the number of samples, thus being scalable.\n\n\n\nThe typical access pattern used for variant data involves one or more rectangles covering a set of genomic ranges across one or more samples. In the figure below, let the black rectangle be the user’s query. Observe that the results are highlighted in blue (v1, v2, v4, v7). However, the rectangle misses v1, i.e., the case where an Indel/CNV range intersects the query rectangle, but the start position is outside the rectangle.\n\n\n\nInterval intersection using expanded query ranges and anchors\n\n\nThis is the motivation behind anchors. TileDB-VCF expands the user’s query range on the left by anchor_gap. It then reports as results the cells that are included in the expanded query if their end position (stored in an attribute) comes after the query range start endpoint. In the example above, TileDB-VCF retrieves anchor a1 and Indel/CNV v3. It reports v1 as a result (as it can be extracted directly from anchor a1), but filters out v3.\n\n\n\n\n\n\nInfo\n\n\n\nBy representing each sample’s variants as non-empty cells in a 2D plane and using anchors and expanded queries, we managed to model population variants as 3D sparse arrays and use the vanilla functionality of TileDB-Embedded, inheriting all its powerful features out of the box.\n\n\nQuite often, the analyses requires data retrieval based on numerous query ranges (up to the order of millions), which must be submitted simultaneously. TileDB-VCF leverages the highly optimized multi-range subarray functionality of TileDB Embedded, which leads to unparalleled performance for such scenarios.\nBut what about updates? That’s the topic of the next section.\n\n\n\nTileDB-VCF is based on TileDB Embedded which supports rapid updates via immutable fragments. That means that every time a new batch of samples is added to the 3D array, the previous contents of the array are not modified at all. Each batch write operation is totally independent and _lock-free—_any number of threads or processes can write simultaneously without synchronization, while ensuring consistency. With TileDB-VCF, both update time and storage size scales linearly with the number of new samples, solving the N+1 problem.\n\n\n\nTileDB-VCF allows you to efficiently store, update and access enormous population genomic datasets, all open-source. And although it is a C++ embedded library (which works on a single machine), it also integrates very well with Spark and Dask, enabling you to scale your analysis to large clusters. However, the burden of managing clusters (spinning them up, deploying the software, monitoring the resources, etc.), falls entirely on you, which is quite an undertaking.\nEnter TileDB Cloud!\nTileDB Cloud allows you to perform parallel ingestion and parallel slicing / processing, 100% serverless. This means that you do not have to spin up large clusters or pay for idle time. You can easily slice data or define complex task flows comprised of thousands of tasks, which TileDB Cloud deploys elastically in its serverless infrastructure, providing an unmatched combination of ease of use and low cost on the cloud—even for your most challenging analyses.\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\nThe shear volume of data generated by modern population genomics applications creates enormous challenges around data management and collaboration.\n\n\nTileDB Cloud offers groundbreaking features for collaborative genomic research:\n\nPopular public datasets made available in the TileDB format for direct analysis\nEasy mechanism for one to share their data and code (Jupyter notebooks and user-defined functions), either with a specific set of users or the entire world\nEasy ways to explore and discover public data and code.\n\nJoin our vision to build a growing community around open data and code!\n\n\n\nIf you are familiar with the TileDB Embedded data model, you can delve into the technical details of how TileDB-VCF stores genomic variant data using [D sparse arrays. Otherwise you can proceed with installation instructions and tutorials."
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_allele_frequencies.html",
    "href": "examples/tutorial_tiledbvcf_allele_frequencies.html",
    "title": "",
    "section": "",
    "text": "This notebook will walk you through - Selecting allele frequencies - Filtering on allele frequencies - Direct allele frequency access\nThis notebook uses the TileDB-Inc/vcf-1kghicov-dragen-v376 version of the 1000 genome project.\n\nimport tiledbvcf\nimport tiledb\n\n\nuri = \"tiledb://TileDB-Inc/vcf-1kghicov-dragen-v376\"\n\n\n\nThe computed allele frequency can be included in results with the info_TILEDB_IAF attribute\n\nattributes = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"fmt_GT\", \"info_TILEDB_IAF\"]\n\nds = tiledbvcf.Dataset(uri, mode=\"r\")\n\n\n# Query for BTD across all 3404 samples\ndf = ds.read(\n    attrs=attributes,\n    regions=[\"chr3:15601341-15722311\"],\n)\n\n\ndf\n\n\n\n\n  \n    \n      \n      sample_name\n      contig\n      pos_start\n      pos_end\n      alleles\n      fmt_GT\n      info_TILEDB_IAF\n    \n  \n  \n    \n      0\n      HG00096\n      chr3\n      15601536\n      15601536\n      [A, G]\n      [1, 1]\n      None\n    \n    \n      1\n      HG00097\n      chr3\n      15601536\n      15601536\n      [A, G]\n      [1, 1]\n      None\n    \n    \n      2\n      HG00099\n      chr3\n      15601536\n      15601536\n      [A, G]\n      [1, 1]\n      None\n    \n    \n      3\n      HG00100\n      chr3\n      15601536\n      15601536\n      [A, G]\n      [1, 1]\n      None\n    \n    \n      4\n      HG00101\n      chr3\n      15601536\n      15601536\n      [A, G]\n      [0, 1]\n      None\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      606391\n      NA21143\n      chr3\n      15721413\n      15721413\n      [C, T]\n      [0, 1]\n      None\n    \n    \n      606392\n      NA21144\n      chr3\n      15721413\n      15721413\n      [C, T]\n      [1, 1]\n      None\n    \n    \n      606393\n      NA21144\n      chr3\n      15721470\n      15721470\n      [T, C]\n      [0, 1]\n      None\n    \n    \n      606394\n      NA21143\n      chr3\n      15721933\n      15721933\n      [C, T]\n      [0, 1]\n      None\n    \n    \n      606395\n      NA21144\n      chr3\n      15722066\n      15722066\n      [A, G]\n      [0, 1]\n      None\n    \n  \n\n606396 rows × 7 columns\n\n\n\n\n\n\nFilters for allele frequency can also be included by using the set_af_filter parameter.\n\n# Query for BTD across all 3404 samples\ndf = ds.read(\n    attrs=attributes,\n    regions=[\"chr3:15601341-15722311\"],\n    set_af_filter=\"<0.5\",\n)\n\n\ndf\n\n\n\n\n  \n    \n      \n      sample_name\n      contig\n      pos_start\n      pos_end\n      alleles\n      fmt_GT\n      info_TILEDB_IAF\n    \n  \n  \n    \n      0\n      HG00101\n      chr3\n      15601536\n      15601536\n      [A, G]\n      [0, 1]\n      [0.027682202, 0.9723178]\n    \n    \n      1\n      HG00100\n      chr3\n      15601668\n      15601668\n      [G, A]\n      [0, 1]\n      [0.43612567, 0.56387436]\n    \n    \n      2\n      HG00100\n      chr3\n      15602568\n      15602568\n      [A, G]\n      [0, 1]\n      [0.42662117, 0.57337886]\n    \n    \n      3\n      HG00100\n      chr3\n      15602688\n      15602688\n      [G, A]\n      [0, 1]\n      [0.47108433, 0.52891564]\n    \n    \n      4\n      HG00096\n      chr3\n      15603161\n      15603161\n      [A, G]\n      [0, 1]\n      [0.44444445, 0.5555556]\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      370583\n      NA21144\n      chr3\n      15721189\n      15721189\n      [C, G]\n      [0, 1]\n      [0.43555242, 0.5644476]\n    \n    \n      370584\n      NA21143\n      chr3\n      15721340\n      15721340\n      [G, A]\n      [0, 1]\n      [0.3335806, 0.6664194]\n    \n    \n      370585\n      NA21143\n      chr3\n      15721413\n      15721413\n      [C, T]\n      [0, 1]\n      [0.33333334, 0.6666667]\n    \n    \n      370586\n      NA21144\n      chr3\n      15721470\n      15721470\n      [T, C]\n      [0, 1]\n      [0.4868421, 0.5131579]\n    \n    \n      370587\n      NA21144\n      chr3\n      15722066\n      15722066\n      [A, G]\n      [0, 1]\n      [0.4359155, 0.56408453]\n    \n  \n\n370588 rows × 7 columns\n\n\n\n\n\n\nAllele frequencies can also be queried directly in addition to as part of the variant query\n\n# Get the variant stats ur\nwith tiledb.Group(uri) as g:\n    alleles_uri  = g[\"variant_stats\"].uri\n\n\n# BRCA1 from https://www.ncbi.nlm.nih.gov/gene/672\ncontig = 'chr17'\nregion = slice(43_036_174, 43_133_600)  # pos is 0 based\n\n\n# Query allele frequencies and get results as a pandas dataframe\n\nwith tiledb.open(alleles_uri) as A:\n    df = A.query(attrs=[\"ac\", \"allele\"], dims=[\"pos\", \"contig\"]).df[contig, region]\n\n\n# Summarize frequencies\n\ndef calc_af(df):\n    \"\"\"Consolidate AC and compute AN, AF\"\"\"\n    # Allele Count (AC) = sum of all AC at the same locus\n    # This step consolidates ACs from all ingested batches\n    df = df.groupby([\"pos\", \"allele\"], sort=True).sum()\n\n    # Allele Number (AN) = sum of AC at the same locus\n    an = df.groupby([\"pos\"], sort=True).ac.sum().rename(\"an\") \n    df = df.join(an, how=\"inner\")\n    \n    # Allele Frequency (AF) = AC / AN\n    df[\"af\"] = df.ac / df.an\n    return df\n\ncalc_af(df)\n\n\n\n\n  \n    \n      \n      \n      ac\n      an\n      af\n    \n    \n      pos\n      allele\n      \n      \n      \n    \n  \n  \n    \n      43036179\n      A\n      33\n      66\n      0.500000\n    \n    \n      G\n      33\n      66\n      0.500000\n    \n    \n      43036181\n      A\n      1\n      2\n      0.500000\n    \n    \n      G\n      1\n      2\n      0.500000\n    \n    \n      43036308\n      C\n      238\n      528\n      0.450758\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43133528\n      C\n      5\n      12\n      0.416667\n    \n    \n      T\n      6\n      12\n      0.500000\n    \n    \n      TC\n      1\n      12\n      0.083333\n    \n    \n      43133556\n      C\n      1\n      2\n      0.500000\n    \n    \n      T\n      1\n      2\n      0.500000\n    \n  \n\n8292 rows × 3 columns"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html",
    "href": "examples/tutorial_tiledbvcf_basics.html",
    "title": "",
    "section": "",
    "text": "This notebook will cover how to:\n\ningest S3 hosted VCF files directly into TileDB-VCF\nquery variant data from TileDB using genomic ranges\nquery variant data and export results to single and group VCFs\nperform distributed queries via TileDB Cloud"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#packages",
    "href": "examples/tutorial_tiledbvcf_basics.html#packages",
    "title": "",
    "section": "Packages",
    "text": "Packages\n\nimport os\nimport tiledb\nimport tiledb.cloud\nimport tiledbvcf\nimport numpy as np\n\nprint(\n    f\"tiledb v{tiledb.version.version}\\n\"\n    f\"numpy v{np.__version__}\\n\"\n    # f\"tiledb-vcf v{tiledbvcf.version}\\n\"\n    f\"tiledb-cloud v{tiledb.cloud.version.version}\\n\"\n)\n\ntiledb v0.21.3\nnumpy v1.21.6\ntiledb-cloud v0.10.0\n\n\n\nSetup TileDB’s virtual file system.\n\nvfs = tiledb.VFS(config=tiledb.Config())"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#example-vcf-files",
    "href": "examples/tutorial_tiledbvcf_basics.html#example-vcf-files",
    "title": "",
    "section": "Example VCF Files",
    "text": "Example VCF Files\nTileDB-VCF inherits TileDB’s native support for working with remote object stores, so we can ingest samples directly from remote services like AWS S3, Google Cloud, and Azure. Here, we’ll use single sample BCF files with chromosome 1 data for phase 3 samples from the 1KG project stored in a publicly readable S3 bucket.\n\nvcf_bucket = \"s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1\"\n\nFor demonstration purposes we’ll ingest individual batches of samples one at a time. To do this, we’ll create a list of S3 URIs pointing to 5 sample BCF files.\n\nbatch1_samples = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\nbatch1_uris = [f\"{vcf_bucket}/{s}\" for s in batch1_samples]\nbatch1_uris\n\n['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00096.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00097.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00099.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00100.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00101.bcf']"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#create-a-new-dataset",
    "href": "examples/tutorial_tiledbvcf_basics.html#create-a-new-dataset",
    "title": "",
    "section": "Create a New Dataset",
    "text": "Create a New Dataset\nBefore we can ingest VCF data into TileDB we must first create our new dataset. First, we’ll create a TileDB-VCF Dataset instance that opens a connection to our desired array_uri in write mode.\n\nds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\nds\n\n<tiledbvcf.dataset.Dataset at 0x7efc1c203350>\n\n\nNext, we’ll create the empty TileDB-VCF dataset.\nNote: We can optionally pass a VCF file to the vcf_attrs argument to automatically materialize all of the INFO and FORMAT fields as separate attributes in the array (rather than htslib-encoded blobs), which can improve query performance.\n\nds.create_dataset(vcf_attrs=batch1_uris[0], enable_allele_count=True, enable_variant_stats=True)\n\n# verify the array exists\nos.listdir(array_uri)\n\n['allele_count',\n 'metadata',\n '__meta',\n 'variant_stats',\n '__group',\n '__tiledb_group.tdb',\n 'data']"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#ingest-initial-batch-of-5-vcf-files",
    "href": "examples/tutorial_tiledbvcf_basics.html#ingest-initial-batch-of-5-vcf-files",
    "title": "",
    "section": "Ingest Initial Batch of 5 VCF Files",
    "text": "Ingest Initial Batch of 5 VCF Files\nWith the empty TileDB-VCF dataset created we can now ingest our first batch of samples.\n\n%%time\nds.ingest_samples(sample_uris = batch1_uris)\n\nCPU times: user 10.2 s, sys: 1.55 s, total: 11.8 s\nWall time: 44.8 s\n\n\nThis should take approximately 30s when ingesting from S3. Note most of this time is spent streaming the data, ingesting local copies of these files takes about 12s.\nWe can verify the samples were ingested by listing the sample IDs contained within the new array:\n\nds = tiledbvcf.Dataset(array_uri, mode = \"r\")\nds.samples()\n\n['HG00096', 'HG00097', 'HG00099', 'HG00100', 'HG00101']"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#ingest-a-second-batch-of-5-vcf-files",
    "href": "examples/tutorial_tiledbvcf_basics.html#ingest-a-second-batch-of-5-vcf-files",
    "title": "",
    "section": "Ingest a Second Batch of 5 VCF Files",
    "text": "Ingest a Second Batch of 5 VCF Files\nUpdate the existing TileDB-VCF dataset with a second batch of samples.\n\nbatch2_samples = [\"HG00102.bcf\", \"HG00103.bcf\", \"HG00105.bcf\", \"HG00106.bcf\", \"HG00107.bcf\"]\nbatch2_uris = [f\"{vcf_bucket}/{s}\" for s in batch2_samples]\nbatch2_uris\n\n['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00102.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00103.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00105.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00106.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00107.bcf']\n\n\nThe process is identical: we need to reopen the Dataset in write mode and then pass another list of VCF URIs to ingest_samples().\n\n%%time \nds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\") #Incremental update to the array, previous data is not touched \nds.ingest_samples(sample_uris = batch2_uris)\n\nCPU times: user 10.2 s, sys: 1.57 s, total: 11.8 s\nWall time: 46.9 s\n\n\n\nds = tiledbvcf.Dataset(array_uri, mode = \"r\") #Open array in read mode, and print out the samples we just ingested to verify\nds.samples()\n\n['HG00096',\n 'HG00097',\n 'HG00099',\n 'HG00100',\n 'HG00101',\n 'HG00102',\n 'HG00103',\n 'HG00105',\n 'HG00106',\n 'HG00107']\n\n\nKey takeaways: - new samples are easily added to existing datasets - incremental updates are extremely efficient"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#setup-1",
    "href": "examples/tutorial_tiledbvcf_basics.html#setup-1",
    "title": "",
    "section": "Setup",
    "text": "Setup\nImport the Delayed module from tiledb.cloud. This is allows us to convert a normal python function into one that can be Delayed and serverlessly executed.\n\nfrom tiledb.cloud.compute import Delayed"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#run-a-basic-query-serverlessly",
    "href": "examples/tutorial_tiledbvcf_basics.html#run-a-basic-query-serverlessly",
    "title": "",
    "section": "Run a Basic Query Serverlessly",
    "text": "Run a Basic Query Serverlessly\nCreate UDF that wraps tiledbvcf.read().\n\ndef vcf_query(uri, attrs, regions, samples=None, sample_partition=(0, 1)):\n    import tiledbvcf\n    import pyarrow\n    cfg = tiledbvcf.ReadConfig(sample_partition=sample_partition)\n    vcf_ds = tiledbvcf.Dataset(uri, mode=\"r\", cfg=cfg)\n    \n    results = [vcf_ds.read_arrow(attrs=attrs, samples=samples, regions=regions)]\n    while not vcf_ds.read_completed():\n        results.append(vcf_ds.continue_read_arrow())\n\n    return pyarrow.concat_tables(results, promote=False)\n\nCreate a delayed instance of this UDF and specify the parameters we want to run it with.\n\narray_uri = \"tiledb://TileDB-Inc/vcf-1kg-nygc\"\n\nquery_attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"fmt_GT\"]\nquery_samples = [\"HG00096\", \"HG00097\", \"HG00099\", \"HG00100\", \"HG00101\", \"HG00102\", \"HG00103\", \"HG00105\", \"HG00106\", \"HG00107\", \"HG00108\", \"HG00109\", \"HG00110\", \"HG00111\", \"HG00112\", \"HG00113\", \"HG00114\", \"HG00115\", \"HG00116\", \"HG00117\", \"HG00118\", \"HG00119\", \"HG00120\", \"HG00121\", \"HG00122\", \"HG00123\", \"HG00125\", \"HG00126\", \"HG00127\", \"HG00128\", \"HG00129\", \"HG00130\", \"HG00131\", \"HG00132\", \"HG00133\", \"HG00136\", \"HG00137\", \"HG00138\", \"HG00139\", \"HG00140\", \"HG00141\", \"HG00142\", \"HG00143\", \"HG00145\", \"HG00146\", \"HG00148\", \"HG00149\", \"HG00150\", \"HG00151\", \"HG00154\", \"HG00155\", \"HG00157\", \"HG00158\", \"HG00159\", \"HG00160\", \"HG00171\", \"HG00173\", \"HG00174\", \"HG00176\", \"HG00177\", \"HG00178\", \"HG00179\", \"HG00180\", \"HG00181\", \"HG00182\", \"HG00183\", \"HG00185\", \"HG00186\", \"HG00187\", \"HG00188\", \"HG00189\", \"HG00190\", \"HG00231\", \"HG00232\", \"HG00233\", \"HG00234\", \"HG00235\", \"HG00236\", \"HG00237\", \"HG00238\", \"HG00239\", \"HG00240\", \"HG00242\", \"HG00243\", \"HG00244\", \"HG00245\", \"HG00246\", \"HG00250\", \"HG00251\", \"HG00252\", \"HG00253\", \"HG00254\", \"HG00255\", \"HG00256\", \"HG00257\", \"HG00258\", \"HG00259\", \"HG00260\", \"HG00261\", \"HG00262\"]\nquery_regions = [\"chr1:43337848-43352772\"]\n\ndelayed_read = Delayed(\n        func_exec=vcf_query,\n        result_format=tiledb.cloud.UDFResultType.ARROW,\n    )(\n        uri=array_uri, \n        attrs=query_attrs, \n        regions=query_regions,\n        samples=query_samples[:10],\n)\n\n\ndelayed_read.visualize()\n\n\n\n\nCall compute() to serverlessly execute the function.\n\n%%time\nres1 = delayed_read.compute()\nres1.to_pandas()\n\nCPU times: user 15.1 ms, sys: 3.82 ms, total: 19 ms\nWall time: 8.21 s\n\n\n\n\n\n  \n    \n      \n      sample_name\n      contig\n      pos_start\n      pos_end\n      fmt_GT\n    \n  \n  \n    \n      0\n      HG00099\n      chr1\n      43337743\n      43338411\n      [0, 0]\n    \n    \n      1\n      HG00106\n      chr1\n      43337749\n      43338055\n      [0, 0]\n    \n    \n      2\n      HG00102\n      chr1\n      43337756\n      43338088\n      [0, 0]\n    \n    \n      3\n      HG00097\n      chr1\n      43337791\n      43338053\n      [0, 0]\n    \n    \n      4\n      HG00100\n      chr1\n      43337812\n      43337850\n      [0, 0]\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      22442\n      HG00105\n      chr1\n      43352768\n      43352779\n      [0, 0]\n    \n    \n      22443\n      HG00106\n      chr1\n      43352769\n      43352769\n      [0, 0]\n    \n    \n      22444\n      HG00106\n      chr1\n      43352770\n      43352791\n      [0, 0]\n    \n    \n      22445\n      HG00100\n      chr1\n      43352771\n      43352771\n      [0, 0]\n    \n    \n      22446\n      HG00100\n      chr1\n      43352772\n      43352781\n      [0, 0]\n    \n  \n\n22447 rows × 5 columns\n\n\n\nKey takeaways: - TileDB-VCF Datasets can be sliced directly from remote object stores - TileDB Cloud provides a flexible platform for building serverless data processing pipelines"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#run-multiple-batches-of-queries-in-parallel",
    "href": "examples/tutorial_tiledbvcf_basics.html#run-multiple-batches-of-queries-in-parallel",
    "title": "",
    "section": "Run Multiple Batches of Queries in Parallel",
    "text": "Run Multiple Batches of Queries in Parallel\nAdd another UDF to combine the outputs into a single result.\n\ndef combine_results(dfs): #Taking resluts and combining them into one table\n    import pyarrow as pa\n    out = pa.concat_tables([x for x in dfs if x is not None])\n    return out\n\nAssemble and visualize the task graph.\n\nnparts = 10\ndelayed_results = []\n\nfor i in range(nparts): #splitting up 100 samples into 10 partitions, that will run in parallel, only thing that changes is the sample index\n    delayed_results.append(\n        Delayed(\n            func_exec=vcf_query, \n            result_format=tiledb.cloud.UDFResultType.ARROW,\n        )(\n            uri=array_uri, \n            attrs=query_attrs,\n            regions=query_regions,\n            samples=query_samples,\n            sample_partition=(i, nparts),\n        )\n    )\ndelayed_results = Delayed(combine_results, local=True)(delayed_results) #Taking all the results and combining them into a single table, via the `combine_results` function\n\ndelayed_results.visualize()\n\n\n\n\n\n%%time\nres2 = delayed_results.compute()\nres2.to_pandas()\n\nCPU times: user 112 ms, sys: 26.7 ms, total: 139 ms\nWall time: 9.29 s\n\n\n\n\n\n  \n    \n      \n      sample_name\n      contig\n      pos_start\n      pos_end\n      fmt_GT\n    \n  \n  \n    \n      0\n      HG00099\n      chr1\n      43337743\n      43338411\n      [0, 0]\n    \n    \n      1\n      HG00106\n      chr1\n      43337749\n      43338055\n      [0, 0]\n    \n    \n      2\n      HG00102\n      chr1\n      43337756\n      43338088\n      [0, 0]\n    \n    \n      3\n      HG00097\n      chr1\n      43337791\n      43338053\n      [0, 0]\n    \n    \n      4\n      HG00100\n      chr1\n      43337812\n      43337850\n      [0, 0]\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      216254\n      HG00260\n      chr1\n      43352762\n      43352785\n      [0, 0]\n    \n    \n      216255\n      HG00262\n      chr1\n      43352762\n      43352766\n      [0, 0]\n    \n    \n      216256\n      HG00261\n      chr1\n      43352766\n      43352769\n      [0, 0]\n    \n    \n      216257\n      HG00262\n      chr1\n      43352767\n      43352775\n      [0, 0]\n    \n    \n      216258\n      HG00261\n      chr1\n      43352770\n      43352779\n      [0, 0]\n    \n  \n\n216259 rows × 5 columns\n\n\n\nKey takeaways: - TileDB Cloud’s serverless infrastructure makes it easy to setup and run large scale distributed queries"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html",
    "href": "examples/tutorial_tiledbvcf_gwas.html",
    "title": "",
    "section": "",
    "text": "In this notebook we’ll perform a rudimentary genome-wide association study using the 1000 Genomes (1KG) dataset. The goal of this tutorial is to demonstrate the mechanics of performing genome-wide analyses using variant call data stored with TileDB-VCF and how such analyses can be easily scaled using TileDB Cloud’s serverless computation platform.\nTo get started we’ll load a few required packages and define several variables that we’ll refer to throughout the notebook."
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#packages",
    "href": "examples/tutorial_tiledbvcf_gwas.html#packages",
    "title": "",
    "section": "Packages",
    "text": "Packages\n\nimport tiledb\nimport tiledbvcf\nimport tiledb.cloud\nfrom tiledb.cloud.compute import Delayed\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nprint(\n    f\"tileDB-vcf v{tiledbvcf.version}\\n\"\n    f\"tileDB-cloud v{tiledb.cloud.version.version}\"\n)\n\ntileDB-vcf v0.22.1.dev27\ntileDB-cloud v0.10.0"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#variables",
    "href": "examples/tutorial_tiledbvcf_gwas.html#variables",
    "title": "",
    "section": "Variables",
    "text": "Variables\n\n# variables\ngenome = \"hg19\"\narray_uri = \"tiledb://TileDB-Inc/vcf-1kg-phase3\"\nsample_array = \"tiledb://TileDB-Inc/vcf-1kg-sample-metadata\"\n\n# vcf attributes to include\nattrs = [\n    \"sample_name\", \n    \"contig\",\n    \"pos_start\",\n    \"pos_end\", \n    \"fmt_GT\"\n]\n\n\n!tiledbvcf stat --uri tiledb://TileDB-Inc/vcf-1kg-phase3\n\n/bin/bash: line 1: tiledbvcf: command not found"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#vcf_snp_query",
    "href": "examples/tutorial_tiledbvcf_gwas.html#vcf_snp_query",
    "title": "",
    "section": "1. vcf_snp_query()",
    "text": "1. vcf_snp_query()\n\nqueries a specified window from the genome\nfilters the sites based on the defined criteria\n\n\n%%time\nres = tiledb.cloud.udf.exec(\n    func = \"aaronwolen/vcf_snp_query\",\n    uri = array_uri,\n    attrs = attrs,\n    regions = bed_regions[:2],\n)\n\nres.to_pandas()\n\nCPU times: user 94.3 ms, sys: 62.6 ms, total: 157 ms\nWall time: 9.48 s\n\n\n\n\n\n  \n    \n      \n      sample_name\n      contig\n      pos_start\n      pos_end\n      fmt_GT\n    \n  \n  \n    \n      0\n      HG00097\n      20\n      60828\n      60828\n      [0, 1]\n    \n    \n      1\n      HG00101\n      20\n      61098\n      61098\n      [0, 1]\n    \n    \n      2\n      HG00103\n      20\n      61098\n      61098\n      [1, 0]\n    \n    \n      3\n      HG00109\n      20\n      61098\n      61098\n      [0, 1]\n    \n    \n      4\n      HG00111\n      20\n      61098\n      61098\n      [0, 1]\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      577809\n      NA21142\n      20\n      195696\n      195696\n      [1, 1]\n    \n    \n      577810\n      NA21143\n      20\n      195696\n      195696\n      [1, 1]\n    \n    \n      577811\n      NA21144\n      20\n      195696\n      195696\n      [1, 1]\n    \n    \n      577812\n      NA21142\n      20\n      196604\n      196604\n      [0, 1]\n    \n    \n      577813\n      NA21144\n      20\n      196604\n      196604\n      [1, 0]\n    \n  \n\n551155 rows × 5 columns"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#filter_variants",
    "href": "examples/tutorial_tiledbvcf_gwas.html#filter_variants",
    "title": "",
    "section": "2. filter_variants()",
    "text": "2. filter_variants()\n\ncalculates allele count and minor allele frequencies\noptinally filters for variants using the maf threshold\n\n\n%%time\nres2 = tiledb.cloud.udf.exec(\n    func = \"aaronwolen/filter_variants\", \n    df = res,\n    maf = (0.05, 0.95),\n)\n\nres2.to_pandas()\n\nCPU times: user 183 ms, sys: 108 ms, total: 292 ms\nWall time: 7.69 s\n\n\n\n\n\n  \n    \n      \n      \n      sample_name\n      pos_end\n      fmt_GT\n      dose\n    \n    \n      contig\n      pos_start\n      \n      \n      \n      \n    \n  \n  \n    \n      20\n      61098\n      HG00101\n      61098\n      [0, 1]\n      1\n    \n    \n      61098\n      HG00103\n      61098\n      [1, 0]\n      1\n    \n    \n      61098\n      HG00109\n      61098\n      [0, 1]\n      1\n    \n    \n      61098\n      HG00111\n      61098\n      [0, 1]\n      1\n    \n    \n      61098\n      HG00115\n      61098\n      [1, 0]\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      195696\n      NA21142\n      195696\n      [1, 1]\n      2\n    \n    \n      195696\n      NA21143\n      195696\n      [1, 1]\n      2\n    \n    \n      195696\n      NA21144\n      195696\n      [1, 1]\n      2\n    \n    \n      196604\n      NA21142\n      196604\n      [0, 1]\n      1\n    \n    \n      196604\n      NA21144\n      196604\n      [1, 0]\n      1\n    \n  \n\n488449 rows × 4 columns"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#calc_gwas",
    "href": "examples/tutorial_tiledbvcf_gwas.html#calc_gwas",
    "title": "",
    "section": "3. calc_gwas()",
    "text": "3. calc_gwas()\nReshapes the Dataframe of variants into a variant by sample matrix and performs a chi-squared analysis on site for the included trait\n\n%%time\nres3 = tiledb.cloud.udf.exec(\n    func = \"aaronwolen/calc_gwas\",\n    df = res2,\n    trait = df_samples.is_female,\n)\n\nres3.to_pandas()\n\nCPU times: user 90.5 ms, sys: 23 ms, total: 113 ms\nWall time: 9.66 s\n\n\n\n\n\n  \n    \n      \n      contig\n      pos_start\n      oddsratio\n      pvalue\n    \n  \n  \n    \n      0\n      20\n      61098\n      0.494083\n      0.482112\n    \n    \n      1\n      20\n      61138\n      0.289018\n      0.590851\n    \n    \n      2\n      20\n      61795\n      0.068686\n      0.793260\n    \n    \n      3\n      20\n      63231\n      0.851720\n      0.356066\n    \n    \n      4\n      20\n      63244\n      0.004420\n      0.946994\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      423\n      20\n      198965\n      0.150694\n      0.697873\n    \n    \n      424\n      20\n      198977\n      0.025577\n      0.872937\n    \n    \n      425\n      20\n      199078\n      0.000250\n      0.987379\n    \n    \n      426\n      20\n      199114\n      0.000250\n      0.987379\n    \n    \n      427\n      20\n      199986\n      0.000250\n      0.987379\n    \n  \n\n428 rows × 4 columns"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#combine_results",
    "href": "examples/tutorial_tiledbvcf_gwas.html#combine_results",
    "title": "",
    "section": "4. combine_results()",
    "text": "4. combine_results()\n\ncreates a single dataframe containing individual results from each partition\n\n\ndef combine_results(dfs):\n    \"\"\"\n    Combine GWAS Results\n\n    :param list of dataframes: Region-specific GWAS results to combine\n    \"\"\"\n    import pyarrow as pa\n    print(f\"Input list contains {len(dfs)} Arrow tables\")\n    out = pa.concat_tables([x for x in dfs if x is not None])\n    return out"
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#summary",
    "href": "examples/tutorial_tiledbvcf_gwas.html#summary",
    "title": "",
    "section": "Summary",
    "text": "Summary\nHere we’ve demonstrated how genome-wide pipelines can be encapsulated as distinct tasks, distributed as UDFs, and easily deployed in parallel at scale."
  }
]