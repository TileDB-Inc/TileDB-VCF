[
  {
    "objectID": "documentation/data-model.html",
    "href": "documentation/data-model.html",
    "title": "",
    "section": "",
    "text": "The Solution section provides a high-level overview of how and why TileDB-VCF uses 3D sparse arrays to store genomic variant data. What follows are the technical implementation details about the underlying arrays, including their schemas, dimensions, tiling order, attributes, metadata.\n\n\nA TileDB-VCF dataset is composed of a group of two or more separate TileDB arrays:\n\na 3D sparse array for the actual genomic variants and associated fields/attributes\na 1D sparse array for the metadata stored in each single-sample VCF header\n\n\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nArray type\nSparse\n\n\nRank\n3D\n\n\nCell order\nRow-major\n\n\nTile order\nRow-major\n\n\n\n\n\n\nThe dimensions in the schema are:\n\n\n\nDimension Name\nTileDB Datatype\nCorresponding VCF Field\n\n\n\n\ncontig\nTILEDB_STRING_ASCII\nCHR\n\n\nstart_pos\nuint32_t\nVCFPOSplus TileDB anchors\n\n\nsample\nTILEDB_STRING_ASCII\nSample name\n\n\n\nAs mentioned before, the coordinates of the 3D array are contig along the first dimension, chromosomal location of the variants start position along the second dimension, and sample names along the third dimension.\n\n\n\nFor each field in a single-sample VCF record there is a corresponding attribute in the schema.\n\n\n\n\n\n\n\n\nAttribute Name\nTileDB Datatype\nDescription\n\n\n\n\nend_pos\nuint32_t\nVCF END position of VCF records\n\n\nqual\nfloat\nVCF QUAL field\n\n\nalleles\nvar<char>\nCSV list of REF and ALT VCF fields\n\n\nid\nvar<char>\nVCF ID field\n\n\nfilter_ids\nvar<int32_t>\nVector of integer IDs of entries in the FILTER VCF field\n\n\nreal_start_pos\nuint32_t\nVCF POS(no anchors)\n\n\ninfo\nvar<uint8_t>\nByte blob containing any INFO fields that are not stored as explicit attributes\n\n\nfmt\nvar<uint8_t>\nByte blob containing any FMT fields that are not stored as explicit attributes\n\n\ninfo_*\nvar<uint8_t>\nOne or more attributes storing specific VCF INFO fields, e.g. info_DP, info_MQ, etc.\n\n\nfmt_*\nvar<uint8_t>\nOne or more attributes storing specific VCF FORMAT fields, e.g. fmt_GT, fmt_MIN_DP, etc.\n\n\n\nThe info_* and fmt_* attributes allow individual INFO or FMT VCF fields to be extracted into explicit array attributes. This can be beneficial if your queries frequently access only a subset of the INFO or FMT fields, as no unrelated data then needs to be fetched from storage.\n{% hint style=“info” %} The choice of which fields to extract as explicit array attributes is user-configurable during array creation. {% endhint %}\nAny extra info or format fields not extracted as explicit array attributes are stored in the byte blob attributes, info and fmt.\n\n\n\n\nanchor_gap Anchor gap value\nextra_attributes List of INFO or FMT field names that are stored as explicit array attributes\nversion Array schema version\n\nThese metadata values are updated during array creation, and are used during the export phase. The metadata is stored as “array metadata” in the sparse data array.\n{% hint style=“warning” %} When ingesting samples, the sample header must be identical for all samples with respect to the contig mappings. That means all samples must have the exact same set of contigs listed in the VCF header. This requirement will be relaxed in future versions. {% endhint %}\n\n\n\n\nThe vcf_headers array stores the original text of every ingested VCF header in order to:\n\nensure the original VCF file can be fully recovered for any given sample\nreconstruct an htslib header instance when reading from the dataset, which is used for operations such as mapping a filter ID back to the filter string, etc.\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nArray type\nSparse\n\n\nRank\n1D\n\n\nCell order\nRow-major\n\n\nTile order\nRow-major\n\n\n\n\n\n\n\n\n\nDimension Name\nTileDB Datatype\nDescription\n\n\n\n\nsample\nTILEDB_STRING_ASCII\nSample name\n\n\n\n\n\n\n\n\n\nAttribute Name\nTileDB Datatype\nDescription\n\n\n\n\nheader\nvar<char>\nOriginal text of the VCF header\n\n\n\n\n\n\n\nTo summarize, we’ve described three main entities:\n\nThe variant data array (3D sparse)\nThe general metadata, stored in the variant data array as metadata\nThe VCF header array (1D sparse)\n\nAll together we term this a “TileDB-VCF dataset.” Expressed as a directory hierarchy, a TileDB-VCF dataset has the following structure:\n<dataset_uri>/\n  |_ __tiledb_group.tdb\n  |_ data/\n      |_ __array_schema.tdb\n      |_ __meta/\n            |_ <general-metadata-here>\n      ... <other array directories/fragments and files>\n  |_ vcf_headers/\n      |_ __array_schema.tdb\n      ... <other array directories/fragments and files>\nThe root of the dataset, <dataset_uri> is a TileDB group. The data member is the TileDB 3D sparse array storing the variant data. This array stores the general TileDB-VCF metadata as its array metadata in folder data/__meta. The vcf_headers member is the TileDB 1D sparse array containing the VCF header data.\n\n\n\nDuring array creation, there are several array-related parameters that the user can control. These are:\n\nArray data tile capacity (default 10,000)\nThe “anchor gap” size (default 1,000)\nThe list of INFO and FMT fields to store as explicit array attributes (default is none).\n\nOnce chosen, these parameters cannot be changed.\nDuring sample ingestion, the user can specify the:\n\nSample batch size (default 10)\n\nThe above parameters may impact read and write performance, as well as the size of the persisted array. Therefore, some care should be taken to determine good values for these parameters before ingesting a large amount of data into an array."
  },
  {
    "objectID": "documentation/index.html#features",
    "href": "documentation/index.html#features",
    "title": "TileDB-VCF",
    "section": "Features",
    "text": "Features\n\nEasily ingest large amounts of variant-call data at scale\nSupports ingesting single sample VCF and BCF files\nNew samples are added incrementally, avoiding computationally expensive merging operations\nAllows for highly compressed storage using TileDB sparse arrays\nEfficient, parallelized queries of variant data stored locally or remotely on S3\nExport lossless VCF/BCF files or extract specific slices of a dataset"
  },
  {
    "objectID": "documentation/index.html#whats-included",
    "href": "documentation/index.html#whats-included",
    "title": "TileDB-VCF",
    "section": "What’s Included?",
    "text": "What’s Included?\n\nCommand line interface (CLI)\nAPIs for C, C++, Python, and Java\nIntegrates with Spark and Dask"
  },
  {
    "objectID": "documentation/index.html#quick-start",
    "href": "documentation/index.html#quick-start",
    "title": "TileDB-VCF",
    "section": "Quick Start",
    "text": "Quick Start\nThe documentation website provides comprehensive usage examples but here are a few quick exercises to get you started.\nWe’ll use a dataset that includes 20 synthetic samples, each one containing over 20 million variants. We host a publicly accessible version of this dataset on S3, so if you have TileDB-VCF installed and you’d like to follow along just swap out the uri’s below for s3://tiledb-inc-demo-data/tiledbvcf-arrays/v4/vcf-samples-20. And if you don’t have TileDB-VCF installed yet, you can use our Docker images to test things out.\n\nCLI\nExport complete chr1 BCF files for a subset of samples:\ntiledbvcf export \\\n  --uri vcf-samples-20 \\\n  --regions chr1:1-248956422 \\\n  --sample-names v2-usVwJUmo,v2-WpXCYApL\nCreate a TSV file containing all variants within one or more regions of interest:\ntiledbvcf export \\\n  --uri vcf-samples-20 \\\n  --sample-names v2-tJjMfKyL,v2-eBAdKwID \\\n  -Ot --tsv-fields \"CHR,POS,REF,S:GT\" \\\n  --regions \"chr7:144000320-144008793,chr11:56490349-56491395\"\n\n\nPython\nRunning the same query in python\nimport tiledbvcf\n\nds = tiledbvcf.Dataset(uri = \"vcf-samples-20\", mode=\"r\")\n\nds.read(\n    attrs = [\"sample_name\", \"pos_start\", \"fmt_GT\"],\n    regions = [\"chr7:144000320-144008793\", \"chr11:56490349-56491395\"],\n    samples = [\"v2-tJjMfKyL\", \"v2-eBAdKwID\"]\n)\nreturns results as a pandas DataFrame\n     sample_name  pos_start    fmt_GT\n0    v2-nGEAqwFT  143999569  [-1, -1]\n1    v2-tJjMfKyL  144000262  [-1, -1]\n2    v2-tJjMfKyL  144000518  [-1, -1]\n3    v2-nGEAqwFT  144000339  [-1, -1]\n4    v2-nzLyDgYW  144000102  [-1, -1]\n..           ...        ...       ...\n566  v2-nGEAqwFT   56491395    [0, 0]\n567  v2-ijrKdkKh   56491373    [0, 0]\n568  v2-eBAdKwID   56491391    [0, 0]\n569  v2-tJjMfKyL   56491392  [-1, -1]\n570  v2-nzLyDgYW   56491365  [-1, -1]"
  },
  {
    "objectID": "documentation/index.html#want-to-learn-more",
    "href": "documentation/index.html#want-to-learn-more",
    "title": "TileDB-VCF",
    "section": "Want to Learn More?",
    "text": "Want to Learn More?\n\nBlog “Population Genomics is a Data Management Problem”\nCheck out the full documentation\n\nWhy use TileDB-VCF?\nData Model\nInstallation\nHow To\nReference"
  },
  {
    "objectID": "documentation/ingestion/overview.html",
    "href": "documentation/ingestion/overview.html",
    "title": "Ingestion",
    "section": "",
    "text": "Ingestion of VCF files is easy and straight forward."
  },
  {
    "objectID": "documentation/ingestion/cli.html",
    "href": "documentation/ingestion/cli.html",
    "title": "CLI",
    "section": "",
    "text": "{% hint style=“success” %} The files used in the following examples can be obtained here. {% endhint %}\nThe first step is to create an empty dataset. Let’s save the dataset in a new array called small_dataset:\ntiledbvcf create --uri small_dataset"
  },
  {
    "objectID": "documentation/ingestion/cli.html#store-samples",
    "href": "documentation/ingestion/cli.html#store-samples",
    "title": "CLI",
    "section": "Store samples",
    "text": "Store samples\n\nIngest from local storage\nWe’ll start with a small example using 3 synthetic VCF files, assuming they are locally available in a folder data/vcfs:\ntree data\n## data\n## ├── gene-promoters-hg38.bed\n## ├── s3-bcf-samples.txt\n## └── vcfs\n##     ├── G1.vcf.gz\n##     ├── G1.vcf.gz.csi\n##     ├── G2.vcf.gz\n##     ├── G2.vcf.gz.csi\n##     ├── G3.vcf.gz\n##     └── G3.vcf.gz.csi\n##\n## 1 directory, 8 files\nIndex files are required for ingestion. If your VCF/BCF files have not been indexed you can use bcftools to do so:\nfor f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\nWe can ingest these files into small_dataset as follows:\ntiledbvcf store --uri small_dataset data/vcfs/G*.vcf.gz\nThat’s it! Let’s verify everything went okay using the stat command to provide high-level statistics about our dataset including the number of samples it contains and the variant attributes it includes.\ntiledbvcf stat --uri small_dataset\n## Statistics for dataset 'small_dataset':\n## - Version: 4\n## - Tile capacity: 10,000\n## - Anchor gap: 1,000\n## - Number of registered samples: 3\n## - Extracted attributes: none\nAt this point you have successfully created and populated a TileDB VCF dataset using data stored locally on your machine. Next we’ll look at the more common scenario of working with files stored on a cloud object store.\n\n\nIngest from S3\nTileDB Embedded’s native cloud features make it possible to ingest samples directly from remote locations. Here, we’ll ingest the following samples located on AWS S3:\ncat data/s3-bcf-samples.txt\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf\n{% hint style=“info” %} Samples in this second batch are stored as BCF files which are also supported by TileDB-VCF. {% endhint %}\nThis process is identical to the steps perfo_r_med above, the only changes needed to our code involve setting --scratch-mb to allocate some temporary space for downloading the files and providing the URLs for the remote files. In this case, we’ll simply pass the s3-bcf-samples.txt file, which includes a list of the BCF files we want to ingest.\n{% hint style=“info” %} When ingesting samples from S3, you must configure enough scratch space to hold at least 20 samples. In general, you need 2 × the sample dimension’s sample_bactch_size (which by default is 10). You can read more about the data model here. {% endhint %}\nYou can add the --verbose flag to print out more information during the store phase.\ntiledbvcf store \\\n  --uri small_dataset \\\n  --samples-file data/s3-bcf-samples.txt \\\n  --verbose\n  \n## Initialization completed in 3.17565 sec.\n## ...\n## Done. Ingested 1,391 records (+ 69,548 anchors) from 7 samples in 10.6751\n## seconds.\nConsolidating and vacuuming fragment metadata and commits are recommended after creating a new dataset or adding several new samples to an existing dataset.\ntiledbvcf utils consolidate fragment_meta --uri small_dataset\ntiledbvcf utils consolidate commits --uri small_dataset\ntiledbvcf utils vacuum fragment_meta --uri small_dataset\ntiledbvcf utils vacuum commits --uri small_dataset"
  },
  {
    "objectID": "documentation/ingestion/cli.html#incremental-updates",
    "href": "documentation/ingestion/cli.html#incremental-updates",
    "title": "CLI",
    "section": "Incremental Updates",
    "text": "Incremental Updates\nA key advantage to using TileDB as a data store for genomic variant data is the ability to efficiently add new samples as they become available. The dataset creation command should be called once. Then you can invoke the store command multiple commands.\n{% hint style=“success” %} The store command is thread- and process-safe, even on the cloud. That means it can be invoked in parallel, arbitrarily scaling out the ingestion of massive datasets. {% endhint %}\nSuppose we run the store for the first 3 local samples, followed by the store command for the 7 samples stored on the S3. If we run the stat command, we can verify that our dataset now includes 10 samples.\ntiledbvcf stat --uri small_dataset\n## Statistics for dataset 'small_dataset':\n## - Version: 4\n## - Tile capacity: 10,000\n## - Anchor gap: 1,000\n## - Number of registered samples: 10\n## - Extracted attributes: none\nBecause TileDB is designed to be updatable, the store process happens efficiently and without ever touching any previously or concurrently ingested data, avoiding computationally expensive operations like regenerating combined VCF files."
  },
  {
    "objectID": "documentation/ingestion/python.html",
    "href": "documentation/ingestion/python.html",
    "title": "Python",
    "section": "",
    "text": "Similar to TileDB-VCF’s command-line interface (CLI), tiledbvcf supports ingesting VCF (or BCF) files into TileDB, either when creating a new dataset or updating an existing dataset with additional samples. See the CLI Usage for a more detailed description of the ingestion process. Here, we’ll only focus on the mechanics of ingestion from Python.\nThe text file data/s3-bcf-samples.txt contains a list of S3 URIs pointing to 7 BCF files from the same cohort.\nwith open(\"data/s3-bcf-samples.txt\") as f:\n    sample_uris = [l.rstrip(\"\\n\") for l in f.readlines()]\nsample_uris\n## ['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf']\nYou can add them to your existing dataset by re-opening it in write mode and providing the file URIs. It’s also necessary to allocate scratch space so the files can be downloaded to a temporary location prior to ingestion.\nsmall_ds = tiledbvcf.Dataset('small_dataset', mode = \"w\")\nsmall_ds.ingest_samples(sample_uris)\nThe TileDB-VCF dataset located at small_dataset now includes records for 660 variants across 10 samples. The next section provides examples demonstrating how to query this dataset."
  },
  {
    "objectID": "documentation/the-solution.html",
    "href": "documentation/the-solution.html",
    "title": "",
    "section": "",
    "text": "Population variant data can be efficiently represented using a 3D sparse array. For each sample, imagine a 2D plane where the vertical axis is the contig and the horizontal axis is the genomic position. Every variant can be represented as a range within this plane; it can be unary (i.e., a SNP) or it can be a longer range (e.g., INDEL or CNV). Each sample is then indexed by a third dimension, which is unbounded to accommodate populations of any size. The figure below shows an example for one sample, with several variants distributed across contigs chr1 , chr2 and chr3.\n\n\n\n3D array representation of population variants\n\n\nIn TileDB-VCF, we represent the start position of each range as a non-empty cell in a sparse array (black squares in the above figure). In each of those array cells, we store the end position of each cell (to create a range) along with all other fields in the corresponding single-sample VCF files for each variant (e.g., REF, ALT, etc.). Therefore, for every sample, we map variants to 2D non-empty sparse array cells.\nTo facilitate rapid retrieval of interval intersections (explained in the next section), we also inject anchors (green square in the above figure) to breakup long ranges. Specifically, we create a new non-empty cell every anchor_gap bases from the start of the range (where anchor_gap is a user-defined parameter), which is identical to the range cell, except that (1) it has a new start coordinate and (2) it stores the real start position in an attribute.\nNote that regardless of the number of samples, we do not inject any additional information other than that of the anchors, which is user configurable and turns out to be negligible for real datasets. In other words, this solution leads to linear storage in the number of samples, thus being scalable.\n\n\n\nThe typical access pattern used for variant data involves one or more rectangles covering a set of genomic ranges across one or more samples. In the figure below, let the black rectangle be the user’s query. Observe that the results are highlighted in blue (v1, v2, v4, v7). However, the rectangle misses v1, i.e., the case where an Indel/CNV range intersects the query rectangle, but the start position is outside the rectangle.\n\n\n\nInterval intersection using expanded query ranges and anchors\n\n\nThis is the motivation behind anchors. TileDB-VCF expands the user’s query range on the left by anchor_gap. It then reports as results the cells that are included in the expanded query if their end position (stored in an attribute) comes after the query range start endpoint. In the example above, TileDB-VCF retrieves anchor a1 and Indel/CNV v3. It reports v1 as a result (as it can be extracted directly from anchor a1), but filters out v3.\n{% hint style=“info” %} By representing each sample’s variants as non-empty cells in a 2D plane and using anchors and expanded queries, we managed to model population variants as 3D sparse arrays and use the vanilla functionality of TileDB-Embedded, inheriting all its powerful features out of the box. {% endhint %}\nQuite often, the analyses requires data retrieval based on numerous query ranges (up to the order of millions), which must be submitted simultaneously. TileDB-VCF leverages the highly optimized multi-range subarray functionality of TileDB Embedded, which leads to unparalleled performance for such scenarios.\nBut what about updates? That’s the topic of the next section.\n\n\n\nTileDB-VCF is based on TileDB Embedded which supports rapid updates via immutable fragments. That means that every time a new batch of samples is added to the 3D array, the previous contents of the array are not modified at all. Each batch write operation is totally independent and _lock-free—_any number of threads or processes can write simultaneously without synchronization, while ensuring consistency. With TileDB-VCF, both update time and storage size scales linearly with the number of new samples, solving the N+1 problem.\n\n\n\nTileDB-VCF allows you to efficiently store, update and access enormous population genomic datasets, all open-source. And although it is a C++ embedded library (which works on a single machine), it also integrates very well with Spark and Dask, enabling you to scale your analysis to large clusters. However, the burden of managing clusters (spinning them up, deploying the software, monitoring the resources, etc.), falls entirely on you, which is quite an undertaking.\nEnter TileDB Cloud!\nTileDB Cloud allows you to perform parallel ingestion and parallel slicing / processing, 100% serverless. This means that you do not have to spin up large clusters or pay for idle time. You can easily slice data or define complex task flows comprised of thousands of tasks, which TileDB Cloud deploys elastically in its serverless infrastructure, providing an unmatched combination of ease of use and low cost on the cloud—even for your most challenging analyses.\n\n\n\n{% hint style=“info” %} The shear volume of data generated by modern population genomics applications creates enormous challenges around data management and collaboration. {% endhint %}\nTileDB Cloud offers groundbreaking features for collaborative genomic research:\n\nPopular public datasets made available in the TileDB format for direct analysis\nEasy mechanism for one to share their data and code (Jupyter notebooks and user-defined functions), either with a specific set of users or the entire world\nEasy ways to explore and discover public data and code.\n\nJoin our vision to build a growing community around open data and code!\n\n\n\nIf you are familiar with the TileDB Embedded data model, you can delve into the technical details of how TileDB-VCF stores genomic variant data using [D sparse arrays. Otherwise you can proceed with installation instructions and tutorials."
  }
]