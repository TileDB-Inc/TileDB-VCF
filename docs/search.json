[
  {
    "objectID": "examples/overview.html",
    "href": "examples/overview.html",
    "title": "Overview",
    "section": "",
    "text": "This section provides fully runnable examples."
  },
  {
    "objectID": "examples/overview.html#tiledb-vcf-basics",
    "href": "examples/overview.html#tiledb-vcf-basics",
    "title": "Overview",
    "section": "TileDB-VCF Basics",
    "text": "TileDB-VCF Basics\nnotebook: tutorial_tiledbvcf_basics\n\nOverview\nProvides a high-level overview of how to work with VCF data using TileDB. You’ll see how to use TileDB-VCF’s Python package to:\n\ncreate and populate new TileDB-VCF datasets\nquery variant data by genomic region and/or sample and selectively retrieve specific VCF attributes\nparallelize queries by partitioning the data and distributing the queries across multiple processes\neasily scale queries using TileDB’s serverless infrastructure## Introduction"
  },
  {
    "objectID": "examples/overview.html#tiledb-vcf-allele-frequencies",
    "href": "examples/overview.html#tiledb-vcf-allele-frequencies",
    "title": "Overview",
    "section": "TileDB-VCF Allele Frequencies",
    "text": "TileDB-VCF Allele Frequencies\nnotebook: tutorial_tiledbvcf_allele_frequencies\n\nOverview\nProvides a high-level overview of how to work with allele frequencies as part of TileDB-VCF. You’ll see how to use TileDB-VCF’s Python package to:\n\nSelecting allele frequencies\nFiltering on allele frequencies\nDirect allele frequency access"
  },
  {
    "objectID": "examples/overview.html#tiledb-vcf-task-graph-based-genome-wide-analysis",
    "href": "examples/overview.html#tiledb-vcf-task-graph-based-genome-wide-analysis",
    "title": "Overview",
    "section": "TileDB-VCF Task Graph-Based Genome-wide Analysis",
    "text": "TileDB-VCF Task Graph-Based Genome-wide Analysis\nnotebook: tutorial_tiledbvcf_gwas\n\nOverview\nIn this notebook we’ll perform a rudimentary genome-wide association study using the 1000 Genomes (1KG) dataset. The goal of this tutorial is to demonstrate the mechanics of performing genome-wide analyses using variant call data stored with TileDB-VCF and how such analyses can be easily scaled using TileDB’s serverless computation platform."
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_allele_frequencies.html",
    "href": "examples/tutorial_tiledbvcf_allele_frequencies.html",
    "title": "TileDB-VCF Allele Frequencies",
    "section": "",
    "text": "This notebook will walk you through\nThis notebook uses the TileDB-Inc/vcf-1kghicov-dragen-v376 version of the 1000 genome project.\nimport tiledbvcf\nimport tiledb\nuri = \"tiledb://TileDB-Inc/vcf-1kghicov-dragen-v376\"",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Allele Frequencies"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_allele_frequencies.html#including-allele-frequency-field",
    "href": "examples/tutorial_tiledbvcf_allele_frequencies.html#including-allele-frequency-field",
    "title": "TileDB-VCF Allele Frequencies",
    "section": "Including Allele Frequency Field",
    "text": "Including Allele Frequency Field\nThe computed allele frequency can be included in results with the info_TILEDB_IAF attribute\n\nattributes = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"fmt_GT\", \"info_TILEDB_IAF\"]\n\nds = tiledbvcf.Dataset(uri, mode=\"r\")\n\n\n# Query for BTD across all 3404 samples\ndf = ds.read(\n    attrs=attributes,\n    regions=[\"chr3:15601341-15722311\"],\n)\n\n\ndf\n\n\n\n\n\n\n\n\nsample_name\ncontig\npos_start\npos_end\nalleles\nfmt_GT\ninfo_TILEDB_IAF\n\n\n\n\n0\nHG00096\nchr3\n15601536\n15601536\n[A, G]\n[1, 1]\nNone\n\n\n1\nHG00097\nchr3\n15601536\n15601536\n[A, G]\n[1, 1]\nNone\n\n\n2\nHG00099\nchr3\n15601536\n15601536\n[A, G]\n[1, 1]\nNone\n\n\n3\nHG00100\nchr3\n15601536\n15601536\n[A, G]\n[1, 1]\nNone\n\n\n4\nHG00101\nchr3\n15601536\n15601536\n[A, G]\n[0, 1]\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n606391\nNA21143\nchr3\n15721413\n15721413\n[C, T]\n[0, 1]\nNone\n\n\n606392\nNA21144\nchr3\n15721413\n15721413\n[C, T]\n[1, 1]\nNone\n\n\n606393\nNA21144\nchr3\n15721470\n15721470\n[T, C]\n[0, 1]\nNone\n\n\n606394\nNA21143\nchr3\n15721933\n15721933\n[C, T]\n[0, 1]\nNone\n\n\n606395\nNA21144\nchr3\n15722066\n15722066\n[A, G]\n[0, 1]\nNone\n\n\n\n\n606396 rows × 7 columns",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Allele Frequencies"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_allele_frequencies.html#filtering-on-iaf",
    "href": "examples/tutorial_tiledbvcf_allele_frequencies.html#filtering-on-iaf",
    "title": "TileDB-VCF Allele Frequencies",
    "section": "Filtering on IAF",
    "text": "Filtering on IAF\nFilters for allele frequency can also be included by using the set_af_filter parameter.\n\n# Query for BTD across all 3404 samples\ndf = ds.read(\n    attrs=attributes,\n    regions=[\"chr3:15601341-15722311\"],\n    set_af_filter=\"&lt;0.5\",\n)\n\n\ndf\n\n\n\n\n\n\n\n\nsample_name\ncontig\npos_start\npos_end\nalleles\nfmt_GT\ninfo_TILEDB_IAF\n\n\n\n\n0\nHG00101\nchr3\n15601536\n15601536\n[A, G]\n[0, 1]\n[0.027682202, 0.9723178]\n\n\n1\nHG00100\nchr3\n15601668\n15601668\n[G, A]\n[0, 1]\n[0.43612567, 0.56387436]\n\n\n2\nHG00100\nchr3\n15602568\n15602568\n[A, G]\n[0, 1]\n[0.42662117, 0.57337886]\n\n\n3\nHG00100\nchr3\n15602688\n15602688\n[G, A]\n[0, 1]\n[0.47108433, 0.52891564]\n\n\n4\nHG00096\nchr3\n15603161\n15603161\n[A, G]\n[0, 1]\n[0.44444445, 0.5555556]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n370583\nNA21144\nchr3\n15721189\n15721189\n[C, G]\n[0, 1]\n[0.43555242, 0.5644476]\n\n\n370584\nNA21143\nchr3\n15721340\n15721340\n[G, A]\n[0, 1]\n[0.3335806, 0.6664194]\n\n\n370585\nNA21143\nchr3\n15721413\n15721413\n[C, T]\n[0, 1]\n[0.33333334, 0.6666667]\n\n\n370586\nNA21144\nchr3\n15721470\n15721470\n[T, C]\n[0, 1]\n[0.4868421, 0.5131579]\n\n\n370587\nNA21144\nchr3\n15722066\n15722066\n[A, G]\n[0, 1]\n[0.4359155, 0.56408453]\n\n\n\n\n370588 rows × 7 columns",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Allele Frequencies"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_allele_frequencies.html#accessing-allele-frequencies-directly",
    "href": "examples/tutorial_tiledbvcf_allele_frequencies.html#accessing-allele-frequencies-directly",
    "title": "TileDB-VCF Allele Frequencies",
    "section": "Accessing Allele Frequencies Directly",
    "text": "Accessing Allele Frequencies Directly\nAllele frequencies can also be queried directly in addition to as part of the variant query\n\n# Get the variant stats ur\nwith tiledb.Group(uri) as g:\n    alleles_uri  = g[\"variant_stats\"].uri\n\n\n# BRCA1 from https://www.ncbi.nlm.nih.gov/gene/672\ncontig = 'chr17'\nregion = slice(43_036_174, 43_133_600)  # pos is 0 based\n\n\n# Query allele frequencies and get results as a pandas dataframe\n\nwith tiledb.open(alleles_uri) as A:\n    df = A.query(attrs=[\"ac\", \"allele\"], dims=[\"pos\", \"contig\"]).df[contig, region]\n\n\n# Summarize frequencies\n\ndef calc_af(df):\n    \"\"\"Consolidate AC and compute AN, AF\"\"\"\n    # Allele Count (AC) = sum of all AC at the same locus\n    # This step consolidates ACs from all ingested batches\n    df = df.groupby([\"pos\", \"allele\"], sort=True).sum()\n\n    # Allele Number (AN) = sum of AC at the same locus\n    an = df.groupby([\"pos\"], sort=True).ac.sum().rename(\"an\") \n    df = df.join(an, how=\"inner\")\n    \n    # Allele Frequency (AF) = AC / AN\n    df[\"af\"] = df.ac / df.an\n    return df\n\ncalc_af(df)\n\n\n\n\n\n\n\n\n\nac\nan\naf\n\n\npos\nallele\n\n\n\n\n\n\n\n43036179\nA\n33\n66\n0.500000\n\n\nG\n33\n66\n0.500000\n\n\n43036181\nA\n1\n2\n0.500000\n\n\nG\n1\n2\n0.500000\n\n\n43036308\nC\n238\n528\n0.450758\n\n\n...\n...\n...\n...\n...\n\n\n43133528\nC\n5\n12\n0.416667\n\n\nT\n6\n12\n0.500000\n\n\nTC\n1\n12\n0.083333\n\n\n43133556\nC\n1\n2\n0.500000\n\n\nT\n1\n2\n0.500000\n\n\n\n\n8292 rows × 3 columns",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Allele Frequencies"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html",
    "href": "documentation/api-reference/cli.html",
    "title": "CLI",
    "section": "",
    "text": "TileDB-VCF -- Efficient variant-call data storage and retrieval.\n\n  This command-line utility provides an interface to create, store and\n  efficiently retrieve variant-call data in the TileDB storage format.\n\n  More information: TileDB-VCF &lt;https://tiledb-inc.github.io/TileDB-VCF&gt;\n\nUsage: tiledbvcf [OPTIONS] SUBCOMMAND\n\nOptions:\n  -h,--help                             Print this help message and exit\n  -v,--version                          Print the version information and exit\n\nSubcommands:\n  create                                Creates an empty TileDB-VCF dataset\n  store                                 Ingests samples into a TileDB-VCF dataset\n  delete                                Delete samples from a TileDB-VCF dataset\n  export                                Exports data from a TileDB-VCF dataset\n  list                                  Lists all sample names present in a TileDB-VCF dataset\n  stat                                  Prints high-level statistics about a TileDB-VCF dataset\n  utils                                 Utils for working with a TileDB-VCF dataset\n  version                               Print the version information and exit\n\n\nCreates an empty TileDB-VCF dataset\n\nUsage: tiledbvcf create [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  -a,--attributes TEXT=[] ... Excludes: --vcf-attributes\n                                        INFO and/or FORMAT field names (comma-delimited) to store as separate attributes.\n                                        Names should be 'fmt_X' or 'info_X' for a field name 'X' (case sensitive).\n  -v,--vcf-attributes TEXT Excludes: --attributes\n                                        Create separate attributes for all INFO and FORMAT fields in the provided VCF file.\n  -g,--anchor-gap UINT=1000             Anchor gap size to use\n  -n,--no-duplicates                    Allow records with duplicate start positions to be written to the array.\n  --compress-sample-dim,--no-compress-sample-dim{false}\n                                        Enable/disable compression of the sample dimension. Enabled by default.\n\nIngestion task options:\n  --enable-allele-count,--disable-allele-count{false}\n                                        Enable/disable allele count array creation. Enabled by default.\n  --enable-variant-stats,--disable-variant-stats{false}\n                                        Enable/disable variant stats array creation. Enabled by default.\n\nTileDB options:\n  -c,--tile-capacity UINT=10000         Tile capacity to use for the array schema\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --checksum ENUM:value in {md5-&gt;md5,none-&gt;none,sha256-&gt;sha256} OR {md5,none,sha256}=sha256\n                                        Checksum to use for dataset validation on read and writes.\n\nDebug options:\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file\n\n\n\nIngests samples into a TileDB-VCF dataset\n\nUsage: tiledbvcf store [OPTIONS] [paths...]\n\nPositionals:\n  paths TEXT=[] ... Excludes: --samples-file\n                                        VCF URIs to ingest\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  -t,--threads UINT=20                  Number of threads\n  -m,--total-memory-budget-mb UINT:UINT in [512 - 64103]=48077\n                                        The total memory budget for ingestion (MiB)\n  -M,--total-memory-percentage FLOAT:FLOAT in [0 - 1]=0\n                                        Percentage of total system memory used for ingestion (overrides '--total-memory-budget-mb')\n  --resume                              Resume incomplete ingestion of sample batch\n\nSample options:\n  -e,--sample-batch-size UINT=10        Number of samples per batch for ingestion\n  -f,--samples-file TEXT Excludes: paths\n                                        File with 1 VCF path to be ingested per line. The format can also include an explicit\n                                        index path on each line, in the format '&lt;vcf-uri&gt;&lt;TAB&gt;&lt;index-uri&gt;'\n  --remove-sample-file Needs: --samples-file\n                                        If specified, the samples file ('-f' argument) is deleted after successful ingestion\n  -d,--scratch-dir TEXT                 Directory used for local storage of downloaded remote samples\n  -s,--scratch-mb UINT=0                Amount of local storage that can be used for downloading remote samples (MB)\n\nTileDB options:\n  -p,--s3-part-size UINT=50             [S3 only] Part size to use for writes (MB)\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --stats                               Enable TileDB stats\n  --stats-vcf-header-array              Enable TileDB stats for vcf header array usage\n\nAdvanced options:\n  --ratio-tiledb-memory FLOAT:FLOAT in [0.01 - 0.99]=0.5\n                                        Ratio of memory budget allocated to TileDB::sm.mem.total_budget\n  --max-tiledb-memory-mb UINT=4096      Maximum memory allocated to TileDB::sm.mem.total_budget (MiB)\n  --input-record-buffer-mb UINT=1       Size of input record buffer for each sample file (MiB)\n  --avg-vcf-record-size INT:INT in [1 - 4096]=512\n                                        Average VCF record size (bytes)\n  --ratio-task-size FLOAT:FLOAT in [0.01 - 1]=0.75\n                                        Ratio of worker task size to computed task size\n  --ratio-output-flush FLOAT:FLOAT in [0.01 - 1]=0.75\n                                        Ratio of output buffer capacity that triggers a flush to TileDB\n\nContig options:\n  --disable-contig-fragment-merging{false} Excludes: --contigs-to-keep-separate --contigs-to-allow-merging\n                                        Disable merging of contigs into fragments. Generally contig fragment merging is good,\n                                        this is a performance optimization to reduce the prefixes on a s3/azure/gcs bucket\n                                        when there is a large number of pseudo contigs which are small in size.\n  --contigs-to-keep-separate TEXT ... Excludes: --disable-contig-fragment-merging --contigs-to-allow-merging\n                                        Comma-separated list of contigs that should not be merged into combined fragments.\n                                        The default list includes all standard human chromosomes in both UCSC (e.g., chr1)\n                                        and Ensembl (e.g., 1) formats.\n  --contigs-to-allow-merging TEXT=[] ... Excludes: --disable-contig-fragment-merging --contigs-to-keep-separate\n                                        Comma-separated list of contigs that should be allowed to be merged into combined\n                                        fragments.\n  --contig-mode ENUM:value in {all-&gt;all,merged-&gt;merged,separate-&gt;separate} OR {all,merged,separate}=all\n                                        Select which contigs are ingested: 'separate', 'merged', or 'all' contigs\n\nDebug options:\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file\n  -v,--verbose :DEPRECATED              Enable verbose output DEPRECATED: please use '--log-level debug' instead\n\nLegacy options:\n  -n,--max-record-buff UINT             Max number of VCF records to buffer per file\n  -k,--thread-task-size UINT            Max length (# columns) of an ingestion task. Affects load balancing of ingestion\n                                        work across threads, and total memory consumption.\n  -b,--mem-budget-mb UINT               The maximum size of TileDB buffers before flushing (MiB)\n\n\n\nDelete samples from a TileDB-VCF dataset\n\nUsage: tiledbvcf delete [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  -s,--sample-names TEXT=[] ...         CSV list of sample names to delete\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file\n\n\n\nExports data from a TileDB-VCF dataset\n\nUsage: tiledbvcf export [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n\nOutput options:\n  -O,--output-format ENUM:value in {b-&gt;b,t-&gt;t,u-&gt;u,v-&gt;v,z-&gt;z} OR {b,t,u,v,z}=b\n                                        Export format. Options are: 'b': bcf (compressed); 'u': bcf; 'z': vcf.gz; 'v': vcf;\n                                        't': TSV\n  -o,--output-path TEXT                 [TSV or combined VCF export only] The name of the output file.\n  -m,--merge Needs: --output-path       Export combined VCF file.\n  -t,--tsv-fields TEXT=[] ...           [TSV export only] An ordered CSV list of fields to export in the TSV. A field name\n                                        can be one of 'SAMPLE', 'ID', 'REF', 'ALT', 'QUAL', 'POS', 'CHR', 'FILTER'. Additionally,\n                                        INFO fields can be specified by 'I:&lt;name&gt;' and FMT fields with 'F:&lt;name&gt;'. To export\n                                        the intersecting query region for each row in the output, use the field names 'Q:POS',\n                                        'Q:END' and 'Q:LINE'.\n  -n,--limit UINT=18446744073709551615  Only export the first N intersecting records.\n  -d,--output-dir TEXT                  Directory used for local output of exported samples\n  --upload-dir TEXT                     If set, all output file(s) from the export process will be copied to the given directory\n                                        (or S3 prefix) upon completion.\n  -c,--count-only Excludes: --af-filter Don't write output files, only print the count of the resulting number of intersecting\n                                        records.\n  --af-filter TEXT Excludes: --count-only\n                                        If set, only export data that passes the AF filter.\n\nRegion options:\n  -r,--regions TEXT=[] ... Excludes: --regions-file\n                                        CSV list of regions to export in the format 'chr:min-max'\n  -R,--regions-file TEXT Excludes: --regions\n                                        File containing regions (BED format)\n  --sorted                              Do not sort regions or regions file if they are pre-sorted\n  --region-partition TEXT               Partitions the list of regions to be exported and causes this export to export only\n                                        a specific partition of them. Specify in the format I:N where I is the partition\n                                        index and N is the total number of partitions. Useful for batch exports.\n\nSample options:\n  -f,--samples-file TEXT Excludes: --sample-names\n                                        Path to file with 1 sample name per line\n  -s,--sample-names TEXT=[] ... Excludes: --samples-file\n                                        CSV list of sample names to export\n  --sample-partition TEXT               Partitions the list of samples to be exported and causes this export to export only\n                                        a specific partition of them. Specify in the format I:N where I is the partition\n                                        index and N is the total number of partitions. Useful for batch exports.\n  --disable-check-samples{false}        Disable validating that sample passed exist in dataset before executing query and\n                                        error if any sample requested is not in the dataset\n\nTileDB options:\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --mem-budget-buffer-percentage FLOAT=25\n                                        The percentage of the memory budget to use for TileDB query buffers.\n  --mem-budget-tile-cache-percentage FLOAT=10\n                                        The percentage of the memory budget to use for TileDB tile cache.\n  -b,--mem-budget-mb UINT=2048          The memory budget (MB) used when submitting TileDB queries.\n  --stats                               Enable TileDB stats\n  --stats-vcf-header-array              Enable TileDB stats for vcf header array usage\n\nDebug options:\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file\n  -v,--verbose :DEPRECATED              Enable verbose output DEPRECATED: please use '--log-level debug' instead\n  --enable-progress-estimation          Enable progress estimation in verbose mode. Progress estimation can sometimes cause\n                                        a performance impact, so enable this with consideration.\n  --debug-print-vcf-regions             Enable debug printing of vcf region passed by user or bed file. Requires verbose\n                                        mode\n  --debug-print-sample-list             Enable debug printing of sample list used in read. Requires verbose mode\n  --debug-print-tiledb-query-ranges     Enable debug printing of tiledb query ranges used in read. Requires verbose mode\n\n\n\nLists all sample names present in a TileDB-VCF dataset\n\nUsage: tiledbvcf list [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file\n\n\n\nPrints high-level statistics about a TileDB-VCF dataset\n\nUsage: tiledbvcf stat [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file\n\n\n\nConsolidate TileDB-VCF dataset\n\nUsage: tiledbvcf utils consolidate [OPTIONS] SUBCOMMAND\n\nOptions:\n  -h,--help                             Print this help message and exit\n\nSubcommands:\n  commits                               Consolidate TileDB-VCF dataset commits\n  fragments                             Consolidate TileDB-VCF dataset fragments\n  fragment_meta                         Consolidate TileDB-VCF dataset fragment metadata\n\n\n\nVacuum TileDB-VCF dataset\n\nUsage: tiledbvcf utils vacuum [OPTIONS] SUBCOMMAND\n\nOptions:\n  -h,--help                             Print this help message and exit\n\nSubcommands:\n  commits                               Vacuum TileDB-VCF dataset commits\n  fragments                             Vacuum TileDB-VCF dataset fragments\n  fragment_meta                         Vacuum TileDB-VCF dataset fragment metadata",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html#create",
    "href": "documentation/api-reference/cli.html#create",
    "title": "CLI",
    "section": "",
    "text": "Creates an empty TileDB-VCF dataset\n\nUsage: tiledbvcf create [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  -a,--attributes TEXT=[] ... Excludes: --vcf-attributes\n                                        INFO and/or FORMAT field names (comma-delimited) to store as separate attributes.\n                                        Names should be 'fmt_X' or 'info_X' for a field name 'X' (case sensitive).\n  -v,--vcf-attributes TEXT Excludes: --attributes\n                                        Create separate attributes for all INFO and FORMAT fields in the provided VCF file.\n  -g,--anchor-gap UINT=1000             Anchor gap size to use\n  -n,--no-duplicates                    Allow records with duplicate start positions to be written to the array.\n  --compress-sample-dim,--no-compress-sample-dim{false}\n                                        Enable/disable compression of the sample dimension. Enabled by default.\n\nIngestion task options:\n  --enable-allele-count,--disable-allele-count{false}\n                                        Enable/disable allele count array creation. Enabled by default.\n  --enable-variant-stats,--disable-variant-stats{false}\n                                        Enable/disable variant stats array creation. Enabled by default.\n\nTileDB options:\n  -c,--tile-capacity UINT=10000         Tile capacity to use for the array schema\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --checksum ENUM:value in {md5-&gt;md5,none-&gt;none,sha256-&gt;sha256} OR {md5,none,sha256}=sha256\n                                        Checksum to use for dataset validation on read and writes.\n\nDebug options:\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html#store",
    "href": "documentation/api-reference/cli.html#store",
    "title": "CLI",
    "section": "",
    "text": "Ingests samples into a TileDB-VCF dataset\n\nUsage: tiledbvcf store [OPTIONS] [paths...]\n\nPositionals:\n  paths TEXT=[] ... Excludes: --samples-file\n                                        VCF URIs to ingest\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  -t,--threads UINT=20                  Number of threads\n  -m,--total-memory-budget-mb UINT:UINT in [512 - 64103]=48077\n                                        The total memory budget for ingestion (MiB)\n  -M,--total-memory-percentage FLOAT:FLOAT in [0 - 1]=0\n                                        Percentage of total system memory used for ingestion (overrides '--total-memory-budget-mb')\n  --resume                              Resume incomplete ingestion of sample batch\n\nSample options:\n  -e,--sample-batch-size UINT=10        Number of samples per batch for ingestion\n  -f,--samples-file TEXT Excludes: paths\n                                        File with 1 VCF path to be ingested per line. The format can also include an explicit\n                                        index path on each line, in the format '&lt;vcf-uri&gt;&lt;TAB&gt;&lt;index-uri&gt;'\n  --remove-sample-file Needs: --samples-file\n                                        If specified, the samples file ('-f' argument) is deleted after successful ingestion\n  -d,--scratch-dir TEXT                 Directory used for local storage of downloaded remote samples\n  -s,--scratch-mb UINT=0                Amount of local storage that can be used for downloading remote samples (MB)\n\nTileDB options:\n  -p,--s3-part-size UINT=50             [S3 only] Part size to use for writes (MB)\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --stats                               Enable TileDB stats\n  --stats-vcf-header-array              Enable TileDB stats for vcf header array usage\n\nAdvanced options:\n  --ratio-tiledb-memory FLOAT:FLOAT in [0.01 - 0.99]=0.5\n                                        Ratio of memory budget allocated to TileDB::sm.mem.total_budget\n  --max-tiledb-memory-mb UINT=4096      Maximum memory allocated to TileDB::sm.mem.total_budget (MiB)\n  --input-record-buffer-mb UINT=1       Size of input record buffer for each sample file (MiB)\n  --avg-vcf-record-size INT:INT in [1 - 4096]=512\n                                        Average VCF record size (bytes)\n  --ratio-task-size FLOAT:FLOAT in [0.01 - 1]=0.75\n                                        Ratio of worker task size to computed task size\n  --ratio-output-flush FLOAT:FLOAT in [0.01 - 1]=0.75\n                                        Ratio of output buffer capacity that triggers a flush to TileDB\n\nContig options:\n  --disable-contig-fragment-merging{false} Excludes: --contigs-to-keep-separate --contigs-to-allow-merging\n                                        Disable merging of contigs into fragments. Generally contig fragment merging is good,\n                                        this is a performance optimization to reduce the prefixes on a s3/azure/gcs bucket\n                                        when there is a large number of pseudo contigs which are small in size.\n  --contigs-to-keep-separate TEXT ... Excludes: --disable-contig-fragment-merging --contigs-to-allow-merging\n                                        Comma-separated list of contigs that should not be merged into combined fragments.\n                                        The default list includes all standard human chromosomes in both UCSC (e.g., chr1)\n                                        and Ensembl (e.g., 1) formats.\n  --contigs-to-allow-merging TEXT=[] ... Excludes: --disable-contig-fragment-merging --contigs-to-keep-separate\n                                        Comma-separated list of contigs that should be allowed to be merged into combined\n                                        fragments.\n  --contig-mode ENUM:value in {all-&gt;all,merged-&gt;merged,separate-&gt;separate} OR {all,merged,separate}=all\n                                        Select which contigs are ingested: 'separate', 'merged', or 'all' contigs\n\nDebug options:\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file\n  -v,--verbose :DEPRECATED              Enable verbose output DEPRECATED: please use '--log-level debug' instead\n\nLegacy options:\n  -n,--max-record-buff UINT             Max number of VCF records to buffer per file\n  -k,--thread-task-size UINT            Max length (# columns) of an ingestion task. Affects load balancing of ingestion\n                                        work across threads, and total memory consumption.\n  -b,--mem-budget-mb UINT               The maximum size of TileDB buffers before flushing (MiB)",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html#delete",
    "href": "documentation/api-reference/cli.html#delete",
    "title": "CLI",
    "section": "",
    "text": "Delete samples from a TileDB-VCF dataset\n\nUsage: tiledbvcf delete [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  -s,--sample-names TEXT=[] ...         CSV list of sample names to delete\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html#export",
    "href": "documentation/api-reference/cli.html#export",
    "title": "CLI",
    "section": "",
    "text": "Exports data from a TileDB-VCF dataset\n\nUsage: tiledbvcf export [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n\nOutput options:\n  -O,--output-format ENUM:value in {b-&gt;b,t-&gt;t,u-&gt;u,v-&gt;v,z-&gt;z} OR {b,t,u,v,z}=b\n                                        Export format. Options are: 'b': bcf (compressed); 'u': bcf; 'z': vcf.gz; 'v': vcf;\n                                        't': TSV\n  -o,--output-path TEXT                 [TSV or combined VCF export only] The name of the output file.\n  -m,--merge Needs: --output-path       Export combined VCF file.\n  -t,--tsv-fields TEXT=[] ...           [TSV export only] An ordered CSV list of fields to export in the TSV. A field name\n                                        can be one of 'SAMPLE', 'ID', 'REF', 'ALT', 'QUAL', 'POS', 'CHR', 'FILTER'. Additionally,\n                                        INFO fields can be specified by 'I:&lt;name&gt;' and FMT fields with 'F:&lt;name&gt;'. To export\n                                        the intersecting query region for each row in the output, use the field names 'Q:POS',\n                                        'Q:END' and 'Q:LINE'.\n  -n,--limit UINT=18446744073709551615  Only export the first N intersecting records.\n  -d,--output-dir TEXT                  Directory used for local output of exported samples\n  --upload-dir TEXT                     If set, all output file(s) from the export process will be copied to the given directory\n                                        (or S3 prefix) upon completion.\n  -c,--count-only Excludes: --af-filter Don't write output files, only print the count of the resulting number of intersecting\n                                        records.\n  --af-filter TEXT Excludes: --count-only\n                                        If set, only export data that passes the AF filter.\n\nRegion options:\n  -r,--regions TEXT=[] ... Excludes: --regions-file\n                                        CSV list of regions to export in the format 'chr:min-max'\n  -R,--regions-file TEXT Excludes: --regions\n                                        File containing regions (BED format)\n  --sorted                              Do not sort regions or regions file if they are pre-sorted\n  --region-partition TEXT               Partitions the list of regions to be exported and causes this export to export only\n                                        a specific partition of them. Specify in the format I:N where I is the partition\n                                        index and N is the total number of partitions. Useful for batch exports.\n\nSample options:\n  -f,--samples-file TEXT Excludes: --sample-names\n                                        Path to file with 1 sample name per line\n  -s,--sample-names TEXT=[] ... Excludes: --samples-file\n                                        CSV list of sample names to export\n  --sample-partition TEXT               Partitions the list of samples to be exported and causes this export to export only\n                                        a specific partition of them. Specify in the format I:N where I is the partition\n                                        index and N is the total number of partitions. Useful for batch exports.\n  --disable-check-samples{false}        Disable validating that sample passed exist in dataset before executing query and\n                                        error if any sample requested is not in the dataset\n\nTileDB options:\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --mem-budget-buffer-percentage FLOAT=25\n                                        The percentage of the memory budget to use for TileDB query buffers.\n  --mem-budget-tile-cache-percentage FLOAT=10\n                                        The percentage of the memory budget to use for TileDB tile cache.\n  -b,--mem-budget-mb UINT=2048          The memory budget (MB) used when submitting TileDB queries.\n  --stats                               Enable TileDB stats\n  --stats-vcf-header-array              Enable TileDB stats for vcf header array usage\n\nDebug options:\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file\n  -v,--verbose :DEPRECATED              Enable verbose output DEPRECATED: please use '--log-level debug' instead\n  --enable-progress-estimation          Enable progress estimation in verbose mode. Progress estimation can sometimes cause\n                                        a performance impact, so enable this with consideration.\n  --debug-print-vcf-regions             Enable debug printing of vcf region passed by user or bed file. Requires verbose\n                                        mode\n  --debug-print-sample-list             Enable debug printing of sample list used in read. Requires verbose mode\n  --debug-print-tiledb-query-ranges     Enable debug printing of tiledb query ranges used in read. Requires verbose mode",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html#list",
    "href": "documentation/api-reference/cli.html#list",
    "title": "CLI",
    "section": "",
    "text": "Lists all sample names present in a TileDB-VCF dataset\n\nUsage: tiledbvcf list [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html#stat",
    "href": "documentation/api-reference/cli.html#stat",
    "title": "CLI",
    "section": "",
    "text": "Prints high-level statistics about a TileDB-VCF dataset\n\nUsage: tiledbvcf stat [OPTIONS]\n\nOptions:\n  -u,--uri TEXT REQUIRED                TileDB-VCF dataset URI\n  --tiledb-config TEXT=[] ...           CSV string of the format 'param1=val1,param2=val2...' specifying optional TileDB\n                                        configuration parameter settings.\n  --log-level TEXT:{fatal,error,warn,info,debug,trace}=fatal\n                                        Log message level\n  --log-file TEXT                       Log message output file",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html#consolidate",
    "href": "documentation/api-reference/cli.html#consolidate",
    "title": "CLI",
    "section": "",
    "text": "Consolidate TileDB-VCF dataset\n\nUsage: tiledbvcf utils consolidate [OPTIONS] SUBCOMMAND\n\nOptions:\n  -h,--help                             Print this help message and exit\n\nSubcommands:\n  commits                               Consolidate TileDB-VCF dataset commits\n  fragments                             Consolidate TileDB-VCF dataset fragments\n  fragment_meta                         Consolidate TileDB-VCF dataset fragment metadata",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/api-reference/cli.html#vacuum",
    "href": "documentation/api-reference/cli.html#vacuum",
    "title": "CLI",
    "section": "",
    "text": "Vacuum TileDB-VCF dataset\n\nUsage: tiledbvcf utils vacuum [OPTIONS] SUBCOMMAND\n\nOptions:\n  -h,--help                             Print this help message and exit\n\nSubcommands:\n  commits                               Vacuum TileDB-VCF dataset commits\n  fragments                             Vacuum TileDB-VCF dataset fragments\n  fragment_meta                         Vacuum TileDB-VCF dataset fragment metadata",
    "crumbs": [
      "Home page",
      "API Reference",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/ingestion/python.html",
    "href": "documentation/ingestion/python.html",
    "title": "Python",
    "section": "",
    "text": "Similar to TileDB-VCF’s command-line interface (CLI), tiledbvcf supports ingesting VCF (or BCF) files into TileDB, either when creating a new dataset or updating an existing dataset with additional samples. See the CLI Usage for a more detailed description of the ingestion process. Here, we’ll only focus on the mechanics of ingestion from Python.\nThe text file data/s3-bcf-samples.txt contains a list of S3 URIs pointing to 7 BCF files from the same cohort.\nwith open(\"data/s3-bcf-samples.txt\") as f:\n    sample_uris = [l.rstrip(\"\\n\") for l in f.readlines()]\nsample_uris\n## ['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf']\nYou can add them to your existing dataset by re-opening it in write mode and providing the file URIs. It’s also necessary to allocate scratch space so the files can be downloaded to a temporary location prior to ingestion.\nsmall_ds = tiledbvcf.Dataset('small_dataset', mode = \"w\")\nsmall_ds.ingest_samples(sample_uris)\nThe TileDB-VCF dataset located at small_dataset now includes records for 660 variants across 10 samples. The next section provides examples demonstrating how to query this dataset.",
    "crumbs": [
      "Home page",
      "Ingestion",
      "Python"
    ]
  },
  {
    "objectID": "documentation/ingestion/python.html#python-ingestion",
    "href": "documentation/ingestion/python.html#python-ingestion",
    "title": "Python",
    "section": "",
    "text": "Similar to TileDB-VCF’s command-line interface (CLI), tiledbvcf supports ingesting VCF (or BCF) files into TileDB, either when creating a new dataset or updating an existing dataset with additional samples. See the CLI Usage for a more detailed description of the ingestion process. Here, we’ll only focus on the mechanics of ingestion from Python.\nThe text file data/s3-bcf-samples.txt contains a list of S3 URIs pointing to 7 BCF files from the same cohort.\nwith open(\"data/s3-bcf-samples.txt\") as f:\n    sample_uris = [l.rstrip(\"\\n\") for l in f.readlines()]\nsample_uris\n## ['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf',\n##   's3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf']\nYou can add them to your existing dataset by re-opening it in write mode and providing the file URIs. It’s also necessary to allocate scratch space so the files can be downloaded to a temporary location prior to ingestion.\nsmall_ds = tiledbvcf.Dataset('small_dataset', mode = \"w\")\nsmall_ds.ingest_samples(sample_uris)\nThe TileDB-VCF dataset located at small_dataset now includes records for 660 variants across 10 samples. The next section provides examples demonstrating how to query this dataset.",
    "crumbs": [
      "Home page",
      "Ingestion",
      "Python"
    ]
  },
  {
    "objectID": "documentation/ingestion/distributed-ingestion.html",
    "href": "documentation/ingestion/distributed-ingestion.html",
    "title": "Distributed Ingestion",
    "section": "",
    "text": "TileDB has built in support for scalable distributed ingestion.\ningest is a simply python command that will dispatch and run a task graph to load VCF samples in parallel across a number of machines.",
    "crumbs": [
      "Home page",
      "Ingestion",
      "Distributed Ingestion"
    ]
  },
  {
    "objectID": "documentation/ingestion/distributed-ingestion.html#example",
    "href": "documentation/ingestion/distributed-ingestion.html#example",
    "title": "Distributed Ingestion",
    "section": "Example",
    "text": "Example\nIn order to run this example please make sure to have install TileDB-Cloud-Py with pip install --user tiledb-cloud.\nimport tiledb.cloud\nfrom tiledb.cloud.vcf import ingest\n\ns3_storage_uri = \"s3://my_bucket/my_array\"\nvcf_location = \"s3://1000genomes-dragen-v3.7.6/data/individuals/hg38-graph-based\"\npattern = \"*.hard-filtered.vcf.gz\"\nmax_files = 75\nname = f\"dragen-v3.7.6-example-{max_files}\"\n\nnamespace = \"my-organization\"\ntiledb_uri = f\"tiledb://{namespace}/{name}\"\n\n# Define which contigs we want to ingest\ncontigs = Contigs.CHROMOSOMES\n\n\ningest(\n    s3_storage_uri,\n    config=config,\n    search_uri=vcf_location,\n    pattern=pattern,\n    max_files=max_files,\n    contigs=contigs,\n)\n\nContigs\nTileDB support specifying the contigs you wish to ingest. The default behavior is to ingestion all contigs present in a VCF file. However you can specify if you’d like to restrict to a specific list or a predefined list\nOptions for contigs:\n\nALL\nCHROMOSOMES\nOTHER",
    "crumbs": [
      "Home page",
      "Ingestion",
      "Distributed Ingestion"
    ]
  },
  {
    "objectID": "documentation/the-solution.html",
    "href": "documentation/the-solution.html",
    "title": "The Solution",
    "section": "",
    "text": "Population variant data can be efficiently represented using a 3D sparse array. For each sample, imagine a 2D plane where the vertical axis is the contig and the horizontal axis is the genomic position. Every variant can be represented as a range within this plane; it can be unary (i.e., a SNP) or it can be a longer range (e.g., INDEL or CNV). Each sample is then indexed by a third dimension, which is unbounded to accommodate populations of any size. The figure below shows an example for one sample, with several variants distributed across contigs chr1 , chr2 and chr3.\n\n\n\n3D array representation of population variants\n\n\nIn TileDB-VCF, we represent the start position of each range as a non-empty cell in a sparse array (black squares in the above figure). In each of those array cells, we store the end position of each cell (to create a range) along with all other fields in the corresponding single-sample VCF files for each variant (e.g., REF, ALT, etc.). Therefore, for every sample, we map variants to 2D non-empty sparse array cells.\nTo facilitate rapid retrieval of interval intersections (explained in the next section), we also inject anchors (green square in the above figure) to breakup long ranges. Specifically, we create a new non-empty cell every anchor_gap bases from the start of the range (where anchor_gap is a user-defined parameter), which is identical to the range cell, except that (1) it has a new start coordinate and (2) it stores the real start position in an attribute.\nNote that regardless of the number of samples, we do not inject any additional information other than that of the anchors, which is user configurable and turns out to be negligible for real datasets. In other words, this solution leads to linear storage in the number of samples, thus being scalable.\n\n\n\nThe typical access pattern used for variant data involves one or more rectangles covering a set of genomic ranges across one or more samples. In the figure below, let the black rectangle be the user’s query. Observe that the results are highlighted in blue (v1, v2, v4, v7). However, the rectangle misses v1, i.e., the case where an Indel/CNV range intersects the query rectangle, but the start position is outside the rectangle.\n\n\n\nInterval intersection using expanded query ranges and anchors\n\n\nThis is the motivation behind anchors. TileDB-VCF expands the user’s query range on the left by anchor_gap. It then reports as results the cells that are included in the expanded query if their end position (stored in an attribute) comes after the query range start endpoint. In the example above, TileDB-VCF retrieves anchor a1 and Indel/CNV v3. It reports v1 as a result (as it can be extracted directly from anchor a1), but filters out v3.\n\n\n\n\n\n\nNote\n\n\n\nBy representing each sample’s variants as non-empty cells in a 2D plane and using anchors and expanded queries, we managed to model population variants as 3D sparse arrays and use the vanilla functionality of TileDB-Embedded, inheriting all its powerful features out of the box.\n\n\nQuite often, the analyses requires data retrieval based on numerous query ranges (up to the order of millions), which must be submitted simultaneously. TileDB-VCF leverages the highly optimized multi-range subarray functionality of TileDB Embedded, which leads to unparalleled performance for such scenarios.\nBut what about updates? That’s the topic of the next section.\n\n\n\nTileDB-VCF is based on TileDB Embedded which supports rapid updates via immutable fragments. That means that every time a new batch of samples is added to the 3D array, the previous contents of the array are not modified at all. Each batch write operation is totally independent and _lock-free—_any number of threads or processes can write simultaneously without synchronization, while ensuring consistency. With TileDB-VCF, both update time and storage size scales linearly with the number of new samples, solving the N+1 problem.\n\n\n\nTileDB allows you to perform parallel ingestion and parallel slicing / processing, 100% serverless. This means that you do not have to spin up large clusters or pay for idle time. You can easily slice data or define complex task flows comprised of thousands of tasks, which TileDB deploys elastically in its serverless infrastructure, providing an unmatched combination of ease of use and low cost on the cloud—even for your most challenging analyses.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe shear volume of data generated by modern population genomics applications creates enormous challenges around data management and collaboration.\n\n\nTileDB offers groundbreaking features for collaborative genomic research:\n\nPopular public datasets made available in the TileDB format for direct analysis\nEasy mechanism for one to share their data and code (Jupyter notebooks and user-defined functions), either with a specific set of users or the entire world\nEasy ways to explore and discover public data and code.\n\nJoin our vision to build a growing community around open data and code!\n\n\n\nIf you are familiar with the TileDB Embedded data model, you can delve into the technical details of how TileDB-VCF stores genomic variant data using [D sparse arrays. Otherwise you can proceed with installation instructions and tutorials.",
    "crumbs": [
      "Home page",
      "Background",
      "The Solution"
    ]
  },
  {
    "objectID": "documentation/the-solution.html#storing-variants-as-3d-sparse-arrays",
    "href": "documentation/the-solution.html#storing-variants-as-3d-sparse-arrays",
    "title": "The Solution",
    "section": "",
    "text": "Population variant data can be efficiently represented using a 3D sparse array. For each sample, imagine a 2D plane where the vertical axis is the contig and the horizontal axis is the genomic position. Every variant can be represented as a range within this plane; it can be unary (i.e., a SNP) or it can be a longer range (e.g., INDEL or CNV). Each sample is then indexed by a third dimension, which is unbounded to accommodate populations of any size. The figure below shows an example for one sample, with several variants distributed across contigs chr1 , chr2 and chr3.\n\n\n\n3D array representation of population variants\n\n\nIn TileDB-VCF, we represent the start position of each range as a non-empty cell in a sparse array (black squares in the above figure). In each of those array cells, we store the end position of each cell (to create a range) along with all other fields in the corresponding single-sample VCF files for each variant (e.g., REF, ALT, etc.). Therefore, for every sample, we map variants to 2D non-empty sparse array cells.\nTo facilitate rapid retrieval of interval intersections (explained in the next section), we also inject anchors (green square in the above figure) to breakup long ranges. Specifically, we create a new non-empty cell every anchor_gap bases from the start of the range (where anchor_gap is a user-defined parameter), which is identical to the range cell, except that (1) it has a new start coordinate and (2) it stores the real start position in an attribute.\nNote that regardless of the number of samples, we do not inject any additional information other than that of the anchors, which is user configurable and turns out to be negligible for real datasets. In other words, this solution leads to linear storage in the number of samples, thus being scalable.",
    "crumbs": [
      "Home page",
      "Background",
      "The Solution"
    ]
  },
  {
    "objectID": "documentation/the-solution.html#fast-retrieval",
    "href": "documentation/the-solution.html#fast-retrieval",
    "title": "The Solution",
    "section": "",
    "text": "The typical access pattern used for variant data involves one or more rectangles covering a set of genomic ranges across one or more samples. In the figure below, let the black rectangle be the user’s query. Observe that the results are highlighted in blue (v1, v2, v4, v7). However, the rectangle misses v1, i.e., the case where an Indel/CNV range intersects the query rectangle, but the start position is outside the rectangle.\n\n\n\nInterval intersection using expanded query ranges and anchors\n\n\nThis is the motivation behind anchors. TileDB-VCF expands the user’s query range on the left by anchor_gap. It then reports as results the cells that are included in the expanded query if their end position (stored in an attribute) comes after the query range start endpoint. In the example above, TileDB-VCF retrieves anchor a1 and Indel/CNV v3. It reports v1 as a result (as it can be extracted directly from anchor a1), but filters out v3.\n\n\n\n\n\n\nNote\n\n\n\nBy representing each sample’s variants as non-empty cells in a 2D plane and using anchors and expanded queries, we managed to model population variants as 3D sparse arrays and use the vanilla functionality of TileDB-Embedded, inheriting all its powerful features out of the box.\n\n\nQuite often, the analyses requires data retrieval based on numerous query ranges (up to the order of millions), which must be submitted simultaneously. TileDB-VCF leverages the highly optimized multi-range subarray functionality of TileDB Embedded, which leads to unparalleled performance for such scenarios.\nBut what about updates? That’s the topic of the next section.",
    "crumbs": [
      "Home page",
      "Background",
      "The Solution"
    ]
  },
  {
    "objectID": "documentation/the-solution.html#updates",
    "href": "documentation/the-solution.html#updates",
    "title": "The Solution",
    "section": "",
    "text": "TileDB-VCF is based on TileDB Embedded which supports rapid updates via immutable fragments. That means that every time a new batch of samples is added to the 3D array, the previous contents of the array are not modified at all. Each batch write operation is totally independent and _lock-free—_any number of threads or processes can write simultaneously without synchronization, while ensuring consistency. With TileDB-VCF, both update time and storage size scales linearly with the number of new samples, solving the N+1 problem.",
    "crumbs": [
      "Home page",
      "Background",
      "The Solution"
    ]
  },
  {
    "objectID": "documentation/the-solution.html#easy-and-cost-efficient-scaling",
    "href": "documentation/the-solution.html#easy-and-cost-efficient-scaling",
    "title": "The Solution",
    "section": "",
    "text": "TileDB allows you to perform parallel ingestion and parallel slicing / processing, 100% serverless. This means that you do not have to spin up large clusters or pay for idle time. You can easily slice data or define complex task flows comprised of thousands of tasks, which TileDB deploys elastically in its serverless infrastructure, providing an unmatched combination of ease of use and low cost on the cloud—even for your most challenging analyses.",
    "crumbs": [
      "Home page",
      "Background",
      "The Solution"
    ]
  },
  {
    "objectID": "documentation/the-solution.html#collaboration",
    "href": "documentation/the-solution.html#collaboration",
    "title": "The Solution",
    "section": "",
    "text": "Note\n\n\n\nThe shear volume of data generated by modern population genomics applications creates enormous challenges around data management and collaboration.\n\n\nTileDB offers groundbreaking features for collaborative genomic research:\n\nPopular public datasets made available in the TileDB format for direct analysis\nEasy mechanism for one to share their data and code (Jupyter notebooks and user-defined functions), either with a specific set of users or the entire world\nEasy ways to explore and discover public data and code.\n\nJoin our vision to build a growing community around open data and code!",
    "crumbs": [
      "Home page",
      "Background",
      "The Solution"
    ]
  },
  {
    "objectID": "documentation/the-solution.html#whats-next",
    "href": "documentation/the-solution.html#whats-next",
    "title": "The Solution",
    "section": "",
    "text": "If you are familiar with the TileDB Embedded data model, you can delve into the technical details of how TileDB-VCF stores genomic variant data using [D sparse arrays. Otherwise you can proceed with installation instructions and tutorials.",
    "crumbs": [
      "Home page",
      "Background",
      "The Solution"
    ]
  },
  {
    "objectID": "documentation/reference/config_logging.html",
    "href": "documentation/reference/config_logging.html",
    "title": "config_logging",
    "section": "",
    "text": "config_logging(level='fatal', log_file='')\nConfigure tiledbvcf logging.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlevel\nstr\nLog level from (fatal|error|warn|info|debug|trace)\n'fatal'\n\n\nlog_file\nstr\nLog file path.\n''"
  },
  {
    "objectID": "documentation/reference/config_logging.html#parameters",
    "href": "documentation/reference/config_logging.html#parameters",
    "title": "config_logging",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nlevel\nstr\nLog level from (fatal|error|warn|info|debug|trace)\n'fatal'\n\n\nlog_file\nstr\nLog file path.\n''"
  },
  {
    "objectID": "documentation/reference/index.html",
    "href": "documentation/reference/index.html",
    "title": "Python",
    "section": "",
    "text": "Dataset\nA class that provides read/write access to a TileDB-VCF dataset.\n\n\nReadConfig\nConfig settings for a TileDB-VCF dataset.\n\n\nconfig_logging\nConfigure tiledbvcf logging.",
    "crumbs": [
      "Home page",
      "API Reference",
      "Python"
    ]
  },
  {
    "objectID": "documentation/reference/index.html#tiledbvcf",
    "href": "documentation/reference/index.html#tiledbvcf",
    "title": "Python",
    "section": "",
    "text": "Dataset\nA class that provides read/write access to a TileDB-VCF dataset.\n\n\nReadConfig\nConfig settings for a TileDB-VCF dataset.\n\n\nconfig_logging\nConfigure tiledbvcf logging.",
    "crumbs": [
      "Home page",
      "API Reference",
      "Python"
    ]
  },
  {
    "objectID": "documentation/how-to/create-a-dataset.html",
    "href": "documentation/how-to/create-a-dataset.html",
    "title": "Create a Dataset",
    "section": "",
    "text": "Create a Dataset\nThe first step before ingesting any VCF samples is to create a dataset. This effectively creates a TileDB group and the appropriate empty arrays in it. \nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"w\") # sets dataset to \"Write\" mode\nds.create_dataset()                     # creates the dataset and\n                                        # keeps it in \"Write\" mode\ntiledbvcf create --uri my_vcf_dataset\n\nIf you wish to turn some of the `INFO` and `FMT` fields into separate _materialized_ attributes, you can do so as follows (names should be `fmt_X` or `info_X` for a field name `X` - case sensitive).\n\n```python\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"w\") \nds.create_dataset(extra_attrs=[\"info_AA\"])\ntiledbvcf create --uri my_vcf_dataset --attributes info_AA",
    "crumbs": [
      "Home page",
      "How To",
      "Create a Dataset"
    ]
  },
  {
    "objectID": "documentation/how-to/export-to-vcf.html",
    "href": "documentation/how-to/export-to-vcf.html",
    "title": "Export to VCF",
    "section": "",
    "text": "You can use the TileDB-VCF CLI to export the TileDB-VCF ingested dataset back into VCF formats for downstream analyses, in a lossless way.\n\n\n\n\n\n\nWarning\n\n\n\nWhile these exports are lossless in terms of the actual data stored, they may not be identical to the original files. For example, fields within the INFO and FORMAT columns may appear in a slightly different order in the exported files.\n\n\n\n\nTo recreate all of original (single-sample) VCF files simply run the export command and set the --output-formatto v, for VCF. \ntiledbvcf export \\\n  --uri my_vcf_dataset \\\n  --output-format v \\\n  --output-dir exported-vcfs\nIf bcftools is available on your system you can use it to easily examine any of the exported files:\nbcftools view --no-header exported-vcfs/G1.vcf\n\n## 1    13350    .    A    &lt;NON_REF&gt;    .    .    END=36258    GT:DP:GQ:MIN_DP:PL    1/0:50:3:43:44,29,99\n## 1    42091    .    A    &lt;NON_REF&gt;    .    .    END=101445    GT:DP:GQ:MIN_DP:PL    0/0:8:91:60:35,62,92\n## 2    11625    .    T    &lt;NON_REF&gt;    .    .    END=106375    GT:DP:GQ:MIN_DP:PL    0/0:27:72:76:70,30,83\n## 3    14580    .    T    &lt;NON_REF&gt;    .    .    END=86190    GT:DP:GQ:MIN_DP:PL    0/1:50:78:41:67,11,43\n\n\n\nThe same mechanics covered in the reading for filtering records by sample and genomic region also apply to exporting VCF files. \ntiledbvcf export \\\n  --uri my_vcf_dataset \\\n  --output-format v \\\n  --sample-names G1,G2,G3 \\\n  --regions 4:53227-196092,9:214865-465259 \\\n  --output-dir exported-filtered-vcfs",
    "crumbs": [
      "Home page",
      "How To",
      "Export to VCF"
    ]
  },
  {
    "objectID": "documentation/how-to/export-to-vcf.html#basic-export",
    "href": "documentation/how-to/export-to-vcf.html#basic-export",
    "title": "Export to VCF",
    "section": "",
    "text": "To recreate all of original (single-sample) VCF files simply run the export command and set the --output-formatto v, for VCF. \ntiledbvcf export \\\n  --uri my_vcf_dataset \\\n  --output-format v \\\n  --output-dir exported-vcfs\nIf bcftools is available on your system you can use it to easily examine any of the exported files:\nbcftools view --no-header exported-vcfs/G1.vcf\n\n## 1    13350    .    A    &lt;NON_REF&gt;    .    .    END=36258    GT:DP:GQ:MIN_DP:PL    1/0:50:3:43:44,29,99\n## 1    42091    .    A    &lt;NON_REF&gt;    .    .    END=101445    GT:DP:GQ:MIN_DP:PL    0/0:8:91:60:35,62,92\n## 2    11625    .    T    &lt;NON_REF&gt;    .    .    END=106375    GT:DP:GQ:MIN_DP:PL    0/0:27:72:76:70,30,83\n## 3    14580    .    T    &lt;NON_REF&gt;    .    .    END=86190    GT:DP:GQ:MIN_DP:PL    0/1:50:78:41:67,11,43",
    "crumbs": [
      "Home page",
      "How To",
      "Export to VCF"
    ]
  },
  {
    "objectID": "documentation/how-to/export-to-vcf.html#filtering-variants",
    "href": "documentation/how-to/export-to-vcf.html#filtering-variants",
    "title": "Export to VCF",
    "section": "",
    "text": "The same mechanics covered in the reading for filtering records by sample and genomic region also apply to exporting VCF files. \ntiledbvcf export \\\n  --uri my_vcf_dataset \\\n  --output-format v \\\n  --sample-names G1,G2,G3 \\\n  --regions 4:53227-196092,9:214865-465259 \\\n  --output-dir exported-filtered-vcfs",
    "crumbs": [
      "Home page",
      "How To",
      "Export to VCF"
    ]
  },
  {
    "objectID": "documentation/how-to/perform-distributed-queries-with-tiledb-cloud.html",
    "href": "documentation/how-to/perform-distributed-queries-with-tiledb-cloud.html",
    "title": "Perform Distributed Queries with TileDB-Cloud",
    "section": "",
    "text": "The tiledbvcf Python package includes integration with TileDB to enable distributing large queries in a serverless manner. \n\n\nYou can use the tiledbvcf package’s TileDB integration to partition read operations across regions and samples. The partitioning semantics are identical to those used by the CLI. \nimport tiledbvcf\nimport tiledb.cloud.vcf\n\ntiledb.cloud.vcf.query.read('my-large-dataset',\n                       attrs=['sample_name', 'pos_start', 'pos_end'],\n                       bed_file='very-large-bedfile.bed',\n                       region_partitions=10,\n                       sample_partitions=2)\nThe result is a pyarrow table.",
    "crumbs": [
      "Home page",
      "How To",
      "Perform Distributed Queries with TileDB-Cloud"
    ]
  },
  {
    "objectID": "documentation/how-to/perform-distributed-queries-with-tiledb-cloud.html#task-graphs",
    "href": "documentation/how-to/perform-distributed-queries-with-tiledb-cloud.html#task-graphs",
    "title": "Perform Distributed Queries with TileDB-Cloud",
    "section": "",
    "text": "You can use the tiledbvcf package’s TileDB integration to partition read operations across regions and samples. The partitioning semantics are identical to those used by the CLI. \nimport tiledbvcf\nimport tiledb.cloud.vcf\n\ntiledb.cloud.vcf.query.read('my-large-dataset',\n                       attrs=['sample_name', 'pos_start', 'pos_end'],\n                       bed_file='very-large-bedfile.bed',\n                       region_partitions=10,\n                       sample_partitions=2)\nThe result is a pyarrow table.",
    "crumbs": [
      "Home page",
      "How To",
      "Perform Distributed Queries with TileDB-Cloud"
    ]
  },
  {
    "objectID": "documentation/how-to/work-with-cloud-object-stores.html",
    "href": "documentation/how-to/work-with-cloud-object-stores.html",
    "title": "Work with Cloud Object Stores",
    "section": "",
    "text": "TileDB Embedded provides native support for reading from and writing to cloud object stores like AWS S3, Google Cloud Storage, and Microsoft Azure Blob Store. This guide will cover some considerations for using TileDB-VCF with these services. The examples will focus exclusively on S3, which is the most widely used, but note any of the aforementioned services can be substituted, as well as on-premise services like MinIO that provide S3-compatible APIs.\n\n\nThe process of creating a TileDB-VCF dataset on S3 is nearly identical to creating a local dataset. The only difference being an s3:// address is passed to the --uri argument rather than a local file path.\ntiledbvcf create --uri s3://my-bucket/my_dataset\nThis also works when querying a TileDB-VCF dataset located on S3.\ntiledbvcf export \\\n  --uri s3://tiledb-inc-demo-data/tiledbvcf-arrays/v4/vcf-samples-20 \\\n  --sample-names v2-tJjMfKyL,v2-eBAdKwID \\\n  -Ot --tsv-fields \"CHR,POS,REF,S:GT\" \\\n  --regions \"chr7:144000320-144008793,chr11:56490349-56491395\"\n\n\n\nVCF files located on S3 can be ingested directly into a TileDBVCF dataset using 1 of 2 different possible approaches. \n\n\nThe first approach is the easiest, you simply pass the tiledbvcf store command a list of S3 URIs and TileDB-VCF takes care of the rest:\ntiledbvcf store \\\n    --uri my_dataset \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\nIn this approach, remote VCF index files (which are relatively tiny) are downloaded locally, allowing TileDB-VCF to retrieve chunks of variant data from the remote VCF files without having to download them in full. By default, index files are downloaded to your current working directory, however, you can choose to store them in different location (e.g., a temporary directory) using the --scratch-dir argument.\n\n\n\nThe second approach is to download batches of VCF files in their entirety before ingestion, which may slightly improve ingestion performance. This approach requires allocating TileDB-VCF with scratch disk space using the --scratch-mb and --scratch-dir arguments.\ntiledbvcf store \\\n    --uri my_dataset \\\n    --scratch-dir \"$TMPDIR\" \\\n    --scratch-mb 4096 \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\nThe number of VCF files that are downloaded at a time is determined by the --sample-batch-size parameter, which defaults to 10. Downloading and ingestion happens asynchronously, so, for example, batch 3 will be downloaded as batch 2 is being ingestion. As a result, you must configure enough scratch space to store at least 20 samples, assuming a batch size of 10.\n\n\n\n\nFor TileDB to access a remote storage bucket you must be properly authenticated on the machine running TileDB. For S3, this means having access to the appropriate AWS access key ID and secret access key. This typically happens in one of three ways:\n\n\nIf the AWS Command Line Interface (CLI) is installed on your machine, running aws configure will store your credentials in a local profile that TileDB can access. You can verify the CLI has been previously configured by running:\naws s3 ls\nIf properly configured, this will output a list of the S3 buckets you (and thus TileDB) can access.\n\n\n\nYou can pass your AWS access key ID and secret access key to TileDB-VCF directly via the --tiledb-config argument, which expects a comma-separated string:\ntiledbvcf store \\\n    --uri my_dataset \\\n    --tiledb-config vfs.s3.aws_access_key_id=&lt;id&gt;,vfs.s3.aws_secret_access_key=&lt;secret&gt; \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n\n\n\nYour AWS credentials can also be passed to TileDB by defining the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.",
    "crumbs": [
      "Home page",
      "How To",
      "Work with Cloud Object Stores"
    ]
  },
  {
    "objectID": "documentation/how-to/work-with-cloud-object-stores.html#remote-datasets",
    "href": "documentation/how-to/work-with-cloud-object-stores.html#remote-datasets",
    "title": "Work with Cloud Object Stores",
    "section": "",
    "text": "The process of creating a TileDB-VCF dataset on S3 is nearly identical to creating a local dataset. The only difference being an s3:// address is passed to the --uri argument rather than a local file path.\ntiledbvcf create --uri s3://my-bucket/my_dataset\nThis also works when querying a TileDB-VCF dataset located on S3.\ntiledbvcf export \\\n  --uri s3://tiledb-inc-demo-data/tiledbvcf-arrays/v4/vcf-samples-20 \\\n  --sample-names v2-tJjMfKyL,v2-eBAdKwID \\\n  -Ot --tsv-fields \"CHR,POS,REF,S:GT\" \\\n  --regions \"chr7:144000320-144008793,chr11:56490349-56491395\"",
    "crumbs": [
      "Home page",
      "How To",
      "Work with Cloud Object Stores"
    ]
  },
  {
    "objectID": "documentation/how-to/work-with-cloud-object-stores.html#remote-vcf-files",
    "href": "documentation/how-to/work-with-cloud-object-stores.html#remote-vcf-files",
    "title": "Work with Cloud Object Stores",
    "section": "",
    "text": "VCF files located on S3 can be ingested directly into a TileDBVCF dataset using 1 of 2 different possible approaches. \n\n\nThe first approach is the easiest, you simply pass the tiledbvcf store command a list of S3 URIs and TileDB-VCF takes care of the rest:\ntiledbvcf store \\\n    --uri my_dataset \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\nIn this approach, remote VCF index files (which are relatively tiny) are downloaded locally, allowing TileDB-VCF to retrieve chunks of variant data from the remote VCF files without having to download them in full. By default, index files are downloaded to your current working directory, however, you can choose to store them in different location (e.g., a temporary directory) using the --scratch-dir argument.\n\n\n\nThe second approach is to download batches of VCF files in their entirety before ingestion, which may slightly improve ingestion performance. This approach requires allocating TileDB-VCF with scratch disk space using the --scratch-mb and --scratch-dir arguments.\ntiledbvcf store \\\n    --uri my_dataset \\\n    --scratch-dir \"$TMPDIR\" \\\n    --scratch-mb 4096 \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\nThe number of VCF files that are downloaded at a time is determined by the --sample-batch-size parameter, which defaults to 10. Downloading and ingestion happens asynchronously, so, for example, batch 3 will be downloaded as batch 2 is being ingestion. As a result, you must configure enough scratch space to store at least 20 samples, assuming a batch size of 10.",
    "crumbs": [
      "Home page",
      "How To",
      "Work with Cloud Object Stores"
    ]
  },
  {
    "objectID": "documentation/how-to/work-with-cloud-object-stores.html#authentication",
    "href": "documentation/how-to/work-with-cloud-object-stores.html#authentication",
    "title": "Work with Cloud Object Stores",
    "section": "",
    "text": "For TileDB to access a remote storage bucket you must be properly authenticated on the machine running TileDB. For S3, this means having access to the appropriate AWS access key ID and secret access key. This typically happens in one of three ways:\n\n\nIf the AWS Command Line Interface (CLI) is installed on your machine, running aws configure will store your credentials in a local profile that TileDB can access. You can verify the CLI has been previously configured by running:\naws s3 ls\nIf properly configured, this will output a list of the S3 buckets you (and thus TileDB) can access.\n\n\n\nYou can pass your AWS access key ID and secret access key to TileDB-VCF directly via the --tiledb-config argument, which expects a comma-separated string:\ntiledbvcf store \\\n    --uri my_dataset \\\n    --tiledb-config vfs.s3.aws_access_key_id=&lt;id&gt;,vfs.s3.aws_secret_access_key=&lt;secret&gt; \\\n    s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n\n\n\nYour AWS credentials can also be passed to TileDB by defining the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.",
    "crumbs": [
      "Home page",
      "How To",
      "Work with Cloud Object Stores"
    ]
  },
  {
    "objectID": "documentation/index.html",
    "href": "documentation/index.html",
    "title": "TileDB-VCF",
    "section": "",
    "text": "A C++ library for efficient storage and retrieval of genomic variant-call data using TileDB Embedded.",
    "crumbs": [
      "Home page",
      "TileDB-VCF"
    ]
  },
  {
    "objectID": "documentation/index.html#features",
    "href": "documentation/index.html#features",
    "title": "TileDB-VCF",
    "section": "Features",
    "text": "Features\n\nEasily ingest large amounts of variant-call data at scale\nSupports ingesting single sample VCF and BCF files\nNew samples are added incrementally, avoiding computationally expensive merging operations\nAllows for highly compressed storage using TileDB sparse arrays\nEfficient, parallelized queries of variant data stored locally or remotely on S3\nExport lossless VCF/BCF files or extract specific slices of a dataset",
    "crumbs": [
      "Home page",
      "TileDB-VCF"
    ]
  },
  {
    "objectID": "documentation/index.html#whats-included",
    "href": "documentation/index.html#whats-included",
    "title": "TileDB-VCF",
    "section": "What’s Included?",
    "text": "What’s Included?\n\nCommand line interface (CLI)\nAPIs for C, C++, Python, and Java",
    "crumbs": [
      "Home page",
      "TileDB-VCF"
    ]
  },
  {
    "objectID": "documentation/index.html#quick-start",
    "href": "documentation/index.html#quick-start",
    "title": "TileDB-VCF",
    "section": "Quick Start",
    "text": "Quick Start\nThe documentation website provides comprehensive usage examples but here are a few quick exercises to get you started.\nWe’ll use a dataset that includes 20 synthetic samples, each one containing over 20 million variants. We host a publicly accessible version of this dataset on S3, so if you have TileDB-VCF installed and you’d like to follow along just swap out the uri’s below for s3://tiledb-inc-demo-data/tiledbvcf-arrays/v4/vcf-samples-20. And if you don’t have TileDB-VCF installed yet, you can use our Docker images to test things out.\n\nCLI\nExport complete chr1 BCF files for a subset of samples:\ntiledbvcf export \\\n  --uri vcf-samples-20 \\\n  --regions chr1:1-248956422 \\\n  --sample-names v2-usVwJUmo,v2-WpXCYApL\nCreate a TSV file containing all variants within one or more regions of interest:\ntiledbvcf export \\\n  --uri vcf-samples-20 \\\n  --sample-names v2-tJjMfKyL,v2-eBAdKwID \\\n  -Ot --tsv-fields \"CHR,POS,REF,S:GT\" \\\n  --regions \"chr7:144000320-144008793,chr11:56490349-56491395\"\n\n\nPython\nRunning the same query in python\nimport tiledbvcf\n\nds = tiledbvcf.Dataset(uri = \"vcf-samples-20\", mode=\"r\")\n\nds.read(\n    attrs = [\"sample_name\", \"pos_start\", \"fmt_GT\"],\n    regions = [\"chr7:144000320-144008793\", \"chr11:56490349-56491395\"],\n    samples = [\"v2-tJjMfKyL\", \"v2-eBAdKwID\"]\n)\nreturns results as a pandas DataFrame\n     sample_name  pos_start    fmt_GT\n0    v2-nGEAqwFT  143999569  [-1, -1]\n1    v2-tJjMfKyL  144000262  [-1, -1]\n2    v2-tJjMfKyL  144000518  [-1, -1]\n3    v2-nGEAqwFT  144000339  [-1, -1]\n4    v2-nzLyDgYW  144000102  [-1, -1]\n..           ...        ...       ...\n566  v2-nGEAqwFT   56491395    [0, 0]\n567  v2-ijrKdkKh   56491373    [0, 0]\n568  v2-eBAdKwID   56491391    [0, 0]\n569  v2-tJjMfKyL   56491392  [-1, -1]\n570  v2-nzLyDgYW   56491365  [-1, -1]",
    "crumbs": [
      "Home page",
      "TileDB-VCF"
    ]
  },
  {
    "objectID": "documentation/index.html#want-to-learn-more",
    "href": "documentation/index.html#want-to-learn-more",
    "title": "TileDB-VCF",
    "section": "Want to Learn More?",
    "text": "Want to Learn More?\n\nBlog “Population Genomics is a Data Management Problem”\nCheck out the full documentation\n\nWhy use TileDB-VCF?\nData Model\nInstallation\nHow To\nReference",
    "crumbs": [
      "Home page",
      "TileDB-VCF"
    ]
  },
  {
    "objectID": "documentation/how-to/read-from-the-dataset.html",
    "href": "documentation/how-to/read-from-the-dataset.html",
    "title": "Read from the Dataset",
    "section": "",
    "text": "Before slicing the data, you may wish to get some information about your dataset, such as the sample names, the attributes you can query, etc.\n You can get the sample names as follows:\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.samples()\ntiledbvcf list -u my_vcf_dataset\nYou can get the attributes as follows:\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.attributes()                      # will print all queryable attributes\nds.attributes(attr_type = \"builtin\") # will print all materialized attributes\ntiledbvcf stat -u my_vcf_datset\n\n\n\nYou can rapidly read from a TileDB-VCF dataset by providing three main parameters (all optional):\n\nA subset of the samples\nA subset of the attributes\nOne or more genomic ranges\n\nEither as strings in format chr:pos_range\nOr via a BED file\n\n\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.read(\n    attrs = [\"alleles\", \"pos_start\", \"pos_end\"],\n    regions = [\"1:113409605-113475691\", \"1:113500000-113600000\"],\n    # or pass regions as follows:\n    # bed_file = &lt;bed_filename&gt;\n    samples = ['HG0099', 'HG00100']\n)\ntiledbvcf export \\\n    --uri my_vcf_dataset \\\n    --output-format t \\\n    --tsv-fields ALT,Q:POS,Q:END\n    --sample-names HG0099,HG00100\n    --regions 1:113409605-113475691,1:113500000-113600000\n    # or pass the regions in a BED file as follows:\n    # --regions-file &lt;bed_filename&gt;",
    "crumbs": [
      "Home page",
      "How To",
      "Read from the Dataset"
    ]
  },
  {
    "objectID": "documentation/how-to/read-from-the-dataset.html#basic-utils",
    "href": "documentation/how-to/read-from-the-dataset.html#basic-utils",
    "title": "Read from the Dataset",
    "section": "",
    "text": "Before slicing the data, you may wish to get some information about your dataset, such as the sample names, the attributes you can query, etc.\n You can get the sample names as follows:\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.samples()\ntiledbvcf list -u my_vcf_dataset\nYou can get the attributes as follows:\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.attributes()                      # will print all queryable attributes\nds.attributes(attr_type = \"builtin\") # will print all materialized attributes\ntiledbvcf stat -u my_vcf_datset",
    "crumbs": [
      "Home page",
      "How To",
      "Read from the Dataset"
    ]
  },
  {
    "objectID": "documentation/how-to/read-from-the-dataset.html#reading",
    "href": "documentation/how-to/read-from-the-dataset.html#reading",
    "title": "Read from the Dataset",
    "section": "",
    "text": "You can rapidly read from a TileDB-VCF dataset by providing three main parameters (all optional):\n\nA subset of the samples\nA subset of the attributes\nOne or more genomic ranges\n\nEither as strings in format chr:pos_range\nOr via a BED file\n\n\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"r\") # open in \"Read\" mode\nds.read(\n    attrs = [\"alleles\", \"pos_start\", \"pos_end\"],\n    regions = [\"1:113409605-113475691\", \"1:113500000-113600000\"],\n    # or pass regions as follows:\n    # bed_file = &lt;bed_filename&gt;\n    samples = ['HG0099', 'HG00100']\n)\ntiledbvcf export \\\n    --uri my_vcf_dataset \\\n    --output-format t \\\n    --tsv-fields ALT,Q:POS,Q:END\n    --sample-names HG0099,HG00100\n    --regions 1:113409605-113475691,1:113500000-113600000\n    # or pass the regions in a BED file as follows:\n    # --regions-file &lt;bed_filename&gt;",
    "crumbs": [
      "Home page",
      "How To",
      "Read from the Dataset"
    ]
  },
  {
    "objectID": "documentation/how-to/handle-large-queries.html",
    "href": "documentation/how-to/handle-large-queries.html",
    "title": "Handle Large Queries",
    "section": "",
    "text": "Unlike TileDB-VCF’s CLI, which exports directly to disk, results for queries performed using Python are read into memory. Therefore, when querying even moderately sized genomic datasets, the amount of available memory must be taken into consideration.\nThis guide demonstrates several of the TileDB-VCF features for overcoming memory limitations when querying large datasets. \n\n\nOne strategy for accommodating large queries is to simply increase the amount of memory available to tiledbvcf. By default tiledbvcf allocates 2GB of memory for queries. However, this value can be adjusted using the memory_budget_mb parameter. For the purposes of this tutorial the budget will be decreased to demonstrate how tiledbvcf is able to perform genome-scale queries even in a memory constrained environment.\nimport tiledbvcf\ncfg = tiledbvcf.ReadConfig(memory_budget_mb=256)\nds = tiledbvcf.Dataset(uri, mode = \"r\", cfg = cfg)\n\n\n\nFor queries that encompass many genomic regions you can simply provide an external bed file. In this example, you will query for any variants located in the promoter region of a known gene located on chromosomes 1-4.\nAfter performing a query, you can use read_completed() to verify whether or not all results were successfully returned.\nattrs = [\"sample_name\", \"contig\", \"pos_start\", \"fmt_GT\"]\ndf = ds.read(attrs, bed_file = \"data/gene-promoters-hg38.bed\")\nds.read_completed()\n\n## False\nIn this case, it returned False, indicating the requested data was too large to fit into the allocated memory so tiledbvcf retrieved as many records as possible in this first batch. The remaining records can be retrieved using continue_read(). Here, we’ve setup our code to accommodate the possibility that the full set of results are split across multiple batches.\nprint (\"The dataframe contains\")\n\nwhile not ds.read_completed():\n    print (f\"\\t...{df.shape[0]} rows\")\n    df = df.append(ds.continue_read())\n\nprint (f\"\\t...{df.shape[0]} rows\")\n\n## The dataframe contains\n##   ...1525201 rows\n##   ...3050402 rows\n##   ...3808687 rows\nHere is the final dataframe, which includes 3,808,687 records:\ndf\n\n##         sample_name contig  pos_start    fmt_GT\n## 0       v2-Qhhvcspe   chr1          1  [-1, -1]\n## 1       v2-YMaDHIoW   chr1          1  [-1, -1]\n## 2       v2-Mcwmkqnx   chr1          1  [-1, -1]\n## 3       v2-RzweTRSv   chr1          1  [-1, -1]\n## 4       v2-ijrKdkKh   chr1          1  [-1, -1]\n## ...             ...    ...        ...       ...\n## 758280  v2-PDeVyHSO   chr4  190063262    [0, 0]\n## 758281  v2-PDeVyHSO   chr4  190063264  [-1, -1]\n## 758282  v2-PDeVyHSO   chr4  190063265  [-1, -1]\n## 758283  v2-PDeVyHSO   chr4  190063392    [0, 0]\n## 758284  v2-PDeVyHSO   chr4  190063418  [-1, -1]\n## \n## [3808687 rows x 4 columns]\n\n\n\nA Python generator version of the read method is also provided. This pattern provides a powerful interface for batch processing variant data.\nds = tiledbvcf.Dataset(uri, mode = \"r\", cfg = cfg)\n\ndf = pd.DataFrame()\nfor batch in ds.read_iter(attrs, bed_file = \"data/gene-promoters-hg38.bed\"):\n    df = df.append(batch, ignore_index = True)\n\ndf\n\n##          sample_name contig  pos_start    fmt_GT\n## 0        v2-Qhhvcspe   chr1          1  [-1, -1]\n## 1        v2-YMaDHIoW   chr1          1  [-1, -1]\n## 2        v2-Mcwmkqnx   chr1          1  [-1, -1]\n## 3        v2-RzweTRSv   chr1          1  [-1, -1]\n## 4        v2-ijrKdkKh   chr1          1  [-1, -1]\n## ...              ...    ...        ...       ...\n## 3808682  v2-PDeVyHSO   chr4  190063262    [0, 0]\n## 3808683  v2-PDeVyHSO   chr4  190063264  [-1, -1]\n## 3808684  v2-PDeVyHSO   chr4  190063265  [-1, -1]\n## 3808685  v2-PDeVyHSO   chr4  190063392    [0, 0]\n## 3808686  v2-PDeVyHSO   chr4  190063418  [-1, -1]\n## \n## [3808687 rows x 4 columns]",
    "crumbs": [
      "Home page",
      "How To",
      "Handle Large Queries"
    ]
  },
  {
    "objectID": "documentation/how-to/handle-large-queries.html#setting-the-memory-budget",
    "href": "documentation/how-to/handle-large-queries.html#setting-the-memory-budget",
    "title": "Handle Large Queries",
    "section": "",
    "text": "One strategy for accommodating large queries is to simply increase the amount of memory available to tiledbvcf. By default tiledbvcf allocates 2GB of memory for queries. However, this value can be adjusted using the memory_budget_mb parameter. For the purposes of this tutorial the budget will be decreased to demonstrate how tiledbvcf is able to perform genome-scale queries even in a memory constrained environment.\nimport tiledbvcf\ncfg = tiledbvcf.ReadConfig(memory_budget_mb=256)\nds = tiledbvcf.Dataset(uri, mode = \"r\", cfg = cfg)",
    "crumbs": [
      "Home page",
      "How To",
      "Handle Large Queries"
    ]
  },
  {
    "objectID": "documentation/how-to/handle-large-queries.html#performing-batched-reads",
    "href": "documentation/how-to/handle-large-queries.html#performing-batched-reads",
    "title": "Handle Large Queries",
    "section": "",
    "text": "For queries that encompass many genomic regions you can simply provide an external bed file. In this example, you will query for any variants located in the promoter region of a known gene located on chromosomes 1-4.\nAfter performing a query, you can use read_completed() to verify whether or not all results were successfully returned.\nattrs = [\"sample_name\", \"contig\", \"pos_start\", \"fmt_GT\"]\ndf = ds.read(attrs, bed_file = \"data/gene-promoters-hg38.bed\")\nds.read_completed()\n\n## False\nIn this case, it returned False, indicating the requested data was too large to fit into the allocated memory so tiledbvcf retrieved as many records as possible in this first batch. The remaining records can be retrieved using continue_read(). Here, we’ve setup our code to accommodate the possibility that the full set of results are split across multiple batches.\nprint (\"The dataframe contains\")\n\nwhile not ds.read_completed():\n    print (f\"\\t...{df.shape[0]} rows\")\n    df = df.append(ds.continue_read())\n\nprint (f\"\\t...{df.shape[0]} rows\")\n\n## The dataframe contains\n##   ...1525201 rows\n##   ...3050402 rows\n##   ...3808687 rows\nHere is the final dataframe, which includes 3,808,687 records:\ndf\n\n##         sample_name contig  pos_start    fmt_GT\n## 0       v2-Qhhvcspe   chr1          1  [-1, -1]\n## 1       v2-YMaDHIoW   chr1          1  [-1, -1]\n## 2       v2-Mcwmkqnx   chr1          1  [-1, -1]\n## 3       v2-RzweTRSv   chr1          1  [-1, -1]\n## 4       v2-ijrKdkKh   chr1          1  [-1, -1]\n## ...             ...    ...        ...       ...\n## 758280  v2-PDeVyHSO   chr4  190063262    [0, 0]\n## 758281  v2-PDeVyHSO   chr4  190063264  [-1, -1]\n## 758282  v2-PDeVyHSO   chr4  190063265  [-1, -1]\n## 758283  v2-PDeVyHSO   chr4  190063392    [0, 0]\n## 758284  v2-PDeVyHSO   chr4  190063418  [-1, -1]\n## \n## [3808687 rows x 4 columns]",
    "crumbs": [
      "Home page",
      "How To",
      "Handle Large Queries"
    ]
  },
  {
    "objectID": "documentation/how-to/handle-large-queries.html#iteration",
    "href": "documentation/how-to/handle-large-queries.html#iteration",
    "title": "Handle Large Queries",
    "section": "",
    "text": "A Python generator version of the read method is also provided. This pattern provides a powerful interface for batch processing variant data.\nds = tiledbvcf.Dataset(uri, mode = \"r\", cfg = cfg)\n\ndf = pd.DataFrame()\nfor batch in ds.read_iter(attrs, bed_file = \"data/gene-promoters-hg38.bed\"):\n    df = df.append(batch, ignore_index = True)\n\ndf\n\n##          sample_name contig  pos_start    fmt_GT\n## 0        v2-Qhhvcspe   chr1          1  [-1, -1]\n## 1        v2-YMaDHIoW   chr1          1  [-1, -1]\n## 2        v2-Mcwmkqnx   chr1          1  [-1, -1]\n## 3        v2-RzweTRSv   chr1          1  [-1, -1]\n## 4        v2-ijrKdkKh   chr1          1  [-1, -1]\n## ...              ...    ...        ...       ...\n## 3808682  v2-PDeVyHSO   chr4  190063262    [0, 0]\n## 3808683  v2-PDeVyHSO   chr4  190063264  [-1, -1]\n## 3808684  v2-PDeVyHSO   chr4  190063265  [-1, -1]\n## 3808685  v2-PDeVyHSO   chr4  190063392    [0, 0]\n## 3808686  v2-PDeVyHSO   chr4  190063418  [-1, -1]\n## \n## [3808687 rows x 4 columns]",
    "crumbs": [
      "Home page",
      "How To",
      "Handle Large Queries"
    ]
  },
  {
    "objectID": "documentation/how-to/ingest-samples.html",
    "href": "documentation/how-to/ingest-samples.html",
    "title": "Ingest Samples",
    "section": "",
    "text": "Ingest Samples\n\n\n\n\n\n\nNote\n\n\n\nIndexed files are required for ingestion. If your VCF/BCF files have not been indexed, you can use bcftoolsto do so:\n\n\nfor f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\nYou can ingest samples into an already created dataset as follows:\nimport tiledbvcf\n\nuri = \"my_vcf_dataset\" \nds = tiledbvcf.Dataset(uri, mode = \"w\")\nds.ingest_samples(sample_uris = [\"sample_1\", \"samples_2\"])\nJust add a regular expression for the VCF file locations at the end of the store command:\ntiledbvcf store --uri my_vcf_dataset *.bcf \nAlternatively, provide a text file with the absolute locations of the VCF files, separated by a new line:\ntiledbvcf store --uri my_vcf_dataset --samples-file samples.txt\n\n\n\n\n\n\nNote\n\n\n\nIncremental updates work in the same manner as the ingestion above, nothing special is needed. In addition, the ingestion is thread- and process-safe and, therefore, can be performed in parallel.",
    "crumbs": [
      "Home page",
      "How To",
      "Ingest Samples"
    ]
  },
  {
    "objectID": "documentation/how-to/overview.html",
    "href": "documentation/how-to/overview.html",
    "title": "How To",
    "section": "",
    "text": "How To\nThis section includes useful guides about the usage of TileDB-VCF:\n{% content-ref url=“handle-large-queries.md” %} handle-large-queries.md {% endcontent-ref %}\n{% content-ref url=“work-with-cloud-object-stores.md” %} work-with-cloud-object-stores.md {% endcontent-ref %}"
  },
  {
    "objectID": "documentation/reference/ReadConfig.html",
    "href": "documentation/reference/ReadConfig.html",
    "title": "ReadConfig",
    "section": "",
    "text": "ReadConfig\nConfig settings for a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nlimit\nint\nMax number of records (rows) to read\n\n\nregion_partition\ntuple\nRegion partition tuple (idx, num_partitions)\n\n\nsample_partition\ntuple\nSamples partition tuple (idx, num_partitions)\n\n\nsort_regions\nbool\nWhether or not to sort the regions to be read, default True\n\n\nmemory_budget_mb\nint\nMemory budget (MB) for buffer and internal allocations, default 2048MB\n\n\ntiledb_config\nList[str]\nList of strings of format ‘option=value’\n\n\nbuffer_percentage\nint\nPercentage of memory to dedicate to TileDB Query Buffers, default 25\n\n\ntiledb_tile_cache_percentage\nint\nPercentage of memory to dedicate to TileDB Tile Cache, default 10"
  },
  {
    "objectID": "documentation/reference/ReadConfig.html#attributes",
    "href": "documentation/reference/ReadConfig.html#attributes",
    "title": "ReadConfig",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nlimit\nint\nMax number of records (rows) to read\n\n\nregion_partition\ntuple\nRegion partition tuple (idx, num_partitions)\n\n\nsample_partition\ntuple\nSamples partition tuple (idx, num_partitions)\n\n\nsort_regions\nbool\nWhether or not to sort the regions to be read, default True\n\n\nmemory_budget_mb\nint\nMemory budget (MB) for buffer and internal allocations, default 2048MB\n\n\ntiledb_config\nList[str]\nList of strings of format ‘option=value’\n\n\nbuffer_percentage\nint\nPercentage of memory to dedicate to TileDB Query Buffers, default 25\n\n\ntiledb_tile_cache_percentage\nint\nPercentage of memory to dedicate to TileDB Tile Cache, default 10"
  },
  {
    "objectID": "documentation/reference/Dataset.html",
    "href": "documentation/reference/Dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "Dataset(self, uri, mode='r', cfg=None, stats=False, verbose=False, tiledb_config=None)\nA class that provides read/write access to a TileDB-VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of the dataset.\nrequired\n\n\nmode\nstr\nMode of operation (‘r’|‘w’)\n'r'\n\n\ncfg\nReadConfig\nTileDB-VCF configuration.\nNone\n\n\nstats\nbool\nEnable internal TileDB statistics.\nFalse\n\n\nverbose\nbool\nEnable verbose output.\nFalse\n\n\ntiledb_config\ndict\nTileDB configuration, alternative to cfg.tiledb_config.\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nRegion\nRepresents a 1-based inclusive region.\n\n\n\n\n\ndataset.Dataset.Region(region)\nRepresents a 1-based inclusive region.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregion\nstr\nA string in the form “:-”.\nrequired\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nattributes\nReturn a list of queryable attributes available in the VCF dataset.\n\n\nclose\nClose the dataset and release resources.\n\n\ncontinue_read\nContinue an incomplete read.\n\n\ncontinue_read_arrow\nContinue an incomplete read.\n\n\ncount\nCount records in the dataset.\n\n\ncreate_dataset\nCreate a new dataset.\n\n\ndelete\nDelete the dataset.\n\n\nexport\nExports data to multiple VCF files or a combined VCF file.\n\n\ningest_samples\nIngest VCF files into the dataset.\n\n\nread\nRead data from the dataset into a Pandas DataFrame.\n\n\nread_allele_count\nRead allele count from the dataset into a Pandas DataFrame\n\n\nread_allele_count_arrow\nRead allele count from the dataset into a Pandas DataFrame\n\n\nread_arrow\nRead data from the dataset into a PyArrow Table.\n\n\nread_completed\nReturns true if the previous read operation was complete.\n\n\nread_iter\nIterator version of read().\n\n\nread_variant_stats\nRead variant stats from the dataset into a Pandas DataFrame\n\n\nread_variant_stats_arrow\nRead variant stats from the dataset into a PyArrow Table\n\n\nsample_count\nGet the number of samples in the dataset.\n\n\nsamples\nGet the list of samples in the dataset.\n\n\nschema_version\nGet the VCF schema version of the dataset.\n\n\ntiledb_stats\nGet TileDB stats as a string.\n\n\nversion\nReturn the TileDB-VCF version used to create the dataset.\n\n\n\n\n\nDataset.attributes(attr_type='all')\nReturn a list of queryable attributes available in the VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattr_type\nstr\nThe subset of attributes to retrieve; “info” or “fmt” will only retrieve attributes ingested from the VCF INFO and FORMAT fields, respectively, “builtin” retrieves the static attributes defined in TileDB-VCF’s schema, “all” (the default) returns all queryable attributes.\n'all'\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nA list of attribute names.\n\n\n\n\n\n\n\nDataset.close()\nClose the dataset and release resources.\n\n\n\nDataset.continue_read(release_buffers=True)\nContinue an incomplete read.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrelease_buffers\nbool\nRelease the buffers after reading.\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe next batch of data as a Pandas DataFrame.\n\n\n\n\n\n\n\nDataset.continue_read_arrow(release_buffers=True)\nContinue an incomplete read.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrelease_buffers\nbool\nRelease the buffers after reading.\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npa.Table\nThe next batch of data as a PyArrow Table.\n\n\n\n\n\n\n\nDataset.count(samples=None, regions=None)\nCount records in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\n(str, List[str])\nSample names to include in the count.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to include in the count.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nNumber of intersecting records in the dataset.\n\n\n\n\n\n\n\nDataset.create_dataset(extra_attrs=None, vcf_attrs=None, tile_capacity=10000, anchor_gap=1000, checksum_type='sha256', allow_duplicates=True, enable_allele_count=True, enable_variant_stats=True, enable_sample_stats=True, compress_sample_dim=True, compression_level=4, variant_stats_version=2)\nCreate a new dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nextra_attrs\nstr\nCSV list of extra attributes to materialize from fmt and info fields.\nNone\n\n\nvcf_attrs\nstr\nURI of VCF file with all fmt and info fields to materialize in the dataset.\nNone\n\n\ntile_capacity\nint\nTile capacity to use for the array schema.\n10000\n\n\nanchor_gap\nint\nLength of gaps between inserted anchor records in bases.\n1000\n\n\nchecksum_type\nstr\nOptional checksum type for the dataset, “sha256” or “md5”.\n'sha256'\n\n\nallow_duplicates\nbool\nAllow records with duplicate start positions to be written to the array.\nTrue\n\n\nenable_allele_count\nbool\nEnable the allele count ingestion task.\nTrue\n\n\nenable_variant_stats\nbool\nEnable the variant stats ingestion task.\nTrue\n\n\nenable_sample_stats\nbool\nEnable the sample stats ingestion task.\nTrue\n\n\ncompress_sample_dim\nbool\nEnable compression on the sample dimension.\nTrue\n\n\ncompression_level\nint\nCompression level for zstd compression.\n4\n\n\nvariant_stats_version\nint\nVersion of the variant stats array.\n2\n\n\n\n\n\n\n\nDataset.delete(uri, *, config=None)\nDelete the dataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of the dataset.\nrequired\n\n\nconfig\ndict\nTileDB configuration.\nNone\n\n\n\n\n\n\n\nDataset.export(samples=None, regions=None, samples_file=None, bed_file=None, skip_check_samples=False, merge=False, output_format='z', output_path='', output_dir='.')\nExports data to multiple VCF files or a combined VCF file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\n(str, List[str])\nSample names to be read.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to be read.\nNone\n\n\nsamples_file\nstr\nURI of file containing sample names to be read, one per line.\nNone\n\n\nbed_file\nstr\nURI of a BED file of genomic regions to be read.\nNone\n\n\nskip_check_samples\nbool\nSkip checking if the samples in samples_file exist in the dataset.\nFalse\n\n\nset_af_filter\n\nFilter variants by internal allele frequency. For example, to include variants with AF &gt; 0.1, set this to “&gt;0.1”.\nrequired\n\n\nscan_all_samples\n\nScan all samples when computing internal allele frequency.\nrequired\n\n\nmerge\nbool\nMerge samples to create a combined VCF file.\nFalse\n\n\noutput_format\nstr\nExport file format: ‘b’: bcf (compressed), ‘u’: bcf, ‘z’:vcf.gz, ‘v’: vcf.\n'z'\n\n\noutput_path\nstr\nCombined VCF output file.\n''\n\n\noutput_dir\nstr\nDirectory used for local output of exported samples.\n'.'\n\n\n\n\n\n\n\nDataset.ingest_samples(sample_uris=None, threads=None, total_memory_budget_mb=None, total_memory_percentage=None, ratio_tiledb_memory=None, max_tiledb_memory_mb=None, input_record_buffer_mb=None, avg_vcf_record_size=None, ratio_task_size=None, ratio_output_flush=None, scratch_space_path=None, scratch_space_size=None, sample_batch_size=None, resume=False, contig_fragment_merging=True, contigs_to_keep_separate=None, contigs_to_allow_merging=None, contig_mode='all', thread_task_size=None, memory_budget_mb=None, record_limit=None)\nIngest VCF files into the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_uris\nList[str]\nList of sample URIs to ingest.\nNone\n\n\nthreads\nint\nSet the number of threads used for ingestion.\nNone\n\n\ntotal_memory_budget_mb\nint\nTotal memory budget for ingestion (MiB).\nNone\n\n\ntotal_memory_percentage\nfloat\nPercentage of total system memory used for ingestion (overrides ‘total_memory_budget_mb’).\nNone\n\n\nratio_tiledb_memory\nfloat\nRatio of memory budget allocated to TileDB::sm.mem.total_budget.\nNone\n\n\nmax_tiledb_memory_mb\nint\nMaximum memory allocated to TileDB::sm.mem.total_budget (MiB).\nNone\n\n\ninput_record_buffer_mb\nint\nSize of input record buffer for each sample file (MiB).\nNone\n\n\navg_vcf_record_size\nint\nAverage VCF record size (bytes).\nNone\n\n\nratio_task_size\nfloat\nRatio of worker task size to computed task size.\nNone\n\n\nratio_output_flush\nfloat\nRatio of output buffer capacity that triggers a flush to TileDB.\nNone\n\n\nscratch_space_path\nstr\nDirectory used for local storage of downloaded remote samples.\nNone\n\n\nscratch_space_size\nint\nAmount of local storage that can be used for downloading remote samples (MB).\nNone\n\n\nsample_batch_size\nint\nNumber of samples per batch for ingestion (default 10).\nNone\n\n\nresume\nbool\nWhether to check and attempt to resume a partial completed ingestion.\nFalse\n\n\ncontig_fragment_merging\nbool\nWhether to enable merging of contigs into fragments. This overrides the contigs-to-keep-separate/contigs-to-allow- merging options. Generally contig fragment merging is good, this is a performance optimization to reduce the prefixes on a s3/azure/gcs bucket when there is a large number of pseudo contigs which are small in size.\nTrue\n\n\ncontigs_to_keep_separate\nList[str]\nList of contigs that should not be merged into combined fragments. The default list includes all standard human chromosomes in both UCSC (e.g., chr1) and Ensembl (e.g., 1) formats.\nNone\n\n\ncontigs_to_allow_merging\nList[str]\nList of contigs that should be allowed to be merged into combined fragments.\nNone\n\n\ncontig_mode\nstr\nSelect which contigs are ingested: ‘all’, ‘separate’, or ‘merged’.\n'all'\n\n\nthread_task_size\nint\nDEPRECATED - This parameter will be removed in a future release.\nNone\n\n\nmemory_budget_mb\nint\nDEPRECATED - This parameter will be removed in a future release.\nNone\n\n\nrecord_limit\nint\nDEPRECATED - This parameter will be removed in a future release.\nNone\n\n\n\n\n\n\n\nDataset.read(attrs=DEFAULT_ATTRS, samples=None, regions=None, samples_file=None, bed_file=None, skip_check_samples=False, set_af_filter='', scan_all_samples=False)\nRead data from the dataset into a Pandas DataFrame.\nFor large datasets, a call to read() may not be able to fit all results in memory. In that case, the returned table will contain as many results as possible, and in order to retrieve the rest of the results, use the continue_read() function.\nYou can also use the Python generator version, read_iter().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattrs\nList[str]\nList of attribute names to be read.\nDEFAULT_ATTRS\n\n\nsamples\n(str, List[str])\nSample names to be read.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to be read.\nNone\n\n\nsamples_file\nstr\nURI of file containing sample names to be read, one per line.\nNone\n\n\nbed_file\nstr\nURI of a BED file of genomic regions to be read.\nNone\n\n\nskip_check_samples\nbool\nSkip checking if the samples in samples_file exist in the dataset.\nFalse\n\n\nset_af_filter\nstr\nFilter variants by internal allele frequency. For example, to include variants with AF &gt; 0.1, set this to “&gt;0.1”.\n''\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nQuery results as a Pandas DataFrame.\n\n\n\n\n\n\n\nDataset.read_allele_count(region=None, regions=None)\nRead allele count from the dataset into a Pandas DataFrame\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregions\nList[str]\nGenomic regions to be queried.\nNone\n\n\nregion\nstr\nDEPRECATED - Genomic region to be queried.\nNone\n\n\n\n\n\n\n\nDataset.read_allele_count_arrow(region=None, regions=None)\nRead allele count from the dataset into a Pandas DataFrame\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregions\nList[str]\nGenomic regions to be queried.\nNone\n\n\nregion\nstr\nDEPRECATED - Genomic region to be queried.\nNone\n\n\n\n\n\n\n\nDataset.read_arrow(attrs=DEFAULT_ATTRS, samples=None, regions=None, samples_file=None, bed_file=None, skip_check_samples=False, set_af_filter='', scan_all_samples=False)\nRead data from the dataset into a PyArrow Table.\nFor large queries, a call to read_arrow() may not be able to fit all results in memory. In that case, the returned table will contain as many results as possible, and in order to retrieve the rest of the results, use the continue_read() function.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattrs\nList[str]\nList of attribute names to be read.\nDEFAULT_ATTRS\n\n\nsamples\n(str, List[str])\nSample names to be read.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to be read.\nNone\n\n\nsamples_file\nstr\nURI of file containing sample names to be read, one per line.\nNone\n\n\nbed_file\nstr\nURI of a BED file of genomic regions to be read.\nNone\n\n\nskip_check_samples\nbool\nSkip checking if the samples in samples_file exist in the dataset.\nFalse\n\n\nset_af_filter\nstr\nFilter variants by internal allele frequency. For example, to include variants with AF &gt; 0.1, set this to “&gt;0.1”.\n''\n\n\nscan_all_samples\nbool\nScan all samples when computing internal allele frequency.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npa.Table\nQuery results as a PyArrow Table.\n\n\n\n\n\n\n\nDataset.read_completed()\nReturns true if the previous read operation was complete. A read is considered complete if the resulting dataframe contained all results.\n\n\n\n\n\nType\nDescription\n\n\n\n\nTrue if the previous read operation was complete.\n\n\n\n\n\n\n\n\nDataset.read_iter(attrs=DEFAULT_ATTRS, samples=None, regions=None, samples_file=None, bed_file=None)\nIterator version of read().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattrs\nList[str]\nList of attribute names to be read.\nDEFAULT_ATTRS\n\n\nsamples\n(str, List[str])\nSample names to be read.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to be read.\nNone\n\n\nsamples_file\nstr\nURI of file containing sample names to be read, one per line.\nNone\n\n\nbed_file\nstr\nURI of a BED file of genomic regions to be read.\nNone\n\n\n\n\n\n\n\nDataset.read_variant_stats(region=None, drop_ref=False, regions=None, scan_all_samples=False)\nRead variant stats from the dataset into a Pandas DataFrame\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndrop_ref\nbool\nOmit “ref” alleles from the results\nFalse\n\n\nregions\nList[str]\nGenomic regions to be queried.\nNone\n\n\nscan_all_samples\nbool\nScan all samples when computing internal allele frequency.\nFalse\n\n\nregion\nstr\nDEPRECATED - Genomic region to be queried.\nNone\n\n\n\n\n\n\n\nDataset.read_variant_stats_arrow(region=None, drop_ref=False, regions=None, scan_all_samples=False)\nRead variant stats from the dataset into a PyArrow Table\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndrop_ref\nbool\nOmit “ref” alleles from the results\nFalse\n\n\nregions\nList[str]\nGenomic regions to be queried.\nNone\n\n\nscan_all_samples\nbool\nScan all samples when computing internal allele frequency.\nFalse\n\n\nregion\nstr\nDEPRECATED - Genomic region to be queried.\nNone\n\n\n\n\n\n\n\nDataset.sample_count()\nGet the number of samples in the dataset.\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nNumber of samples in the dataset.\n\n\n\n\n\n\n\nDataset.samples()\nGet the list of samples in the dataset.\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nList of samples in the dataset.\n\n\n\n\n\n\n\nDataset.schema_version()\nGet the VCF schema version of the dataset.\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nVCF schema version of the dataset.\n\n\n\n\n\n\n\nDataset.tiledb_stats()\nGet TileDB stats as a string.\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nTileDB stats as a string.\n\n\n\n\n\n\n\nDataset.version()\nReturn the TileDB-VCF version used to create the dataset.\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe TileDB-VCF version."
  },
  {
    "objectID": "documentation/reference/Dataset.html#parameters",
    "href": "documentation/reference/Dataset.html#parameters",
    "title": "Dataset",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of the dataset.\nrequired\n\n\nmode\nstr\nMode of operation (‘r’|‘w’)\n'r'\n\n\ncfg\nReadConfig\nTileDB-VCF configuration.\nNone\n\n\nstats\nbool\nEnable internal TileDB statistics.\nFalse\n\n\nverbose\nbool\nEnable verbose output.\nFalse\n\n\ntiledb_config\ndict\nTileDB configuration, alternative to cfg.tiledb_config.\nNone"
  },
  {
    "objectID": "documentation/reference/Dataset.html#classes",
    "href": "documentation/reference/Dataset.html#classes",
    "title": "Dataset",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nRegion\nRepresents a 1-based inclusive region.\n\n\n\n\n\ndataset.Dataset.Region(region)\nRepresents a 1-based inclusive region.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregion\nstr\nA string in the form “:-”.\nrequired"
  },
  {
    "objectID": "documentation/reference/Dataset.html#methods",
    "href": "documentation/reference/Dataset.html#methods",
    "title": "Dataset",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nattributes\nReturn a list of queryable attributes available in the VCF dataset.\n\n\nclose\nClose the dataset and release resources.\n\n\ncontinue_read\nContinue an incomplete read.\n\n\ncontinue_read_arrow\nContinue an incomplete read.\n\n\ncount\nCount records in the dataset.\n\n\ncreate_dataset\nCreate a new dataset.\n\n\ndelete\nDelete the dataset.\n\n\nexport\nExports data to multiple VCF files or a combined VCF file.\n\n\ningest_samples\nIngest VCF files into the dataset.\n\n\nread\nRead data from the dataset into a Pandas DataFrame.\n\n\nread_allele_count\nRead allele count from the dataset into a Pandas DataFrame\n\n\nread_allele_count_arrow\nRead allele count from the dataset into a Pandas DataFrame\n\n\nread_arrow\nRead data from the dataset into a PyArrow Table.\n\n\nread_completed\nReturns true if the previous read operation was complete.\n\n\nread_iter\nIterator version of read().\n\n\nread_variant_stats\nRead variant stats from the dataset into a Pandas DataFrame\n\n\nread_variant_stats_arrow\nRead variant stats from the dataset into a PyArrow Table\n\n\nsample_count\nGet the number of samples in the dataset.\n\n\nsamples\nGet the list of samples in the dataset.\n\n\nschema_version\nGet the VCF schema version of the dataset.\n\n\ntiledb_stats\nGet TileDB stats as a string.\n\n\nversion\nReturn the TileDB-VCF version used to create the dataset.\n\n\n\n\n\nDataset.attributes(attr_type='all')\nReturn a list of queryable attributes available in the VCF dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattr_type\nstr\nThe subset of attributes to retrieve; “info” or “fmt” will only retrieve attributes ingested from the VCF INFO and FORMAT fields, respectively, “builtin” retrieves the static attributes defined in TileDB-VCF’s schema, “all” (the default) returns all queryable attributes.\n'all'\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nA list of attribute names.\n\n\n\n\n\n\n\nDataset.close()\nClose the dataset and release resources.\n\n\n\nDataset.continue_read(release_buffers=True)\nContinue an incomplete read.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrelease_buffers\nbool\nRelease the buffers after reading.\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nThe next batch of data as a Pandas DataFrame.\n\n\n\n\n\n\n\nDataset.continue_read_arrow(release_buffers=True)\nContinue an incomplete read.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrelease_buffers\nbool\nRelease the buffers after reading.\nTrue\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npa.Table\nThe next batch of data as a PyArrow Table.\n\n\n\n\n\n\n\nDataset.count(samples=None, regions=None)\nCount records in the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\n(str, List[str])\nSample names to include in the count.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to include in the count.\nNone\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nNumber of intersecting records in the dataset.\n\n\n\n\n\n\n\nDataset.create_dataset(extra_attrs=None, vcf_attrs=None, tile_capacity=10000, anchor_gap=1000, checksum_type='sha256', allow_duplicates=True, enable_allele_count=True, enable_variant_stats=True, enable_sample_stats=True, compress_sample_dim=True, compression_level=4, variant_stats_version=2)\nCreate a new dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nextra_attrs\nstr\nCSV list of extra attributes to materialize from fmt and info fields.\nNone\n\n\nvcf_attrs\nstr\nURI of VCF file with all fmt and info fields to materialize in the dataset.\nNone\n\n\ntile_capacity\nint\nTile capacity to use for the array schema.\n10000\n\n\nanchor_gap\nint\nLength of gaps between inserted anchor records in bases.\n1000\n\n\nchecksum_type\nstr\nOptional checksum type for the dataset, “sha256” or “md5”.\n'sha256'\n\n\nallow_duplicates\nbool\nAllow records with duplicate start positions to be written to the array.\nTrue\n\n\nenable_allele_count\nbool\nEnable the allele count ingestion task.\nTrue\n\n\nenable_variant_stats\nbool\nEnable the variant stats ingestion task.\nTrue\n\n\nenable_sample_stats\nbool\nEnable the sample stats ingestion task.\nTrue\n\n\ncompress_sample_dim\nbool\nEnable compression on the sample dimension.\nTrue\n\n\ncompression_level\nint\nCompression level for zstd compression.\n4\n\n\nvariant_stats_version\nint\nVersion of the variant stats array.\n2\n\n\n\n\n\n\n\nDataset.delete(uri, *, config=None)\nDelete the dataset.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nURI of the dataset.\nrequired\n\n\nconfig\ndict\nTileDB configuration.\nNone\n\n\n\n\n\n\n\nDataset.export(samples=None, regions=None, samples_file=None, bed_file=None, skip_check_samples=False, merge=False, output_format='z', output_path='', output_dir='.')\nExports data to multiple VCF files or a combined VCF file.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsamples\n(str, List[str])\nSample names to be read.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to be read.\nNone\n\n\nsamples_file\nstr\nURI of file containing sample names to be read, one per line.\nNone\n\n\nbed_file\nstr\nURI of a BED file of genomic regions to be read.\nNone\n\n\nskip_check_samples\nbool\nSkip checking if the samples in samples_file exist in the dataset.\nFalse\n\n\nset_af_filter\n\nFilter variants by internal allele frequency. For example, to include variants with AF &gt; 0.1, set this to “&gt;0.1”.\nrequired\n\n\nscan_all_samples\n\nScan all samples when computing internal allele frequency.\nrequired\n\n\nmerge\nbool\nMerge samples to create a combined VCF file.\nFalse\n\n\noutput_format\nstr\nExport file format: ‘b’: bcf (compressed), ‘u’: bcf, ‘z’:vcf.gz, ‘v’: vcf.\n'z'\n\n\noutput_path\nstr\nCombined VCF output file.\n''\n\n\noutput_dir\nstr\nDirectory used for local output of exported samples.\n'.'\n\n\n\n\n\n\n\nDataset.ingest_samples(sample_uris=None, threads=None, total_memory_budget_mb=None, total_memory_percentage=None, ratio_tiledb_memory=None, max_tiledb_memory_mb=None, input_record_buffer_mb=None, avg_vcf_record_size=None, ratio_task_size=None, ratio_output_flush=None, scratch_space_path=None, scratch_space_size=None, sample_batch_size=None, resume=False, contig_fragment_merging=True, contigs_to_keep_separate=None, contigs_to_allow_merging=None, contig_mode='all', thread_task_size=None, memory_budget_mb=None, record_limit=None)\nIngest VCF files into the dataset.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsample_uris\nList[str]\nList of sample URIs to ingest.\nNone\n\n\nthreads\nint\nSet the number of threads used for ingestion.\nNone\n\n\ntotal_memory_budget_mb\nint\nTotal memory budget for ingestion (MiB).\nNone\n\n\ntotal_memory_percentage\nfloat\nPercentage of total system memory used for ingestion (overrides ‘total_memory_budget_mb’).\nNone\n\n\nratio_tiledb_memory\nfloat\nRatio of memory budget allocated to TileDB::sm.mem.total_budget.\nNone\n\n\nmax_tiledb_memory_mb\nint\nMaximum memory allocated to TileDB::sm.mem.total_budget (MiB).\nNone\n\n\ninput_record_buffer_mb\nint\nSize of input record buffer for each sample file (MiB).\nNone\n\n\navg_vcf_record_size\nint\nAverage VCF record size (bytes).\nNone\n\n\nratio_task_size\nfloat\nRatio of worker task size to computed task size.\nNone\n\n\nratio_output_flush\nfloat\nRatio of output buffer capacity that triggers a flush to TileDB.\nNone\n\n\nscratch_space_path\nstr\nDirectory used for local storage of downloaded remote samples.\nNone\n\n\nscratch_space_size\nint\nAmount of local storage that can be used for downloading remote samples (MB).\nNone\n\n\nsample_batch_size\nint\nNumber of samples per batch for ingestion (default 10).\nNone\n\n\nresume\nbool\nWhether to check and attempt to resume a partial completed ingestion.\nFalse\n\n\ncontig_fragment_merging\nbool\nWhether to enable merging of contigs into fragments. This overrides the contigs-to-keep-separate/contigs-to-allow- merging options. Generally contig fragment merging is good, this is a performance optimization to reduce the prefixes on a s3/azure/gcs bucket when there is a large number of pseudo contigs which are small in size.\nTrue\n\n\ncontigs_to_keep_separate\nList[str]\nList of contigs that should not be merged into combined fragments. The default list includes all standard human chromosomes in both UCSC (e.g., chr1) and Ensembl (e.g., 1) formats.\nNone\n\n\ncontigs_to_allow_merging\nList[str]\nList of contigs that should be allowed to be merged into combined fragments.\nNone\n\n\ncontig_mode\nstr\nSelect which contigs are ingested: ‘all’, ‘separate’, or ‘merged’.\n'all'\n\n\nthread_task_size\nint\nDEPRECATED - This parameter will be removed in a future release.\nNone\n\n\nmemory_budget_mb\nint\nDEPRECATED - This parameter will be removed in a future release.\nNone\n\n\nrecord_limit\nint\nDEPRECATED - This parameter will be removed in a future release.\nNone\n\n\n\n\n\n\n\nDataset.read(attrs=DEFAULT_ATTRS, samples=None, regions=None, samples_file=None, bed_file=None, skip_check_samples=False, set_af_filter='', scan_all_samples=False)\nRead data from the dataset into a Pandas DataFrame.\nFor large datasets, a call to read() may not be able to fit all results in memory. In that case, the returned table will contain as many results as possible, and in order to retrieve the rest of the results, use the continue_read() function.\nYou can also use the Python generator version, read_iter().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattrs\nList[str]\nList of attribute names to be read.\nDEFAULT_ATTRS\n\n\nsamples\n(str, List[str])\nSample names to be read.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to be read.\nNone\n\n\nsamples_file\nstr\nURI of file containing sample names to be read, one per line.\nNone\n\n\nbed_file\nstr\nURI of a BED file of genomic regions to be read.\nNone\n\n\nskip_check_samples\nbool\nSkip checking if the samples in samples_file exist in the dataset.\nFalse\n\n\nset_af_filter\nstr\nFilter variants by internal allele frequency. For example, to include variants with AF &gt; 0.1, set this to “&gt;0.1”.\n''\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npd.DataFrame\nQuery results as a Pandas DataFrame.\n\n\n\n\n\n\n\nDataset.read_allele_count(region=None, regions=None)\nRead allele count from the dataset into a Pandas DataFrame\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregions\nList[str]\nGenomic regions to be queried.\nNone\n\n\nregion\nstr\nDEPRECATED - Genomic region to be queried.\nNone\n\n\n\n\n\n\n\nDataset.read_allele_count_arrow(region=None, regions=None)\nRead allele count from the dataset into a Pandas DataFrame\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregions\nList[str]\nGenomic regions to be queried.\nNone\n\n\nregion\nstr\nDEPRECATED - Genomic region to be queried.\nNone\n\n\n\n\n\n\n\nDataset.read_arrow(attrs=DEFAULT_ATTRS, samples=None, regions=None, samples_file=None, bed_file=None, skip_check_samples=False, set_af_filter='', scan_all_samples=False)\nRead data from the dataset into a PyArrow Table.\nFor large queries, a call to read_arrow() may not be able to fit all results in memory. In that case, the returned table will contain as many results as possible, and in order to retrieve the rest of the results, use the continue_read() function.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattrs\nList[str]\nList of attribute names to be read.\nDEFAULT_ATTRS\n\n\nsamples\n(str, List[str])\nSample names to be read.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to be read.\nNone\n\n\nsamples_file\nstr\nURI of file containing sample names to be read, one per line.\nNone\n\n\nbed_file\nstr\nURI of a BED file of genomic regions to be read.\nNone\n\n\nskip_check_samples\nbool\nSkip checking if the samples in samples_file exist in the dataset.\nFalse\n\n\nset_af_filter\nstr\nFilter variants by internal allele frequency. For example, to include variants with AF &gt; 0.1, set this to “&gt;0.1”.\n''\n\n\nscan_all_samples\nbool\nScan all samples when computing internal allele frequency.\nFalse\n\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npa.Table\nQuery results as a PyArrow Table.\n\n\n\n\n\n\n\nDataset.read_completed()\nReturns true if the previous read operation was complete. A read is considered complete if the resulting dataframe contained all results.\n\n\n\n\n\nType\nDescription\n\n\n\n\nTrue if the previous read operation was complete.\n\n\n\n\n\n\n\n\nDataset.read_iter(attrs=DEFAULT_ATTRS, samples=None, regions=None, samples_file=None, bed_file=None)\nIterator version of read().\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nattrs\nList[str]\nList of attribute names to be read.\nDEFAULT_ATTRS\n\n\nsamples\n(str, List[str])\nSample names to be read.\nNone\n\n\nregions\n(str, List[str])\nGenomic regions to be read.\nNone\n\n\nsamples_file\nstr\nURI of file containing sample names to be read, one per line.\nNone\n\n\nbed_file\nstr\nURI of a BED file of genomic regions to be read.\nNone\n\n\n\n\n\n\n\nDataset.read_variant_stats(region=None, drop_ref=False, regions=None, scan_all_samples=False)\nRead variant stats from the dataset into a Pandas DataFrame\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndrop_ref\nbool\nOmit “ref” alleles from the results\nFalse\n\n\nregions\nList[str]\nGenomic regions to be queried.\nNone\n\n\nscan_all_samples\nbool\nScan all samples when computing internal allele frequency.\nFalse\n\n\nregion\nstr\nDEPRECATED - Genomic region to be queried.\nNone\n\n\n\n\n\n\n\nDataset.read_variant_stats_arrow(region=None, drop_ref=False, regions=None, scan_all_samples=False)\nRead variant stats from the dataset into a PyArrow Table\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndrop_ref\nbool\nOmit “ref” alleles from the results\nFalse\n\n\nregions\nList[str]\nGenomic regions to be queried.\nNone\n\n\nscan_all_samples\nbool\nScan all samples when computing internal allele frequency.\nFalse\n\n\nregion\nstr\nDEPRECATED - Genomic region to be queried.\nNone\n\n\n\n\n\n\n\nDataset.sample_count()\nGet the number of samples in the dataset.\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nNumber of samples in the dataset.\n\n\n\n\n\n\n\nDataset.samples()\nGet the list of samples in the dataset.\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist\nList of samples in the dataset.\n\n\n\n\n\n\n\nDataset.schema_version()\nGet the VCF schema version of the dataset.\n\n\n\n\n\nType\nDescription\n\n\n\n\nint\nVCF schema version of the dataset.\n\n\n\n\n\n\n\nDataset.tiledb_stats()\nGet TileDB stats as a string.\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nTileDB stats as a string.\n\n\n\n\n\n\n\nDataset.version()\nReturn the TileDB-VCF version used to create the dataset.\n\n\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe TileDB-VCF version."
  },
  {
    "objectID": "documentation/data-model.html",
    "href": "documentation/data-model.html",
    "title": "Data Model",
    "section": "",
    "text": "The Solution section provides a high-level overview of how and why TileDB-VCF uses 3D sparse arrays to store genomic variant data. What follows are the technical implementation details about the underlying arrays, including their schemas, dimensions, tiling order, attributes, metadata.\n\n\nA TileDB-VCF dataset is composed of a group of two or more separate TileDB arrays:\n\na 3D sparse array for the actual genomic variants and associated fields/attributes\na 1D sparse array for the metadata stored in each single-sample VCF header\n\n\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nArray type\nSparse\n\n\nRank\n3D\n\n\nCell order\nRow-major\n\n\nTile order\nRow-major\n\n\n\n\n\n\nThe dimensions in the schema are:\n\n\n\nDimension Name\nTileDB Datatype\nCorresponding VCF Field\n\n\n\n\ncontig\nTILEDB_STRING_ASCII\nCHR\n\n\nstart_pos\nuint32_t\nVCFPOSplus TileDB anchors\n\n\nsample\nTILEDB_STRING_ASCII\nSample name\n\n\n\nAs mentioned before, the coordinates of the 3D array are contig along the first dimension, chromosomal location of the variants start position along the second dimension, and sample names along the third dimension.\n\n\n\nFor each field in a single-sample VCF record there is a corresponding attribute in the schema.\n\n\n\n\n\n\n\n\nAttribute Name\nTileDB Datatype\nDescription\n\n\n\n\nend_pos\nuint32_t\nVCF END position of VCF records\n\n\nqual\nfloat\nVCF QUAL field\n\n\nalleles\nvar&lt;char&gt;\nCSV list of REF and ALT VCF fields\n\n\nid\nvar&lt;char&gt;\nVCF ID field\n\n\nfilter_ids\nvar&lt;int32_t&gt;\nVector of integer IDs of entries in the FILTER VCF field\n\n\nreal_start_pos\nuint32_t\nVCF POS(no anchors)\n\n\ninfo\nvar&lt;uint8_t&gt;\nByte blob containing any INFO fields that are not stored as explicit attributes\n\n\nfmt\nvar&lt;uint8_t&gt;\nByte blob containing any FMT fields that are not stored as explicit attributes\n\n\ninfo_*\nvar&lt;uint8_t&gt;\nOne or more attributes storing specific VCF INFO fields, e.g. info_DP, info_MQ, etc.\n\n\nfmt_*\nvar&lt;uint8_t&gt;\nOne or more attributes storing specific VCF FORMAT fields, e.g. fmt_GT, fmt_MIN_DP, etc.\n\n\n\nThe info_* and fmt_* attributes allow individual INFO or FMT VCF fields to be extracted into explicit array attributes. This can be beneficial if your queries frequently access only a subset of the INFO or FMT fields, as no unrelated data then needs to be fetched from storage.\n\n\n\n\n\n\nNote\n\n\n\nThe choice of which fields to extract as explicit array attributes is user-configurable during array creation.\n\n\nAny extra info or format fields not extracted as explicit array attributes are stored in the byte blob attributes, info and fmt.\n\n\n\n\nanchor_gap Anchor gap value\nextra_attributes List of INFO or FMT field names that are stored as explicit array attributes\nversion Array schema version\n\nThese metadata values are updated during array creation, and are used during the export phase. The metadata is stored as “array metadata” in the sparse data array.\n\n\n\n\n\n\nWarning\n\n\n\nWhen ingesting samples, the sample header must be identical for all samples with respect to the contig mappings. That means all samples must have the exact same set of contigs listed in the VCF header. This requirement will be relaxed in future versions.\n\n\n\n\n\n\nThe vcf_headers array stores the original text of every ingested VCF header in order to:\n\nensure the original VCF file can be fully recovered for any given sample\nreconstruct an htslib header instance when reading from the dataset, which is used for operations such as mapping a filter ID back to the filter string, etc.\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nArray type\nSparse\n\n\nRank\n1D\n\n\nCell order\nRow-major\n\n\nTile order\nRow-major\n\n\n\n\n\n\n\n\n\nDimension Name\nTileDB Datatype\nDescription\n\n\n\n\nsample\nTILEDB_STRING_ASCII\nSample name\n\n\n\n\n\n\n\n\n\nAttribute Name\nTileDB Datatype\nDescription\n\n\n\n\nheader\nvar&lt;char&gt;\nOriginal text of the VCF header\n\n\n\n\n\n\n\nTo summarize, we’ve described three main entities:\n\nThe variant data array (3D sparse)\nThe general metadata, stored in the variant data array as metadata\nThe VCF header array (1D sparse)\n\nAll together we term this a “TileDB-VCF dataset.” Expressed as a directory hierarchy, a TileDB-VCF dataset has the following structure:\n&lt;dataset_uri&gt;/\n  |_ __tiledb_group.tdb\n  |_ data/\n      |_ __array_schema.tdb\n      |_ __meta/\n            |_ &lt;general-metadata-here&gt;\n      ... &lt;other array directories/fragments and files&gt;\n  |_ vcf_headers/\n      |_ __array_schema.tdb\n      ... &lt;other array directories/fragments and files&gt;\nThe root of the dataset, &lt;dataset_uri&gt; is a TileDB group. The data member is the TileDB 3D sparse array storing the variant data. This array stores the general TileDB-VCF metadata as its array metadata in folder data/__meta. The vcf_headers member is the TileDB 1D sparse array containing the VCF header data.\n\n\n\nDuring array creation, there are several array-related parameters that the user can control. These are:\n\nArray data tile capacity (default 10,000)\nThe “anchor gap” size (default 1,000)\nThe list of INFO and FMT fields to store as explicit array attributes (default is none).\n\nOnce chosen, these parameters cannot be changed.\nDuring sample ingestion, the user can specify the:\n\nSample batch size (default 10)\n\nThe above parameters may impact read and write performance, as well as the size of the persisted array. Therefore, some care should be taken to determine good values for these parameters before ingesting a large amount of data into an array.",
    "crumbs": [
      "Home page",
      "Background",
      "Data Model"
    ]
  },
  {
    "objectID": "documentation/data-model.html#tiledb-vcf-dataset",
    "href": "documentation/data-model.html#tiledb-vcf-dataset",
    "title": "Data Model",
    "section": "",
    "text": "A TileDB-VCF dataset is composed of a group of two or more separate TileDB arrays:\n\na 3D sparse array for the actual genomic variants and associated fields/attributes\na 1D sparse array for the metadata stored in each single-sample VCF header",
    "crumbs": [
      "Home page",
      "Background",
      "Data Model"
    ]
  },
  {
    "objectID": "documentation/data-model.html#data-array",
    "href": "documentation/data-model.html#data-array",
    "title": "Data Model",
    "section": "",
    "text": "Parameter\nValue\n\n\n\n\nArray type\nSparse\n\n\nRank\n3D\n\n\nCell order\nRow-major\n\n\nTile order\nRow-major\n\n\n\n\n\n\nThe dimensions in the schema are:\n\n\n\nDimension Name\nTileDB Datatype\nCorresponding VCF Field\n\n\n\n\ncontig\nTILEDB_STRING_ASCII\nCHR\n\n\nstart_pos\nuint32_t\nVCFPOSplus TileDB anchors\n\n\nsample\nTILEDB_STRING_ASCII\nSample name\n\n\n\nAs mentioned before, the coordinates of the 3D array are contig along the first dimension, chromosomal location of the variants start position along the second dimension, and sample names along the third dimension.\n\n\n\nFor each field in a single-sample VCF record there is a corresponding attribute in the schema.\n\n\n\n\n\n\n\n\nAttribute Name\nTileDB Datatype\nDescription\n\n\n\n\nend_pos\nuint32_t\nVCF END position of VCF records\n\n\nqual\nfloat\nVCF QUAL field\n\n\nalleles\nvar&lt;char&gt;\nCSV list of REF and ALT VCF fields\n\n\nid\nvar&lt;char&gt;\nVCF ID field\n\n\nfilter_ids\nvar&lt;int32_t&gt;\nVector of integer IDs of entries in the FILTER VCF field\n\n\nreal_start_pos\nuint32_t\nVCF POS(no anchors)\n\n\ninfo\nvar&lt;uint8_t&gt;\nByte blob containing any INFO fields that are not stored as explicit attributes\n\n\nfmt\nvar&lt;uint8_t&gt;\nByte blob containing any FMT fields that are not stored as explicit attributes\n\n\ninfo_*\nvar&lt;uint8_t&gt;\nOne or more attributes storing specific VCF INFO fields, e.g. info_DP, info_MQ, etc.\n\n\nfmt_*\nvar&lt;uint8_t&gt;\nOne or more attributes storing specific VCF FORMAT fields, e.g. fmt_GT, fmt_MIN_DP, etc.\n\n\n\nThe info_* and fmt_* attributes allow individual INFO or FMT VCF fields to be extracted into explicit array attributes. This can be beneficial if your queries frequently access only a subset of the INFO or FMT fields, as no unrelated data then needs to be fetched from storage.\n\n\n\n\n\n\nNote\n\n\n\nThe choice of which fields to extract as explicit array attributes is user-configurable during array creation.\n\n\nAny extra info or format fields not extracted as explicit array attributes are stored in the byte blob attributes, info and fmt.\n\n\n\n\nanchor_gap Anchor gap value\nextra_attributes List of INFO or FMT field names that are stored as explicit array attributes\nversion Array schema version\n\nThese metadata values are updated during array creation, and are used during the export phase. The metadata is stored as “array metadata” in the sparse data array.\n\n\n\n\n\n\nWarning\n\n\n\nWhen ingesting samples, the sample header must be identical for all samples with respect to the contig mappings. That means all samples must have the exact same set of contigs listed in the VCF header. This requirement will be relaxed in future versions.",
    "crumbs": [
      "Home page",
      "Background",
      "Data Model"
    ]
  },
  {
    "objectID": "documentation/data-model.html#vcf-headers-array",
    "href": "documentation/data-model.html#vcf-headers-array",
    "title": "Data Model",
    "section": "",
    "text": "The vcf_headers array stores the original text of every ingested VCF header in order to:\n\nensure the original VCF file can be fully recovered for any given sample\nreconstruct an htslib header instance when reading from the dataset, which is used for operations such as mapping a filter ID back to the filter string, etc.\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nArray type\nSparse\n\n\nRank\n1D\n\n\nCell order\nRow-major\n\n\nTile order\nRow-major\n\n\n\n\n\n\n\n\n\nDimension Name\nTileDB Datatype\nDescription\n\n\n\n\nsample\nTILEDB_STRING_ASCII\nSample name\n\n\n\n\n\n\n\n\n\nAttribute Name\nTileDB Datatype\nDescription\n\n\n\n\nheader\nvar&lt;char&gt;\nOriginal text of the VCF header",
    "crumbs": [
      "Home page",
      "Background",
      "Data Model"
    ]
  },
  {
    "objectID": "documentation/data-model.html#putting-it-all-together",
    "href": "documentation/data-model.html#putting-it-all-together",
    "title": "Data Model",
    "section": "",
    "text": "To summarize, we’ve described three main entities:\n\nThe variant data array (3D sparse)\nThe general metadata, stored in the variant data array as metadata\nThe VCF header array (1D sparse)\n\nAll together we term this a “TileDB-VCF dataset.” Expressed as a directory hierarchy, a TileDB-VCF dataset has the following structure:\n&lt;dataset_uri&gt;/\n  |_ __tiledb_group.tdb\n  |_ data/\n      |_ __array_schema.tdb\n      |_ __meta/\n            |_ &lt;general-metadata-here&gt;\n      ... &lt;other array directories/fragments and files&gt;\n  |_ vcf_headers/\n      |_ __array_schema.tdb\n      ... &lt;other array directories/fragments and files&gt;\nThe root of the dataset, &lt;dataset_uri&gt; is a TileDB group. The data member is the TileDB 3D sparse array storing the variant data. This array stores the general TileDB-VCF metadata as its array metadata in folder data/__meta. The vcf_headers member is the TileDB 1D sparse array containing the VCF header data.",
    "crumbs": [
      "Home page",
      "Background",
      "Data Model"
    ]
  },
  {
    "objectID": "documentation/data-model.html#configurable-parameters",
    "href": "documentation/data-model.html#configurable-parameters",
    "title": "Data Model",
    "section": "",
    "text": "During array creation, there are several array-related parameters that the user can control. These are:\n\nArray data tile capacity (default 10,000)\nThe “anchor gap” size (default 1,000)\nThe list of INFO and FMT fields to store as explicit array attributes (default is none).\n\nOnce chosen, these parameters cannot be changed.\nDuring sample ingestion, the user can specify the:\n\nSample batch size (default 10)\n\nThe above parameters may impact read and write performance, as well as the size of the persisted array. Therefore, some care should be taken to determine good values for these parameters before ingesting a large amount of data into an array.",
    "crumbs": [
      "Home page",
      "Background",
      "Data Model"
    ]
  },
  {
    "objectID": "documentation/ingestion/cli.html",
    "href": "documentation/ingestion/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Note\n\n\n\nThe files used in the following examples can be obtained here.\n\n\nThe first step is to create an empty dataset. Let’s save the dataset in a new array called small_dataset:\ntiledbvcf create --uri small_dataset",
    "crumbs": [
      "Home page",
      "Ingestion",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/ingestion/cli.html#create-a-new-dataset",
    "href": "documentation/ingestion/cli.html#create-a-new-dataset",
    "title": "CLI",
    "section": "",
    "text": "Note\n\n\n\nThe files used in the following examples can be obtained here.\n\n\nThe first step is to create an empty dataset. Let’s save the dataset in a new array called small_dataset:\ntiledbvcf create --uri small_dataset",
    "crumbs": [
      "Home page",
      "Ingestion",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/ingestion/cli.html#store-samples",
    "href": "documentation/ingestion/cli.html#store-samples",
    "title": "CLI",
    "section": "Store samples",
    "text": "Store samples\n\nIngest from local storage\nWe’ll start with a small example using 3 synthetic VCF files, assuming they are locally available in a folder data/vcfs:\ntree data\n## data\n## ├── gene-promoters-hg38.bed\n## ├── s3-bcf-samples.txt\n## └── vcfs\n##     ├── G1.vcf.gz\n##     ├── G1.vcf.gz.csi\n##     ├── G2.vcf.gz\n##     ├── G2.vcf.gz.csi\n##     ├── G3.vcf.gz\n##     └── G3.vcf.gz.csi\n##\n## 1 directory, 8 files\nIndex files are required for ingestion. If your VCF/BCF files have not been indexed you can use bcftools to do so:\nfor f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\nWe can ingest these files into small_dataset as follows:\ntiledbvcf store --uri small_dataset data/vcfs/G*.vcf.gz\nThat’s it! Let’s verify everything went okay using the stat command to provide high-level statistics about our dataset including the number of samples it contains and the variant attributes it includes.\ntiledbvcf stat --uri small_dataset\n## Statistics for dataset 'small_dataset':\n## - Version: 4\n## - Tile capacity: 10,000\n## - Anchor gap: 1,000\n## - Number of registered samples: 3\n## - Extracted attributes: none\nAt this point you have successfully created and populated a TileDB VCF dataset using data stored locally on your machine. Next we’ll look at the more common scenario of working with files stored on a cloud object store.\n\n\nIngest from S3\nTileDB Embedded’s native cloud features make it possible to ingest samples directly from remote locations. Here, we’ll ingest the following samples located on AWS S3:\ncat data/s3-bcf-samples.txt\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf\n## s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf\n\n\n\n\n\n\nNote\n\n\n\nSamples in this second batch are stored as BCF files which are also supported by TileDB-VCF.\n\n\nThis process is identical to the steps perfo_r_med above, the only changes needed to our code involve setting --scratch-mb to allocate some temporary space for downloading the files and providing the URLs for the remote files. In this case, we’ll simply pass the s3-bcf-samples.txt file, which includes a list of the BCF files we want to ingest.\n\n\n\n\n\n\nNote\n\n\n\nWhen ingesting samples from S3, you must configure enough scratch space to hold at least 20 samples. In general, you need 2 × the sample dimension’s sample_bactch_size (which by default is 10). You can read more about the data model here.\n\n\nYou can add the --verbose flag to print out more information during the store phase.\ntiledbvcf store \\\n  --uri small_dataset \\\n  --samples-file data/s3-bcf-samples.txt \\\n  --verbose\n  \n## Initialization completed in 3.17565 sec.\n## ...\n## Done. Ingested 1,391 records (+ 69,548 anchors) from 7 samples in 10.6751\n## seconds.\nConsolidating and vacuuming fragment metadata and commits are recommended after creating a new dataset or adding several new samples to an existing dataset.\ntiledbvcf utils consolidate fragment_meta --uri small_dataset\ntiledbvcf utils consolidate commits --uri small_dataset\ntiledbvcf utils vacuum fragment_meta --uri small_dataset\ntiledbvcf utils vacuum commits --uri small_dataset",
    "crumbs": [
      "Home page",
      "Ingestion",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/ingestion/cli.html#incremental-updates",
    "href": "documentation/ingestion/cli.html#incremental-updates",
    "title": "CLI",
    "section": "Incremental Updates",
    "text": "Incremental Updates\nA key advantage to using TileDB as a data store for genomic variant data is the ability to efficiently add new samples as they become available. The dataset creation command should be called once. Then you can invoke the store command multiple commands.\n\n\n\n\n\n\nNote\n\n\n\nThe store command is thread- and process-safe, even on the cloud. That means it can be invoked in parallel, arbitrarily scaling out the ingestion of massive datasets.\n\n\nSuppose we run the store for the first 3 local samples, followed by the store command for the 7 samples stored on the S3. If we run the stat command, we can verify that our dataset now includes 10 samples.\ntiledbvcf stat --uri small_dataset\n## Statistics for dataset 'small_dataset':\n## - Version: 4\n## - Tile capacity: 10,000\n## - Anchor gap: 1,000\n## - Number of registered samples: 10\n## - Extracted attributes: none\nBecause TileDB is designed to be updatable, the store process happens efficiently and without ever touching any previously or concurrently ingested data, avoiding computationally expensive operations like regenerating combined VCF files.",
    "crumbs": [
      "Home page",
      "Ingestion",
      "CLI"
    ]
  },
  {
    "objectID": "documentation/ingestion/overview.html",
    "href": "documentation/ingestion/overview.html",
    "title": "Ingestion",
    "section": "",
    "text": "Ingestion of VCF files is easy and straight forward.\nTileDB offers quick and easy cli, a python api and capabilities in TileDB for one-line distributed ingestion supporting millions of VCF files."
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html",
    "href": "examples/tutorial_tiledbvcf_basics.html",
    "title": "TileDB-VCF Tutorial",
    "section": "",
    "text": "This notebook will cover how to:",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#packages",
    "href": "examples/tutorial_tiledbvcf_basics.html#packages",
    "title": "TileDB-VCF Tutorial",
    "section": "Packages",
    "text": "Packages\n\nimport os\nimport tiledb\nimport tiledb.cloud\nimport tiledbvcf\nimport numpy as np\n\nprint(\n    f\"tiledb v{tiledb.version.version}\\n\"\n    f\"numpy v{np.__version__}\\n\"\n    # f\"tiledb-vcf v{tiledbvcf.version}\\n\"\n    f\"tiledb-cloud v{tiledb.cloud.version.version}\\n\"\n)\n\ntiledb v0.21.3\nnumpy v1.21.6\ntiledb-cloud v0.10.0\n\n\n\nSetup TileDB’s virtual file system.\n\nvfs = tiledb.VFS(config=tiledb.Config())",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#example-vcf-files",
    "href": "examples/tutorial_tiledbvcf_basics.html#example-vcf-files",
    "title": "TileDB-VCF Tutorial",
    "section": "Example VCF Files",
    "text": "Example VCF Files\nTileDB-VCF inherits TileDB’s native support for working with remote object stores, so we can ingest samples directly from remote services like AWS S3, Google Cloud, and Azure. Here, we’ll use single sample BCF files with chromosome 1 data for phase 3 samples from the 1KG project stored in a publicly readable S3 bucket.\n\nvcf_bucket = \"s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1\"\n\nFor demonstration purposes we’ll ingest individual batches of samples one at a time. To do this, we’ll create a list of S3 URIs pointing to 5 sample BCF files.\n\nbatch1_samples = [\"HG00096.bcf\", \"HG00097.bcf\", \"HG00099.bcf\", \"HG00100.bcf\", \"HG00101.bcf\"]\nbatch1_uris = [f\"{vcf_bucket}/{s}\" for s in batch1_samples]\nbatch1_uris\n\n['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00096.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00097.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00099.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00100.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00101.bcf']",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#create-a-new-dataset",
    "href": "examples/tutorial_tiledbvcf_basics.html#create-a-new-dataset",
    "title": "TileDB-VCF Tutorial",
    "section": "Create a New Dataset",
    "text": "Create a New Dataset\nBefore we can ingest VCF data into TileDB we must first create our new dataset. First, we’ll create a TileDB-VCF Dataset instance that opens a connection to our desired array_uri in write mode.\n\nds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\")\nds\n\n&lt;tiledbvcf.dataset.Dataset at 0x7efc1c203350&gt;\n\n\nNext, we’ll create the empty TileDB-VCF dataset.\nNote: We can optionally pass a VCF file to the vcf_attrs argument to automatically materialize all of the INFO and FORMAT fields as separate attributes in the array (rather than htslib-encoded blobs), which can improve query performance.\n\nds.create_dataset(vcf_attrs=batch1_uris[0], enable_allele_count=True, enable_variant_stats=True)\n\n# verify the array exists\nos.listdir(array_uri)\n\n['allele_count',\n 'metadata',\n '__meta',\n 'variant_stats',\n '__group',\n '__tiledb_group.tdb',\n 'data']",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#ingest-initial-batch-of-5-vcf-files",
    "href": "examples/tutorial_tiledbvcf_basics.html#ingest-initial-batch-of-5-vcf-files",
    "title": "TileDB-VCF Tutorial",
    "section": "Ingest Initial Batch of 5 VCF Files",
    "text": "Ingest Initial Batch of 5 VCF Files\nWith the empty TileDB-VCF dataset created we can now ingest our first batch of samples.\n\n%%time\nds.ingest_samples(sample_uris = batch1_uris)\n\nCPU times: user 10.2 s, sys: 1.55 s, total: 11.8 s\nWall time: 44.8 s\n\n\nThis should take approximately 30s when ingesting from S3. Note most of this time is spent streaming the data, ingesting local copies of these files takes about 12s.\nWe can verify the samples were ingested by listing the sample IDs contained within the new array:\n\nds = tiledbvcf.Dataset(array_uri, mode = \"r\")\nds.samples()\n\n['HG00096', 'HG00097', 'HG00099', 'HG00100', 'HG00101']",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#ingest-a-second-batch-of-5-vcf-files",
    "href": "examples/tutorial_tiledbvcf_basics.html#ingest-a-second-batch-of-5-vcf-files",
    "title": "TileDB-VCF Tutorial",
    "section": "Ingest a Second Batch of 5 VCF Files",
    "text": "Ingest a Second Batch of 5 VCF Files\nUpdate the existing TileDB-VCF dataset with a second batch of samples.\n\nbatch2_samples = [\"HG00102.bcf\", \"HG00103.bcf\", \"HG00105.bcf\", \"HG00106.bcf\", \"HG00107.bcf\"]\nbatch2_uris = [f\"{vcf_bucket}/{s}\" for s in batch2_samples]\nbatch2_uris\n\n['s3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00102.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00103.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00105.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00106.bcf',\n 's3://tiledb-inc-demo-data/examples/notebooks/vcfs/1kgp3-chr1/HG00107.bcf']\n\n\nThe process is identical: we need to reopen the Dataset in write mode and then pass another list of VCF URIs to ingest_samples().\n\n%%time \nds = tiledbvcf.Dataset(uri=array_uri, mode=\"w\") #Incremental update to the array, previous data is not touched \nds.ingest_samples(sample_uris = batch2_uris)\n\nCPU times: user 10.2 s, sys: 1.57 s, total: 11.8 s\nWall time: 46.9 s\n\n\n\nds = tiledbvcf.Dataset(array_uri, mode = \"r\") #Open array in read mode, and print out the samples we just ingested to verify\nds.samples()\n\n['HG00096',\n 'HG00097',\n 'HG00099',\n 'HG00100',\n 'HG00101',\n 'HG00102',\n 'HG00103',\n 'HG00105',\n 'HG00106',\n 'HG00107']\n\n\nKey takeaways: - new samples are easily added to existing datasets - incremental updates are extremely efficient",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#setup-1",
    "href": "examples/tutorial_tiledbvcf_basics.html#setup-1",
    "title": "TileDB-VCF Tutorial",
    "section": "Setup",
    "text": "Setup\nImport the Delayed module from tiledb.cloud. This is allows us to convert a normal python function into one that can be Delayed and serverlessly executed.\n\nfrom tiledb.cloud.compute import Delayed",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#run-a-basic-query-serverlessly",
    "href": "examples/tutorial_tiledbvcf_basics.html#run-a-basic-query-serverlessly",
    "title": "TileDB-VCF Tutorial",
    "section": "Run a Basic Query Serverlessly",
    "text": "Run a Basic Query Serverlessly\nCreate UDF that wraps tiledbvcf.read().\n\ndef vcf_query(uri, attrs, regions, samples=None, sample_partition=(0, 1)):\n    import tiledbvcf\n    import pyarrow\n    cfg = tiledbvcf.ReadConfig(sample_partition=sample_partition)\n    vcf_ds = tiledbvcf.Dataset(uri, mode=\"r\", cfg=cfg)\n    \n    results = [vcf_ds.read_arrow(attrs=attrs, samples=samples, regions=regions)]\n    while not vcf_ds.read_completed():\n        results.append(vcf_ds.continue_read_arrow())\n\n    return pyarrow.concat_tables(results, promote=False)\n\nCreate a delayed instance of this UDF and specify the parameters we want to run it with.\n\narray_uri = \"tiledb://TileDB-Inc/vcf-1kg-nygc\"\n\nquery_attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"fmt_GT\"]\nquery_samples = [\"HG00096\", \"HG00097\", \"HG00099\", \"HG00100\", \"HG00101\", \"HG00102\", \"HG00103\", \"HG00105\", \"HG00106\", \"HG00107\", \"HG00108\", \"HG00109\", \"HG00110\", \"HG00111\", \"HG00112\", \"HG00113\", \"HG00114\", \"HG00115\", \"HG00116\", \"HG00117\", \"HG00118\", \"HG00119\", \"HG00120\", \"HG00121\", \"HG00122\", \"HG00123\", \"HG00125\", \"HG00126\", \"HG00127\", \"HG00128\", \"HG00129\", \"HG00130\", \"HG00131\", \"HG00132\", \"HG00133\", \"HG00136\", \"HG00137\", \"HG00138\", \"HG00139\", \"HG00140\", \"HG00141\", \"HG00142\", \"HG00143\", \"HG00145\", \"HG00146\", \"HG00148\", \"HG00149\", \"HG00150\", \"HG00151\", \"HG00154\", \"HG00155\", \"HG00157\", \"HG00158\", \"HG00159\", \"HG00160\", \"HG00171\", \"HG00173\", \"HG00174\", \"HG00176\", \"HG00177\", \"HG00178\", \"HG00179\", \"HG00180\", \"HG00181\", \"HG00182\", \"HG00183\", \"HG00185\", \"HG00186\", \"HG00187\", \"HG00188\", \"HG00189\", \"HG00190\", \"HG00231\", \"HG00232\", \"HG00233\", \"HG00234\", \"HG00235\", \"HG00236\", \"HG00237\", \"HG00238\", \"HG00239\", \"HG00240\", \"HG00242\", \"HG00243\", \"HG00244\", \"HG00245\", \"HG00246\", \"HG00250\", \"HG00251\", \"HG00252\", \"HG00253\", \"HG00254\", \"HG00255\", \"HG00256\", \"HG00257\", \"HG00258\", \"HG00259\", \"HG00260\", \"HG00261\", \"HG00262\"]\nquery_regions = [\"chr1:43337848-43352772\"]\n\ndelayed_read = Delayed(\n        func_exec=vcf_query,\n        result_format=tiledb.cloud.UDFResultType.ARROW,\n    )(\n        uri=array_uri, \n        attrs=query_attrs, \n        regions=query_regions,\n        samples=query_samples[:10],\n)\n\n\ndelayed_read.visualize()\n\n\n\n\nCall compute() to serverlessly execute the function.\n\n%%time\nres1 = delayed_read.compute()\nres1.to_pandas()\n\nCPU times: user 15.1 ms, sys: 3.82 ms, total: 19 ms\nWall time: 8.21 s\n\n\n\n\n\n\n\n\n\nsample_name\ncontig\npos_start\npos_end\nfmt_GT\n\n\n\n\n0\nHG00099\nchr1\n43337743\n43338411\n[0, 0]\n\n\n1\nHG00106\nchr1\n43337749\n43338055\n[0, 0]\n\n\n2\nHG00102\nchr1\n43337756\n43338088\n[0, 0]\n\n\n3\nHG00097\nchr1\n43337791\n43338053\n[0, 0]\n\n\n4\nHG00100\nchr1\n43337812\n43337850\n[0, 0]\n\n\n...\n...\n...\n...\n...\n...\n\n\n22442\nHG00105\nchr1\n43352768\n43352779\n[0, 0]\n\n\n22443\nHG00106\nchr1\n43352769\n43352769\n[0, 0]\n\n\n22444\nHG00106\nchr1\n43352770\n43352791\n[0, 0]\n\n\n22445\nHG00100\nchr1\n43352771\n43352771\n[0, 0]\n\n\n22446\nHG00100\nchr1\n43352772\n43352781\n[0, 0]\n\n\n\n\n22447 rows × 5 columns\n\n\n\nKey takeaways: - TileDB-VCF Datasets can be sliced directly from remote object stores - TileDB provides a flexible platform for building serverless data processing pipelines",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_basics.html#run-multiple-batches-of-queries-in-parallel",
    "href": "examples/tutorial_tiledbvcf_basics.html#run-multiple-batches-of-queries-in-parallel",
    "title": "TileDB-VCF Tutorial",
    "section": "Run Multiple Batches of Queries in Parallel",
    "text": "Run Multiple Batches of Queries in Parallel\nAdd another UDF to combine the outputs into a single result.\n\ndef combine_results(dfs): #Taking resluts and combining them into one table\n    import pyarrow as pa\n    out = pa.concat_tables([x for x in dfs if x is not None])\n    return out\n\nAssemble and visualize the task graph.\n\nnparts = 10\ndelayed_results = []\n\nfor i in range(nparts): #splitting up 100 samples into 10 partitions, that will run in parallel, only thing that changes is the sample index\n    delayed_results.append(\n        Delayed(\n            func_exec=vcf_query, \n            result_format=tiledb.cloud.UDFResultType.ARROW,\n        )(\n            uri=array_uri, \n            attrs=query_attrs,\n            regions=query_regions,\n            samples=query_samples,\n            sample_partition=(i, nparts),\n        )\n    )\ndelayed_results = Delayed(combine_results, local=True)(delayed_results) #Taking all the results and combining them into a single table, via the `combine_results` function\n\ndelayed_results.visualize()\n\n\n\n\n\n%%time\nres2 = delayed_results.compute()\nres2.to_pandas()\n\nCPU times: user 112 ms, sys: 26.7 ms, total: 139 ms\nWall time: 9.29 s\n\n\n\n\n\n\n\n\n\nsample_name\ncontig\npos_start\npos_end\nfmt_GT\n\n\n\n\n0\nHG00099\nchr1\n43337743\n43338411\n[0, 0]\n\n\n1\nHG00106\nchr1\n43337749\n43338055\n[0, 0]\n\n\n2\nHG00102\nchr1\n43337756\n43338088\n[0, 0]\n\n\n3\nHG00097\nchr1\n43337791\n43338053\n[0, 0]\n\n\n4\nHG00100\nchr1\n43337812\n43337850\n[0, 0]\n\n\n...\n...\n...\n...\n...\n...\n\n\n216254\nHG00260\nchr1\n43352762\n43352785\n[0, 0]\n\n\n216255\nHG00262\nchr1\n43352762\n43352766\n[0, 0]\n\n\n216256\nHG00261\nchr1\n43352766\n43352769\n[0, 0]\n\n\n216257\nHG00262\nchr1\n43352767\n43352775\n[0, 0]\n\n\n216258\nHG00261\nchr1\n43352770\n43352779\n[0, 0]\n\n\n\n\n216259 rows × 5 columns\n\n\n\nKey takeaways: - TileDB’s serverless infrastructure makes it easy to setup and run large scale distributed queries",
    "crumbs": [
      "Home page",
      "Examples",
      "TileDB-VCF Tutorial"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html",
    "href": "examples/tutorial_tiledbvcf_gwas.html",
    "title": "GWAS with the 1000 Genomes TileDB-VCF Dataset",
    "section": "",
    "text": "In this notebook we’ll perform a rudimentary genome-wide association study using the 1000 Genomes (1KG) dataset. The goal of this tutorial is to demonstrate the mechanics of performing genome-wide analyses using variant call data stored with TileDB-VCF and how such analyses can be easily scaled using TileDB’s serverless computation platform.\nTo get started we’ll load a few required packages and define several variables that we’ll refer to throughout the notebook.",
    "crumbs": [
      "Home page",
      "Examples",
      "GWAS with the 1000 Genomes TileDB-VCF Dataset"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#packages",
    "href": "examples/tutorial_tiledbvcf_gwas.html#packages",
    "title": "GWAS with the 1000 Genomes TileDB-VCF Dataset",
    "section": "Packages",
    "text": "Packages\n\nimport tiledb\nimport tiledbvcf\nimport tiledb.cloud\nfrom tiledb.cloud.compute import Delayed\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nprint(\n    f\"tileDB-vcf v{tiledbvcf.version}\\n\"\n    f\"tileDB-cloud v{tiledb.cloud.version.version}\"\n)\n\ntileDB-vcf v0.22.1.dev27\ntileDB-cloud v0.10.0",
    "crumbs": [
      "Home page",
      "Examples",
      "GWAS with the 1000 Genomes TileDB-VCF Dataset"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#variables",
    "href": "examples/tutorial_tiledbvcf_gwas.html#variables",
    "title": "GWAS with the 1000 Genomes TileDB-VCF Dataset",
    "section": "Variables",
    "text": "Variables\n\n# variables\ngenome = \"hg19\"\narray_uri = \"tiledb://TileDB-Inc/vcf-1kg-phase3\"\nsample_array = \"tiledb://TileDB-Inc/vcf-1kg-sample-metadata\"\n\n# vcf attributes to include\nattrs = [\n    \"sample_name\", \n    \"contig\",\n    \"pos_start\",\n    \"pos_end\", \n    \"fmt_GT\"\n]\n\n\n!tiledbvcf stat --uri tiledb://TileDB-Inc/vcf-1kg-phase3\n\n/bin/bash: line 1: tiledbvcf: command not found",
    "crumbs": [
      "Home page",
      "Examples",
      "GWAS with the 1000 Genomes TileDB-VCF Dataset"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#vcf_snp_query",
    "href": "examples/tutorial_tiledbvcf_gwas.html#vcf_snp_query",
    "title": "GWAS with the 1000 Genomes TileDB-VCF Dataset",
    "section": "1. vcf_snp_query()",
    "text": "1. vcf_snp_query()\n\nqueries a specified window from the genome\nfilters the sites based on the defined criteria\n\n\n%%time\nres = tiledb.cloud.udf.exec(\n    func = \"aaronwolen/vcf_snp_query\",\n    uri = array_uri,\n    attrs = attrs,\n    regions = bed_regions[:2],\n)\n\nres.to_pandas()\n\nCPU times: user 94.3 ms, sys: 62.6 ms, total: 157 ms\nWall time: 9.48 s\n\n\n\n\n\n\n\n\n\nsample_name\ncontig\npos_start\npos_end\nfmt_GT\n\n\n\n\n0\nHG00097\n20\n60828\n60828\n[0, 1]\n\n\n1\nHG00101\n20\n61098\n61098\n[0, 1]\n\n\n2\nHG00103\n20\n61098\n61098\n[1, 0]\n\n\n3\nHG00109\n20\n61098\n61098\n[0, 1]\n\n\n4\nHG00111\n20\n61098\n61098\n[0, 1]\n\n\n...\n...\n...\n...\n...\n...\n\n\n577809\nNA21142\n20\n195696\n195696\n[1, 1]\n\n\n577810\nNA21143\n20\n195696\n195696\n[1, 1]\n\n\n577811\nNA21144\n20\n195696\n195696\n[1, 1]\n\n\n577812\nNA21142\n20\n196604\n196604\n[0, 1]\n\n\n577813\nNA21144\n20\n196604\n196604\n[1, 0]\n\n\n\n\n551155 rows × 5 columns",
    "crumbs": [
      "Home page",
      "Examples",
      "GWAS with the 1000 Genomes TileDB-VCF Dataset"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#filter_variants",
    "href": "examples/tutorial_tiledbvcf_gwas.html#filter_variants",
    "title": "GWAS with the 1000 Genomes TileDB-VCF Dataset",
    "section": "2. filter_variants()",
    "text": "2. filter_variants()\n\ncalculates allele count and minor allele frequencies\noptinally filters for variants using the maf threshold\n\n\n%%time\nres2 = tiledb.cloud.udf.exec(\n    func = \"aaronwolen/filter_variants\", \n    df = res,\n    maf = (0.05, 0.95),\n)\n\nres2.to_pandas()\n\nCPU times: user 183 ms, sys: 108 ms, total: 292 ms\nWall time: 7.69 s\n\n\n\n\n\n\n\n\n\n\nsample_name\npos_end\nfmt_GT\ndose\n\n\ncontig\npos_start\n\n\n\n\n\n\n\n\n20\n61098\nHG00101\n61098\n[0, 1]\n1\n\n\n61098\nHG00103\n61098\n[1, 0]\n1\n\n\n61098\nHG00109\n61098\n[0, 1]\n1\n\n\n61098\nHG00111\n61098\n[0, 1]\n1\n\n\n61098\nHG00115\n61098\n[1, 0]\n1\n\n\n...\n...\n...\n...\n...\n\n\n195696\nNA21142\n195696\n[1, 1]\n2\n\n\n195696\nNA21143\n195696\n[1, 1]\n2\n\n\n195696\nNA21144\n195696\n[1, 1]\n2\n\n\n196604\nNA21142\n196604\n[0, 1]\n1\n\n\n196604\nNA21144\n196604\n[1, 0]\n1\n\n\n\n\n488449 rows × 4 columns",
    "crumbs": [
      "Home page",
      "Examples",
      "GWAS with the 1000 Genomes TileDB-VCF Dataset"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#calc_gwas",
    "href": "examples/tutorial_tiledbvcf_gwas.html#calc_gwas",
    "title": "GWAS with the 1000 Genomes TileDB-VCF Dataset",
    "section": "3. calc_gwas()",
    "text": "3. calc_gwas()\nReshapes the Dataframe of variants into a variant by sample matrix and performs a chi-squared analysis on site for the included trait\n\n%%time\nres3 = tiledb.cloud.udf.exec(\n    func = \"aaronwolen/calc_gwas\",\n    df = res2,\n    trait = df_samples.is_female,\n)\n\nres3.to_pandas()\n\nCPU times: user 90.5 ms, sys: 23 ms, total: 113 ms\nWall time: 9.66 s\n\n\n\n\n\n\n\n\n\ncontig\npos_start\noddsratio\npvalue\n\n\n\n\n0\n20\n61098\n0.494083\n0.482112\n\n\n1\n20\n61138\n0.289018\n0.590851\n\n\n2\n20\n61795\n0.068686\n0.793260\n\n\n3\n20\n63231\n0.851720\n0.356066\n\n\n4\n20\n63244\n0.004420\n0.946994\n\n\n...\n...\n...\n...\n...\n\n\n423\n20\n198965\n0.150694\n0.697873\n\n\n424\n20\n198977\n0.025577\n0.872937\n\n\n425\n20\n199078\n0.000250\n0.987379\n\n\n426\n20\n199114\n0.000250\n0.987379\n\n\n427\n20\n199986\n0.000250\n0.987379\n\n\n\n\n428 rows × 4 columns",
    "crumbs": [
      "Home page",
      "Examples",
      "GWAS with the 1000 Genomes TileDB-VCF Dataset"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#combine_results",
    "href": "examples/tutorial_tiledbvcf_gwas.html#combine_results",
    "title": "GWAS with the 1000 Genomes TileDB-VCF Dataset",
    "section": "4. combine_results()",
    "text": "4. combine_results()\n\ncreates a single dataframe containing individual results from each partition\n\n\ndef combine_results(dfs):\n    \"\"\"\n    Combine GWAS Results\n\n    :param list of dataframes: Region-specific GWAS results to combine\n    \"\"\"\n    import pyarrow as pa\n    print(f\"Input list contains {len(dfs)} Arrow tables\")\n    out = pa.concat_tables([x for x in dfs if x is not None])\n    return out",
    "crumbs": [
      "Home page",
      "Examples",
      "GWAS with the 1000 Genomes TileDB-VCF Dataset"
    ]
  },
  {
    "objectID": "examples/tutorial_tiledbvcf_gwas.html#summary",
    "href": "examples/tutorial_tiledbvcf_gwas.html#summary",
    "title": "GWAS with the 1000 Genomes TileDB-VCF Dataset",
    "section": "Summary",
    "text": "Summary\nHere we’ve demonstrated how genome-wide pipelines can be encapsulated as distinct tasks, distributed as UDFs, and easily deployed in parallel at scale.",
    "crumbs": [
      "Home page",
      "Examples",
      "GWAS with the 1000 Genomes TileDB-VCF Dataset"
    ]
  }
]